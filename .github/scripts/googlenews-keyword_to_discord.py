import xml.etree.ElementTree as ET
import requests
import re
import os
import time
import random
import logging
import json
import base64
import sqlite3
import sys
import pytz
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode, unquote, quote
from email.utils import parsedate_to_datetime
from datetime import datetime, timedelta
from dateutil import parser
from dateutil.tz import gettz
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK_KEYWORD = os.environ.get('DISCORD_WEBHOOK_KEYWORD')
DISCORD_AVATAR_KEYWORD = os.environ.get('DISCORD_AVATAR_KEYWORD')
DISCORD_USERNAME_KEYWORD = os.environ.get('DISCORD_USERNAME_KEYWORD')
INITIALIZE_KEYWORD = os.environ.get('INITIALIZE_MODE_KEYWORD', 'false').lower() == 'true'
ADVANCED_FILTER_KEYWORD = os.environ.get('ADVANCED_FILTER_KEYWORD', '')
DATE_FILTER_KEYWORD = os.environ.get('DATE_FILTER_KEYWORD', '')
AFTER_DATE = os.environ.get('AFTER_DATE', '')
BEFORE_DATE = os.environ.get('BEFORE_DATE', '')
WHEN = os.environ.get('WHEN', '')
HL = os.environ.get('HL', '')
GL = os.environ.get('GL', '')
CEID = os.environ.get('CEID', '')
ORIGIN_LINK_KEYWORD = os.getenv('ORIGIN_LINK_KEYWORD', '').lower()
ORIGIN_LINK_KEYWORD = ORIGIN_LINK_KEYWORD not in ['false', 'f', '0', 'no', 'n']
KEYWORD_MODE = os.environ.get('KEYWORD_MODE', 'false').lower() == 'true'
KEYWORD = os.environ.get('KEYWORD', '')
RSS_URL_KEYWORD = os.environ.get('RSS_URL_KEYWORD', '')

# DB ì„¤ì •
DB_PATH = 'google_news_keyword.db'

country_configs = {
    # ë™ì•„ì‹œì•„
    'KR': ('ko', 'KR:ko', 'Google ë‰´ìŠ¤', 'í•œêµ­', 'South Korea', 'ğŸ‡°ğŸ‡·', 'Asia/Seoul', '%Yë…„ %mì›” %dì¼ %H:%M:%S (KST)'),
    'JP': ('ja', 'JP:ja', 'Google ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æ—¥æœ¬', 'Japan', 'ğŸ‡¯ğŸ‡µ', 'Asia/Tokyo', '%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S (JST)'),
    'CN': ('zh-CN', 'CN:zh-Hans', 'Google æ–°é—»', 'ä¸­å›½', 'China', 'ğŸ‡¨ğŸ‡³', 'Asia/Shanghai', '%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S (CST)'),
    'TW': ('zh-TW', 'TW:zh-Hant', 'Google æ–°è', 'å°ç£', 'Taiwan', 'ğŸ‡¹ğŸ‡¼', 'Asia/Taipei', '%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S (NST)'),
    'HK': ('zh-HK', 'HK:zh-Hant', 'Google æ–°è', 'é¦™æ¸¯', 'Hong Kong', 'ğŸ‡­ğŸ‡°', 'Asia/Hong_Kong', '%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S (HKT)'),
    
    # ë™ë‚¨ì•„ì‹œì•„
    'VN': ('vi', 'VN:vi', 'Google Tin tá»©c', 'Viá»‡t Nam', 'Vietnam', 'ğŸ‡»ğŸ‡³', 'Asia/Ho_Chi_Minh', '%d/%m/%Y %H:%M:%S (ICT)'),
    'TH': ('th', 'TH:th', 'Google News', 'à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢', 'Thailand', 'ğŸ‡¹ğŸ‡­', 'Asia/Bangkok', '%d/%m/%Y %H:%M:%S (ICT)'),
    'PH': ('en-PH', 'PH:en', 'Google News', 'Philippines', 'Philippines', 'ğŸ‡µğŸ‡­', 'Asia/Manila', '%Y-%m-%d %I:%M:%S %p (PHT)'),
    'MY': ('ms-MY', 'MY:ms', 'Berita Google', 'Malaysia', 'Malaysia', 'ğŸ‡²ğŸ‡¾', 'Asia/Kuala_Lumpur', '%d/%m/%Y %H:%M:%S (MYT)'),
    'SG': ('en-SG', 'SG:en', 'Google News', 'Singapore', 'Singapore', 'ğŸ‡¸ğŸ‡¬', 'Asia/Singapore', '%Y-%m-%d %I:%M:%S %p (SGT)'),
    'ID': ('id', 'ID:id', 'Google Berita', 'Indonesia', 'Indonesia', 'ğŸ‡®ğŸ‡©', 'Asia/Jakarta', '%d/%m/%Y %H:%M:%S (WIB)'),
    
    # ë‚¨ì•„ì‹œì•„
    'IN': ('en-IN', 'IN:en', 'Google News', 'India', 'India', 'ğŸ‡®ğŸ‡³', 'Asia/Kolkata', '%d/%m/%Y %I:%M:%S %p (IST)'),
    'BD': ('bn', 'BD:bn', 'Google News', 'à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶', 'Bangladesh', 'ğŸ‡§ğŸ‡©', 'Asia/Dhaka', '%d/%m/%Y %H:%M:%S (BST)'),
    'PK': ('en-PK', 'PK:en', 'Google News', 'Pakistan', 'Pakistan', 'ğŸ‡µğŸ‡°', 'Asia/Karachi', '%d/%m/%Y %I:%M:%S %p (PKT)'),
    
    # ì„œì•„ì‹œì•„
    'IL': ('he', 'IL:he', '×—×“×©×•×ª Google', '×™×©×¨××œ', 'Israel', 'ğŸ‡®ğŸ‡±', 'Asia/Jerusalem', '%d/%m/%Y %H:%M:%S (IST)'),
    'AE': ('ar', 'AE:ar', 'Ø£Ø®Ø¨Ø§Ø± Google', 'Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©', 'United Arab Emirates', 'ğŸ‡¦ğŸ‡ª', 'Asia/Dubai', '%d/%m/%Y %I:%M:%S %p (GST)'),
    'TR': ('tr', 'TR:tr', 'Google Haberler', 'TÃ¼rkiye', 'Turkey', 'ğŸ‡¹ğŸ‡·', 'Europe/Istanbul', '%d.%m.%Y %H:%M:%S (TRT)'),
    'LB': ('ar', 'LB:ar', 'Ø£Ø®Ø¨Ø§Ø± Google', 'Ù„Ø¨Ù†Ø§Ù†', 'Lebanon', 'ğŸ‡±ğŸ‡§', 'Asia/Beirut', '%d/%m/%Y %I:%M:%S %p (EET)'),

    # ì˜¤ì„¸ì•„ë‹ˆì•„
    'AU': ('en-AU', 'AU:en', 'Google News', 'Australia', 'Australia', 'ğŸ‡¦ğŸ‡º', 'Australia/Sydney', '%d/%m/%Y %I:%M:%S %p (AEST)'),
    'NZ': ('en-NZ', 'NZ:en', 'Google News', 'New Zealand', 'New Zealand', 'ğŸ‡³ğŸ‡¿', 'Pacific/Auckland', '%d/%m/%Y %I:%M:%S %p (NZST)'),

    # ëŸ¬ì‹œì•„ì™€ ë™ìœ ëŸ½
    'RU': ('ru', 'RU:ru', 'Google ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸', 'Ğ Ğ¾ÑÑĞ¸Ñ', 'Russia', 'ğŸ‡·ğŸ‡º', 'Europe/Moscow', '%d.%m.%Y %H:%M:%S (MSK)'),
    'UA': ('uk', 'UA:uk', 'Google ĞĞ¾Ğ²Ğ¸Ğ½Ğ¸', 'Ğ£ĞºÑ€Ğ°Ñ—Ğ½Ğ°', 'Ukraine', 'ğŸ‡ºğŸ‡¦', 'Europe/Kiev', '%d.%m.%Y %H:%M:%S (EET)'),

    # ìœ ëŸ½
    'GR': ('el', 'GR:el', 'Î•Î¹Î´Î®ÏƒÎµÎ¹Ï‚ Google', 'Î•Î»Î»Î¬Î´Î±', 'Greece', 'ğŸ‡¬ğŸ‡·', 'Europe/Athens', '%d/%m/%Y %H:%M:%S (EET)'),
    'DE': ('de', 'DE:de', 'Google News', 'Deutschland', 'Germany', 'ğŸ‡©ğŸ‡ª', 'Europe/Berlin', '%d.%m.%Y %H:%M:%S (CET)'),
    'NL': ('nl', 'NL:nl', 'Google Nieuws', 'Nederland', 'Netherlands', 'ğŸ‡³ğŸ‡±', 'Europe/Amsterdam', '%d-%m-%Y %H:%M:%S (CET)'),
    'NO': ('no', 'NO:no', 'Google Nyheter', 'Norge', 'Norway', 'ğŸ‡³ğŸ‡´', 'Europe/Oslo', '%d.%m.%Y %H:%M:%S (CET)'),
    'LV': ('lv', 'LV:lv', 'Google ziÅ†as', 'Latvija', 'Latvia', 'ğŸ‡±ğŸ‡»', 'Europe/Riga', '%d.%m.%Y %H:%M:%S (EET)'),
    'LT': ('lt', 'LT:lt', 'Google naujienos', 'Lietuva', 'Lithuania', 'ğŸ‡±ğŸ‡¹', 'Europe/Vilnius', '%Y-%m-%d %H:%M:%S (EET)'),
    'RO': ('ro', 'RO:ro', 'È˜tiri Google', 'RomÃ¢nia', 'Romania', 'ğŸ‡·ğŸ‡´', 'Europe/Bucharest', '%d.%m.%Y %H:%M:%S (EET)'),
    'BE': ('fr', 'BE:fr', 'Google ActualitÃ©s', 'Belgique', 'Belgium', 'ğŸ‡§ğŸ‡ª', 'Europe/Brussels', '%d/%m/%Y %H:%M:%S (CET)'),
    'BG': ('bg', 'BG:bg', 'Google ĞĞ¾Ğ²Ğ¸Ğ½Ğ¸', 'Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€Ğ¸Ñ', 'Bulgaria', 'ğŸ‡§ğŸ‡¬', 'Europe/Sofia', '%d.%m.%Y %H:%M:%S (EET)'),
    'SK': ('sk', 'SK:sk', 'SprÃ¡vy Google', 'Slovensko', 'Slovakia', 'ğŸ‡¸ğŸ‡°', 'Europe/Bratislava', '%d.%m.%Y %H:%M:%S (CET)'),
    'SI': ('sl', 'SI:sl', 'Google News', 'Slovenija', 'Slovenia', 'ğŸ‡¸ğŸ‡®', 'Europe/Ljubljana', '%d.%m.%Y %H:%M:%S (CET)'),
    'CH': ('de', 'CH:de', 'Google News', 'Schweiz', 'Switzerland', 'ğŸ‡¨ğŸ‡­', 'Europe/Zurich', '%d.%m.%Y %H:%M:%S (CET)'),
    'ES': ('es', 'ES:es', 'Google News', 'EspaÃ±a', 'Spain', 'ğŸ‡ªğŸ‡¸', 'Europe/Madrid', '%d/%m/%Y %H:%M:%S (CET)'),
    'SE': ('sv', 'SE:sv', 'Google Nyheter', 'Sverige', 'Sweden', 'ğŸ‡¸ğŸ‡ª', 'Europe/Stockholm', '%Y-%m-%d %H:%M:%S (CET)'),
    'RS': ('sr', 'RS:sr', 'Google Ğ²ĞµÑÑ‚Ğ¸', 'Ğ¡Ñ€Ğ±Ğ¸Ñ˜Ğ°', 'Serbia', 'ğŸ‡·ğŸ‡¸', 'Europe/Belgrade', '%d.%m.%Y %H:%M:%S (CET)'),
    'AT': ('de', 'AT:de', 'Google News', 'Ã–sterreich', 'Austria', 'ğŸ‡¦ğŸ‡¹', 'Europe/Vienna', '%d.%m.%Y %H:%M:%S (CET)'),
    'IE': ('en-IE', 'IE:en', 'Google News', 'Ireland', 'Ireland', 'ğŸ‡®ğŸ‡ª', 'Europe/Dublin', '%d/%m/%Y %H:%M:%S (GMT)'),
    'EE': ('et-EE', 'EE:et', 'Google News', 'Eesti', 'Estonia', 'ğŸ‡ªğŸ‡ª', 'Europe/Tallinn', '%d.%m.%Y %H:%M:%S (EET)'),
    'IT': ('it', 'IT:it', 'Google News', 'Italia', 'Italy', 'ğŸ‡®ğŸ‡¹', 'Europe/Rome', '%d/%m/%Y %H:%M:%S (CET)'),
    'CZ': ('cs', 'CZ:cs', 'ZprÃ¡vy Google', 'ÄŒesko', 'Czech Republic', 'ğŸ‡¨ğŸ‡¿', 'Europe/Prague', '%d.%m.%Y %H:%M:%S (CET)'),
    'GB': ('en-GB', 'GB:en', 'Google News', 'United Kingdom', 'United Kingdom', 'ğŸ‡¬ğŸ‡§', 'Europe/London', '%d/%m/%Y %H:%M:%S (GMT)'),
    'PL': ('pl', 'PL:pl', 'Google News', 'Polska', 'Poland', 'ğŸ‡µğŸ‡±', 'Europe/Warsaw', '%d.%m.%Y %H:%M:%S (CET)'),
    'PT': ('pt-PT', 'PT:pt-150', 'Google NotÃ­cias', 'Portugal', 'Portugal', 'ğŸ‡µğŸ‡¹', 'Europe/Lisbon', '%d/%m/%Y %H:%M:%S (WET)'),
    'FI': ('fi-FI', 'FI:fi', 'Google Uutiset', 'Suomi', 'Finland', 'ğŸ‡«ğŸ‡®', 'Europe/Helsinki', '%d.%m.%Y %H:%M:%S (EET)'),
    'FR': ('fr', 'FR:fr', 'Google ActualitÃ©s', 'France', 'France', 'ğŸ‡«ğŸ‡·', 'Europe/Paris', '%d/%m/%Y %H:%M:%S (CET)'),
    'HU': ('hu', 'HU:hu', 'Google HÃ­rek', 'MagyarorszÃ¡g', 'Hungary', 'ğŸ‡­ğŸ‡º', 'Europe/Budapest', '%Y.%m.%d %H:%M:%S (CET)'),

    # ë¶ë¯¸
    'CA': ('en-CA', 'CA:en', 'Google News', 'Canada', 'Canada', 'ğŸ‡¨ğŸ‡¦', 'America/Toronto', '%Y-%m-%d %I:%M:%S %p (EST)'),
    'MX': ('es-419', 'MX:es-419', 'Google Noticias', 'MÃ©xico', 'Mexico', 'ğŸ‡²ğŸ‡½', 'America/Mexico_City', '%d/%m/%Y %H:%M:%S (CST)'),
    'US': ('en-US', 'US:en', 'Google News', 'United States', 'United States', 'ğŸ‡ºğŸ‡¸', 'America/New_York', '%Y-%m-%d %I:%M:%S %p (EST)'),
    'CU': ('es-419', 'CU:es-419', 'Google Noticias', 'Cuba', 'Cuba', 'ğŸ‡¨ğŸ‡º', 'America/Havana', '%d/%m/%Y %H:%M:%S (CST)'),

    # ë‚¨ë¯¸
    'AR': ('es-419', 'AR:es-419', 'Google Noticias', 'Argentina', 'Argentina', 'ğŸ‡¦ğŸ‡·', 'America/Buenos_Aires', '%d/%m/%Y %H:%M:%S (ART)'),
    'BR': ('pt-BR', 'BR:pt-419', 'Google NotÃ­cias', 'Brasil', 'Brazil', 'ğŸ‡§ğŸ‡·', 'America/Sao_Paulo', '%d/%m/%Y %H:%M:%S (BRT)'),
    'CL': ('es-419', 'CL:es-419', 'Google Noticias', 'Chile', 'Chile', 'ğŸ‡¨ğŸ‡±', 'America/Santiago', '%d-%m-%Y %H:%M:%S (CLT)'),
    'CO': ('es-419', 'CO:es-419', 'Google Noticias', 'Colombia', 'Colombia', 'ğŸ‡¨ğŸ‡´', 'America/Bogota', '%d/%m/%Y %I:%M:%S %p (COT)'),
    'PE': ('es-419', 'PE:es-419', 'Google Noticias', 'PerÃº', 'Peru', 'ğŸ‡µğŸ‡ª', 'America/Lima', '%d/%m/%Y %I:%M:%S %p (PET)'),
    'VE': ('es-419', 'VE:es-419', 'Google Noticias', 'Venezuela', 'Venezuela', 'ğŸ‡»ğŸ‡ª', 'America/Caracas', '%d/%m/%Y %I:%M:%S %p (VET)'),

    # ì•„í”„ë¦¬ì¹´
    'ZA': ('en-ZA', 'ZA:en', 'Google News', 'South Africa', 'South Africa', 'ğŸ‡¿ğŸ‡¦', 'Africa/Johannesburg', '%Y-%m-%d %H:%M:%S (SAST)'),
    'NG': ('en-NG', 'NG:en', 'Google News', 'Nigeria', 'Nigeria', 'ğŸ‡³ğŸ‡¬', 'Africa/Lagos', '%d/%m/%Y %I:%M:%S %p (WAT)'),
    'EG': ('ar', 'EG:ar', 'Ø£Ø®Ø¨Ø§Ø± Google', 'Ù…ØµØ±', 'Egypt', 'ğŸ‡ªğŸ‡¬', 'Africa/Cairo', '%d/%m/%Y %I:%M:%S %p (EET)'),
    'KE': ('en-KE', 'KE:en', 'Google News', 'Kenya', 'Kenya', 'ğŸ‡°ğŸ‡ª', 'Africa/Nairobi', '%d/%m/%Y %I:%M:%S %p (EAT)'),
    'MA': ('fr', 'MA:fr', 'Google ActualitÃ©s', 'Maroc', 'Morocco', 'ğŸ‡²ğŸ‡¦', 'Africa/Casablanca', '%d/%m/%Y %H:%M:%S (WET)'),
    'SN': ('fr', 'SN:fr', 'Google ActualitÃ©s', 'SÃ©nÃ©gal', 'Senegal', 'ğŸ‡¸ğŸ‡³', 'Africa/Dakar', '%d/%m/%Y %H:%M:%S (GMT)'),
    'UG': ('en-UG', 'UG:en', 'Google News', 'Uganda', 'Uganda', 'ğŸ‡ºğŸ‡¬', 'Africa/Kampala', '%d/%m/%Y %I:%M:%S %p (EAT)'),
    'TZ': ('en-TZ', 'TZ:en', 'Google News', 'Tanzania', 'Tanzania', 'ğŸ‡¹ğŸ‡¿', 'Africa/Dar_es_Salaam', '%d/%m/%Y %I:%M:%S %p (EAT)'),
    'ZW': ('en-ZW', 'ZW:en', 'Google News', 'Zimbabwe', 'Zimbabwe', 'ğŸ‡¿ğŸ‡¼', 'Africa/Harare', '%d/%m/%Y %I:%M:%S %p (CAT)'),
    'ET': ('en-ET', 'ET:en', 'Google News', 'Ethiopia', 'Ethiopia', 'ğŸ‡ªğŸ‡¹', 'Africa/Addis_Ababa', '%d/%m/%Y %I:%M:%S %p (EAT)'),
    'GH': ('en-GH', 'GH:en', 'Google News', 'Ghana', 'Ghana', 'ğŸ‡¬ğŸ‡­', 'Africa/Accra', '%d/%m/%Y %I:%M:%S %p (GMT)'),
}


def check_env_variables():
    """í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not DISCORD_WEBHOOK_KEYWORD:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK_KEYWORD")
    if KEYWORD_MODE and not KEYWORD:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆì§€ë§Œ KEYWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not KEYWORD_MODE and not RSS_URL_KEYWORD:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆì§€ë§Œ RSS_URL_KEYWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if AFTER_DATE and not is_valid_date(AFTER_DATE):
        raise ValueError("AFTER_DATE í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì´ ì•„ë‹™ë‹ˆë‹¤.")
    if BEFORE_DATE and not is_valid_date(BEFORE_DATE):
        raise ValueError("BEFORE_DATE í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì´ ì•„ë‹™ë‹ˆë‹¤.")
    if WHEN and (AFTER_DATE or BEFORE_DATE):
        logging.error("WHENê³¼ AFTER_DATE/BEFORE_DATEëŠ” í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. WHENì„ ì‚¬ìš©í•˜ê±°ë‚˜ AFTER_DATE/BEFORE_DATEë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        raise ValueError("ì˜ëª»ëœ ë‚ ì§œ ì¿¼ë¦¬ ì¡°í•©ì…ë‹ˆë‹¤.")
    if GL and not country_configs.get(GL):
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆì§€ë§Œ GL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if ADVANCED_FILTER_KEYWORD:
        logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {ADVANCED_FILTER_KEYWORD}")
    if DATE_FILTER_KEYWORD:
        logging.info(f"ë‚ ì§œ í•„í„°ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {DATE_FILTER_KEYWORD}")

def is_valid_date(date_string):
    """ë‚ ì§œ ë¬¸ìì—´ì´ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        datetime.strptime(date_string, '%Y-%m-%d')
        return True
    except ValueError:
        return False

def init_db(reset=False):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        try:
            if reset:
                c.execute("DROP TABLE IF EXISTS news_items")
                logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œë¨")
            
            c.execute('''CREATE TABLE IF NOT EXISTS news_items
                         (pub_date TEXT,
                          guid TEXT PRIMARY KEY,
                          title TEXT,
                          link TEXT,
                          related_news TEXT)''')
            
            c.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_guid ON news_items(guid)")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ë¬´ê²°ì„± ê²€ì‚¬
            c.execute("PRAGMA integrity_check")
            integrity_result = c.fetchone()[0]
            if integrity_result != "ok":
                logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨: {integrity_result}")
                raise sqlite3.IntegrityError("ë°ì´í„°ë² ì´ìŠ¤ ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨")
            
            # í…Œì´ë¸”ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            c.execute("SELECT COUNT(*) FROM news_items")
            count = c.fetchone()[0]
            
            if reset or count == 0:
                logging.info("ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logging.info(f"ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í˜„ì¬ {count}ê°œì˜ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤.")
            
        except sqlite3.Error as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    logging.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

def is_guid_posted(guid, conn):
    """ì£¼ì–´ì§„ GUIDê°€ ì´ë¯¸ ê²Œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        c = conn.cursor()
        c.execute("SELECT 1 FROM news_items WHERE guid = ?", (guid,))
        result = c.fetchone() is not None
        logging.info(f"GUID {guid} í™•ì¸ ê²°ê³¼: {'ì´ë¯¸ ê²Œì‹œë¨' if result else 'ìƒˆë¡œìš´ í•­ëª©'}")
        return result
    except sqlite3.Error as e:
        logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ (GUID í™•ì¸ ì¤‘): {e}")
        return False

def save_news_item(pub_date, guid, title, link, related_news):
    """ë‰´ìŠ¤ í•­ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” êµ¬ì¡° í™•ì¸
        c.execute("PRAGMA table_info(news_items)")
        columns = [column[1] for column in c.fetchall()]
        
        # ê´€ë ¨ ë‰´ìŠ¤ í•­ëª© ìˆ˜ í™•ì¸
        related_news_items = json.loads(related_news)
        related_news_count = len(related_news_items)
        
        # í•„ìš”í•œ ì—´ ì¶”ê°€
        for i in range(related_news_count):
            title_col = f"related_title_{i+1}"
            press_col = f"related_press_{i+1}"
            link_col = f"related_link_{i+1}"
            
            if title_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {title_col} TEXT")
            if press_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {press_col} TEXT")
            if link_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {link_col} TEXT")
        
        # ë°ì´í„° ì‚½ì…ì„ ìœ„í•œ SQL ì¿¼ë¦¬ ì¤€ë¹„
        columns = ["pub_date", "guid", "title", "link", "related_news"]
        values = [pub_date, guid, title, link, related_news]
        
        for i, item in enumerate(related_news_items):
            columns.extend([f"related_title_{i+1}", f"related_press_{i+1}", f"related_link_{i+1}"])
            values.extend([item.get('title', ''), item.get('press', ''), item.get('link', '')])
        
        placeholders = ", ".join(["?" for _ in values])
        columns_str = ", ".join(columns)
        
        c.execute(f"INSERT OR REPLACE INTO news_items ({columns_str}) VALUES ({placeholders})", values)
        
        logging.info(f"ë‰´ìŠ¤ í•­ëª© ì €ì¥/ì—…ë°ì´íŠ¸: {guid}")

def fetch_decoded_batch_execute(id):
    s = (
        '[[["Fbv4je","[\\"garturlreq\\",[[\\"en-US\\",\\"US\\",[\\"FINANCE_TOP_INDICES\\",\\"WEB_TEST_1_0_0\\"],'
        'null,null,1,1,\\"US:en\\",null,180,null,null,null,null,null,0,null,null,[1608992183,723341000]],'
        '\\"en-US\\",\\"US\\",1,[2,3,4,8],1,0,\\"655000234\\",0,0,null,0],\\"' +
        id +
        '\\"]",null,"generic"]]]'
    )

    headers = {
        "Content-Type": "application/x-www-form-urlencoded;charset=utf-8",
        "Referer": "https://news.google.com/"
    }

    response = requests.post(
        "https://news.google.com/_/DotsSplashUi/data/batchexecute?rpcids=Fbv4je",
        headers=headers,
        data={"f.req": s}
    )

    if response.status_code != 200:
        raise Exception("Failed to fetch data from Google.")

    text = response.text
    header = '[\\"garturlres\\",\\"'
    footer = '\\",'
    if header not in text:
        raise Exception(f"Header not found in response: {text}")
    start = text.split(header, 1)[1]
    if footer not in start:
        raise Exception("Footer not found in response.")
    url = start.split(footer, 1)[0]
    return url

def decode_base64_url_part(encoded_str):
    base64_str = encoded_str.replace("-", "+").replace("_", "/")
    base64_str += "=" * ((4 - len(base64_str) % 4) % 4)
    try:
        decoded_bytes = base64.urlsafe_b64decode(base64_str)
        decoded_str = decoded_bytes.decode('latin1')
        return decoded_str
    except Exception as e:
        return f"ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def extract_youtube_id(decoded_str):
    pattern = r'\x08 "\x0b([\w-]{11})\x98\x01\x01'
    match = re.search(pattern, decoded_str)
    if match:
        return match.group(1)
    return None

def extract_regular_url(decoded_str):
    """ë””ì½”ë”©ëœ ë¬¸ìì—´ì—ì„œ ì¼ë°˜ URL ì¶”ì¶œ"""
    parts = re.split(r'[^\x20-\x7E]+', decoded_str)
    url_pattern = r'(https?://[^\s]+)'
    for part in parts:
        match = re.search(url_pattern, part)
        if match:
            return match.group(0)
    return None

def unescape_unicode(text):
    """ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ë¥¼ ì‹¤ì œ ë¬¸ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return re.sub(
        r'\\u([0-9a-fA-F]{4})',
        lambda m: chr(int(m.group(1), 16)),
        text
    )

def clean_url(url):
    """URLì„ ì •ë¦¬í•˜ê³  ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    # ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ ì²˜ë¦¬
    url = unescape_unicode(url)
    
    # ë°±ìŠ¬ë˜ì‹œë¥¼ ì •ë¦¬
    url = url.replace('\\', '')
    
    # URL ë””ì½”ë”© (ì˜ˆ: %2F -> /, %40 -> @ ë“±)
    url = unquote(url)

    parsed_url = urlparse(url)
    
    # MSN ë§í¬ íŠ¹ë³„ ì²˜ë¦¬: HTTPSë¡œ ë³€í™˜ ë° ë¶ˆí•„ìš”í•œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
    if parsed_url.netloc.endswith('msn.com'):
        parsed_url = parsed_url._replace(scheme='https')
        query_params = parse_qs(parsed_url.query)
        cleaned_params = {k: v[0] for k, v in query_params.items() if k in ['id', 'article']}
        cleaned_query = urlencode(cleaned_params)
        parsed_url = parsed_url._replace(query=cleaned_query)
    
    # ê³µë°± ë“± ë¹„ì •ìƒì ì¸ ë¬¸ì ì²˜ë¦¬
    # safe íŒŒë¼ë¯¸í„°ì— íŠ¹ìˆ˜ ë¬¸ìë“¤ì„ í¬í•¨í•˜ì—¬ ì¸ì½”ë”©ë˜ì§€ ì•Šë„ë¡ ì„¤ì •
    safe_chars = "/:@&=+$,?#"
    cleaned_path = quote(parsed_url.path, safe=safe_chars)
    cleaned_query = quote(parsed_url.query, safe=safe_chars)
    
    # URL ì¬êµ¬ì„±
    cleaned_url = urlunparse(parsed_url._replace(path=cleaned_path, query=cleaned_query))
    
    return cleaned_url

def decode_google_news_url(source_url):
    url = urlparse(source_url)
    path = url.path.split("/")
    if url.hostname == "news.google.com" and len(path) > 1 and path[-2] == "articles":
        base64_str = path[-1]
        
        # ë¨¼ì € ìƒˆë¡œìš´ ë°©ì‹ ì‹œë„
        try:
            decoded_bytes = base64.urlsafe_b64decode(base64_str + '==')
            decoded_str = decoded_bytes.decode('latin1')

            prefix = b'\x08\x13\x22'.decode('latin1')
            if decoded_str.startswith(prefix):
                decoded_str = decoded_str[len(prefix):]

            suffix = b'\xd2\x01\x00'.decode('latin1')
            if decoded_str.endswith(suffix):
                decoded_str = decoded_str[:-len(suffix)]

            bytes_array = bytearray(decoded_str, 'latin1')
            length = bytes_array[0]
            if length >= 0x80:
                decoded_str = decoded_str[2:length+1]
            else:
                decoded_str = decoded_str[1:length+1]

            if decoded_str.startswith("AU_yqL"):
                return clean_url(fetch_decoded_batch_execute(base64_str))

            regular_url = extract_regular_url(decoded_str)
            if regular_url:
                return clean_url(regular_url)
        except Exception:
            pass  # ìƒˆë¡œìš´ ë°©ì‹ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ì¡´ ë°©ì‹ ì‹œë„

        # ê¸°ì¡´ ë°©ì‹ ì‹œë„ (ìœ íŠœë¸Œ ë§í¬ í¬í•¨)
        decoded_str = decode_base64_url_part(base64_str)
        youtube_id = extract_youtube_id(decoded_str)
        if youtube_id:
            return f"https://www.youtube.com/watch?v={youtube_id}"

        regular_url = extract_regular_url(decoded_str)
        if regular_url:
            return clean_url(regular_url)

    return clean_url(source_url)  # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ì •ë¦¬ í›„ ë°˜í™˜

def get_original_url(google_link, session, max_retries=5):
    if ORIGIN_LINK_KEYWORD:
        original_url = decode_google_news_url(google_link)
        if original_url != google_link:
            return original_url

        # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ requests ë°©ì‹ ì‹œë„
        retries = 0
        while retries < max_retries:
            try:
                response = session.get(google_link, allow_redirects=True)
                if response.status_code == 200:
                    return clean_url(response.url)
            except requests.RequestException as e:
                logging.error(f"Failed to get original URL: {e}")
            retries += 1
        
        logging.warning(f"ì˜¤ë¦¬ì§€ë„ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return clean_url(google_link)
    else:
        logging.info(f"ORIGIN_LINK_KEYWORDê°€ False, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return clean_url(google_link)

def fetch_rss_feed(url, max_retries=3, retry_delay=5):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response.content
        except requests.RequestException as e:
            logging.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt + 1 < max_retries:
                time.sleep(retry_delay)
            else:
                logging.error(f"RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {url}")
                raise

def parse_rss_feed(rss_data):
    """RSS í”¼ë“œë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    try:
        root = ET.fromstring(rss_data)
        return root.findall('.//item')
    except ET.ParseError as e:
        logging.error(f"RSS ë°ì´í„° íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def get_rss_url():
    rss_base_url = "https://news.google.com/rss/search"
    
    if KEYWORD_MODE:
        encoded_keyword = requests.utils.quote(KEYWORD)
        query_params = [f"q={encoded_keyword}"]
        
        if WHEN:
            query_params[-1] += f"+when:{WHEN}"
        elif AFTER_DATE or BEFORE_DATE:
            if AFTER_DATE:
                query_params[-1] += f"+after:{AFTER_DATE}"
            if BEFORE_DATE:
                query_params[-1] += f"+before:{BEFORE_DATE}"
        
        query_string = "+".join(query_params)
        
        if HL and GL and CEID:
            rss_url = f"{rss_base_url}?{query_string}&hl={HL}&gl={GL}&ceid={CEID}"
        else:
            rss_url = f"{rss_base_url}?{query_string}&hl=ko&gl=KR&ceid=KR:ko"
        
        return rss_url, KEYWORD
    else:
        return RSS_URL_KEYWORD, None

def extract_rss_feed_keyword(title):
    """RSS í”¼ë“œ ì œëª©ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r'"([^"]+)', title)
    if match:
        keyword = match.group(1)
        if 'when:' in keyword:
            keyword = keyword.split('when:')[0].strip()
        return keyword
    return None

def replace_brackets(text):
    """ëŒ€ê´„í˜¸ì™€ êº¾ì‡ ê´„í˜¸ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
    text = text.replace('[', 'ï¼»').replace(']', 'ï¼½')
    text = text.replace('<', 'ã€ˆ').replace('>', 'ã€‰')
    text = re.sub(r'(?<!\s)(?<!^)ï¼»', ' ï¼»', text)
    text = re.sub(r'ï¼½(?!\s)', 'ï¼½ ', text)
    text = re.sub(r'(?<!\s)(?<!^)ã€ˆ', ' ã€ˆ', text)
    text = re.sub(r'ã€‰(?!\s)', 'ã€‰ ', text)
    return text

def convert_to_local_time(pub_date, country_code):
    try:
        utc_time = parsedate_to_datetime(pub_date)
    except ValueError:
        return pub_date

    _, _, _, _, _, _, _, timezone, date_format = country_configs.get(country_code, country_configs['US'])

    local_time = utc_time.astimezone(pytz.timezone(timezone))
    return local_time.strftime(date_format)

def parse_rss_date(pub_date, country_code='KR'):
    return convert_to_local_time(pub_date, country_code)

def send_discord_message(webhook_url, message, avatar_url=None, username=None, max_retries=3, retry_delay=5):
    """Discord ì›¹í›…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤. ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„í•©ë‹ˆë‹¤."""
    payload = {"content": message}
    
    if avatar_url and avatar_url.strip():
        payload["avatar_url"] = avatar_url
    
    if username and username.strip():
        payload["username"] = username
    
    headers = {"Content-Type": "application/json"}

    for attempt in range(max_retries):
        try:
            response = requests.post(webhook_url, json=payload, headers=headers)
            response.raise_for_status()
            logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
            return
        except requests.RequestException as e:
            if attempt < max_retries - 1:
                logging.warning(f"Discord ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                time.sleep(retry_delay)
            else:
                logging.error(f"Discord ë©”ì‹œì§€ ì „ì†¡ ìµœì¢… ì‹¤íŒ¨: {e}")
                raise

    time.sleep(3)  # ì„±ê³µì ì¸ ì „ì†¡ í›„ 3ì´ˆ ëŒ€ê¸°

def extract_news_items(description, session):
    """HTML ì„¤ëª…ì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(description, 'html.parser')
    news_items = []
    for li in soup.find_all('li'):
        a_tag = li.find('a')
        if a_tag:
            title = replace_brackets(a_tag.text)
            google_link = a_tag['href']
            link = get_original_url(google_link, session)
            press = li.find('font', color="#6f6f6f").text if li.find('font', color="#6f6f6f") else ""
            news_items.append({"title": title, "link": link, "press": press})
    return news_items

def parse_html_description(html_desc, session, main_title, main_link):
    """HTML ì„¤ëª…ì„ íŒŒì‹±í•˜ì—¬ ê´€ë ¨ ë‰´ìŠ¤ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    news_items = extract_news_items(html_desc, session)
    
    news_items = [item for item in news_items if item['title'] != main_title or item['link'] != main_link]
    
    if len(news_items) == 0:
        return "", []  # ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ë©”ì¸ ë‰´ìŠ¤ì™€ ë™ì¼í•œ ê²½ìš°
    elif len(news_items) == 1:
        return "", news_items  # ê´€ë ¨ ë‰´ìŠ¤ê°€ 1ê°œì¸ ê²½ìš° (í‘œì‹œí•˜ì§€ ì•ŠìŒ)
    else:
        news_string = '\n'.join([f"> - [{item['title']}]({item['link']}) | {item['press']}" for item in news_items])
        return news_string, news_items

def apply_advanced_filter(title, description, advanced_filter):
    """ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²Œì‹œë¬¼ì„ ì „ì†¡í• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    if not advanced_filter:
        return True

    text_to_check = (title + ' ' + description).lower()

    terms = re.findall(r'([+-]?)(?:"([^"]*)"|\S+)', advanced_filter)

    for prefix, term in terms:
        term = term.lower() if term else prefix.lower()
        if prefix == '+' or not prefix:  # í¬í•¨í•´ì•¼ í•˜ëŠ” ë‹¨ì–´
            if term not in text_to_check:
                return False
        elif prefix == '-':  # ì œì™¸í•´ì•¼ í•˜ëŠ” ë‹¨ì–´ ë˜ëŠ” êµ¬ë¬¸
            exclude_terms = term.split()
            if len(exclude_terms) > 1:
                if ' '.join(exclude_terms) in text_to_check:
                    return False
            else:
                if term in text_to_check:
                    return False

    return True

def parse_date_filter(filter_string):
    since_date = None
    until_date = None
    past_date = None

    logging.info(f"íŒŒì‹± ì¤‘ì¸ ë‚ ì§œ í•„í„° ë¬¸ìì—´: {filter_string}")

    if not filter_string:
        logging.warning("ë‚ ì§œ í•„í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return since_date, until_date, past_date

    since_match = re.search(r'since:(\d{4}-\d{2}-\d{2})', filter_string)
    until_match = re.search(r'until:(\d{4}-\d{2}-\d{2})', filter_string)
    
    if since_match:
        since_date = datetime.strptime(since_match.group(1), '%Y-%m-%d').replace(tzinfo=pytz.UTC)
        logging.info(f"since_date íŒŒì‹± ê²°ê³¼: {since_date}")
    if until_match:
        until_date = datetime.strptime(until_match.group(1), '%Y-%m-%d').replace(tzinfo=pytz.UTC)
        logging.info(f"until_date íŒŒì‹± ê²°ê³¼: {until_date}")

    past_match = re.search(r'past:(\d+)([hdmy])', filter_string)
    if past_match:
        value = int(past_match.group(1))
        unit = past_match.group(2)
        now = datetime.now(pytz.UTC)
        if unit == 'h':
            past_date = now - timedelta(hours=value)
        elif unit == 'd':
            past_date = now - timedelta(days=value)
        elif unit == 'm':
            past_date = now - timedelta(days=value*30)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        elif unit == 'y':
            past_date = now - timedelta(days=value*365)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        logging.info(f"past_date íŒŒì‹± ê²°ê³¼: {past_date}")
    else:
        logging.warning("past: í˜•ì‹ì˜ ë‚ ì§œ í•„í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    logging.info(f"ìµœì¢… íŒŒì‹± ê²°ê³¼ - since_date: {since_date}, until_date: {until_date}, past_date: {past_date}")
    return since_date, until_date, past_date

def is_within_date_range(pub_date, since_date, until_date, past_date):
    pub_datetime = parser.parse(pub_date).replace(tzinfo=pytz.UTC)
    now = datetime.now(pytz.UTC)
    
    logging.info(f"ê²€ì‚¬ ì¤‘ì¸ ê¸°ì‚¬ ë‚ ì§œ: {pub_datetime}")
    logging.info(f"í˜„ì¬ ë‚ ì§œ: {now}")
    logging.info(f"ì„¤ì •ëœ í•„í„° - since_date: {since_date}, until_date: {until_date}, past_date: {past_date}")

    if past_date:
        result = pub_datetime >= past_date
        logging.info(f"past_date í•„í„° ì ìš© ê²°ê³¼: {result}")
        return result
    
    if since_date and pub_datetime < since_date:
        logging.info(f"since_date í•„í„°ì— ì˜í•´ ì œì™¸ë¨")
        return False
    if until_date and pub_datetime > until_date:
        logging.info(f"until_date í•„í„°ì— ì˜í•´ ì œì™¸ë¨")
        return False
    
    logging.info(f"ëª¨ë“  ë‚ ì§œ í•„í„°ë¥¼ í†µê³¼í•¨")
    return True

def get_rss_url():
    rss_base_url = "https://news.google.com/rss/search"
    
    if os.getenv('KEYWORD_MODE') == 'true':
        keyword = os.getenv('KEYWORD')
        encoded_keyword = requests.utils.quote(keyword)
        query_params = [f"q={encoded_keyword}"]
        
        if os.getenv('WHEN'):
            query_params[-1] += f"+when:{os.getenv('WHEN')}"
        elif os.getenv('AFTER_DATE') or os.getenv('BEFORE_DATE'):
            if os.getenv('AFTER_DATE'):
                query_params[-1] += f"+after:{os.getenv('AFTER_DATE')}"
            if os.getenv('BEFORE_DATE'):
                query_params[-1] += f"+before:{os.getenv('BEFORE_DATE')}"
        
        query_string = "+".join(query_params)
        
        country_code = os.getenv('GL', 'KR')
        hl, ceid = country_configs.get(country_code, country_configs['US'])[:2]
        
        rss_url = f"{rss_base_url}?{query_string}&hl={hl}&gl={country_code}&ceid={ceid}"
        return rss_url, keyword, country_code
    else:
        return os.getenv('RSS_URL_KEYWORD'), None, 'KR'

def main():
    try:
        rss_url, keyword, country_code = get_rss_url()
        
        logging.info(f"RSS í”¼ë“œ URL: {rss_url}")
        logging.debug(f"ORIGIN_LINK_KEYWORD ê°’: {ORIGIN_LINK_KEYWORD}")

        rss_data = fetch_rss_feed(rss_url)
        news_items = parse_rss_feed(rss_data)
        
        total_items = len(news_items)
        logging.info(f"ì´ {total_items}ê°œì˜ ë‰´ìŠ¤ í•­ëª©ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

        init_db(reset=INITIALIZE_KEYWORD)

        session = requests.Session()
        
        if INITIALIZE_KEYWORD:
            news_items = sorted(news_items, key=lambda item: parser.parse(item.find('pubDate').text))
            logging.info("ì´ˆê¸° ì‹¤í–‰: ë‰´ìŠ¤ í•­ëª©ì„ ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬í–ˆìŠµë‹ˆë‹¤.")
        else:
            with sqlite3.connect(DB_PATH) as conn:
                new_items = [item for item in reversed(news_items) if not is_guid_posted(item.find('guid').text, conn)]
            news_items = new_items
            logging.info(f"í›„ì† ì‹¤í–‰: {len(news_items)}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ í•­ëª©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        if not news_items:
            logging.info("ì²˜ë¦¬í•  ìƒˆë¡œìš´ ë‰´ìŠ¤ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        since_date, until_date, past_date = parse_date_filter(DATE_FILTER_KEYWORD)
        logging.debug(f"ì ìš©ëœ ë‚ ì§œ í•„í„° - since: {since_date}, until: {until_date}, past: {past_date}")

        hl, ceid, google_news, country_name, country_name_en, flag, timezone, date_format = country_configs.get(country_code, country_configs['US'])

        processed_count = 0
        for item in news_items:
            try:
                guid = item.find('guid').text
                pub_date = item.find('pubDate').text
                if not is_within_date_range(pub_date, since_date, until_date, past_date):
                    logging.debug(f"ë‚ ì§œ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {item.find('title').text}")
                    continue

                title = replace_brackets(item.find('title').text)
                google_link = item.find('link').text
                link = get_original_url(google_link, session)
                description_html = item.find('description').text

                description, related_news = parse_html_description(description_html, session, title, link)

                if not apply_advanced_filter(title, description, ADVANCED_FILTER_KEYWORD):
                    logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
                    continue

                formatted_date = convert_to_local_time(pub_date, country_code)

                discord_message = f"`{google_news} - {keyword} - {country_name} {flag}`\n**{title}**\n{link}"
                if description:
                    discord_message += f"\n{description}"
                discord_message += f"\n\nğŸ“… {formatted_date}"

                send_discord_message(
                    DISCORD_WEBHOOK_KEYWORD,
                    discord_message,
                    avatar_url=DISCORD_AVATAR_KEYWORD,
                    username=DISCORD_USERNAME_KEYWORD
                )

                save_news_item(pub_date, guid, title, link, json.dumps(related_news, ensure_ascii=False))

                processed_count += 1
                logging.info(f"ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬ ì™„ë£Œ: {title}")

            except Exception as e:
                logging.error(f"ë‰´ìŠ¤ í•­ëª© '{item.find('title').text if item.find('title') is not None else 'Unknown'}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                continue

        logging.info(f"ì´ {processed_count}ê°œì˜ ë‰´ìŠ¤ í•­ëª©ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logging.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")
