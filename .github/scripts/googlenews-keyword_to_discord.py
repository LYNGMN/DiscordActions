import xml.etree.ElementTree as ET
import requests
import re
import os
import time
import random
import logging
import json
import base64
import sqlite3
import sys
import pytz
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode, unquote, quote
from datetime import datetime, timedelta
from dateutil import parser
from dateutil.tz import gettz
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK_KEYWORD = os.environ.get('DISCORD_WEBHOOK_KEYWORD')
DISCORD_AVATAR_KEYWORD = os.environ.get('DISCORD_AVATAR_KEYWORD')
DISCORD_USERNAME_KEYWORD = os.environ.get('DISCORD_USERNAME_KEYWORD')
INITIALIZE_KEYWORD = os.environ.get('INITIALIZE_MODE_KEYWORD', 'false').lower() == 'true'
KEYWORD_MODE = os.environ.get('KEYWORD_MODE', 'false').lower() == 'true'
KEYWORD = os.environ.get('KEYWORD', '')
RSS_URL_KEYWORD = os.environ.get('RSS_URL_KEYWORD', '')
AFTER_DATE = os.environ.get('AFTER_DATE', '')
BEFORE_DATE = os.environ.get('BEFORE_DATE', '')
WHEN = os.environ.get('WHEN', '')
HL = os.environ.get('HL', '')
GL = os.environ.get('GL', '')
CEID = os.environ.get('CEID', '')
ADVANCED_FILTER_KEYWORD = os.environ.get('ADVANCED_FILTER_KEYWORD', '')
DATE_FILTER_KEYWORD = os.environ.get('DATE_FILTER_KEYWORD', '')
ORIGIN_LINK_KEYWORD = os.getenv('ORIGIN_LINK_KEYWORD', '').lower()
ORIGIN_LINK_KEYWORD = ORIGIN_LINK_KEYWORD not in ['false', 'f', '0', 'no', 'n']

# DB ì„¤ì •
DB_PATH = 'google_news_keyword.db'

def check_env_variables():
    """í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not DISCORD_WEBHOOK_KEYWORD:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK_KEYWORD")
    if KEYWORD_MODE and not KEYWORD:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆì§€ë§Œ KEYWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not KEYWORD_MODE and not RSS_URL_KEYWORD:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆì§€ë§Œ RSS_URL_KEYWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if AFTER_DATE and not is_valid_date(AFTER_DATE):
        raise ValueError("AFTER_DATE í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì´ ì•„ë‹™ë‹ˆë‹¤.")
    if BEFORE_DATE and not is_valid_date(BEFORE_DATE):
        raise ValueError("BEFORE_DATE í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì´ ì•„ë‹™ë‹ˆë‹¤.")
    if WHEN and (AFTER_DATE or BEFORE_DATE):
        logging.error("WHENê³¼ AFTER_DATE/BEFORE_DATEëŠ” í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. WHENì„ ì‚¬ìš©í•˜ê±°ë‚˜ AFTER_DATE/BEFORE_DATEë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        raise ValueError("ì˜ëª»ëœ ë‚ ì§œ ì¿¼ë¦¬ ì¡°í•©ì…ë‹ˆë‹¤.")
    if (HL or GL or CEID) and not (HL and GL and CEID):
        raise ValueError("HL, GL, CEID í™˜ê²½ ë³€ìˆ˜ëŠ” ëª¨ë‘ ì„¤ì •ë˜ê±°ë‚˜ ëª¨ë‘ ì„¤ì •ë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤.")
    if ADVANCED_FILTER_KEYWORD:
        logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {ADVANCED_FILTER_KEYWORD}")
    if DATE_FILTER_KEYWORD:
        logging.info(f"ë‚ ì§œ í•„í„°ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {DATE_FILTER_KEYWORD}")

def is_valid_date(date_string):
    """ë‚ ì§œ ë¬¸ìì—´ì´ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        datetime.strptime(date_string, '%Y-%m-%d')
        return True
    except ValueError:
        return False

def replace_brackets(text):
    """ëŒ€ê´„í˜¸ì™€ êº¾ì‡ ê´„í˜¸ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
    text = text.replace('[', 'ï¼»').replace(']', 'ï¼½')
    text = text.replace('<', 'ã€ˆ').replace('>', 'ã€‰')
    text = re.sub(r'(?<!\s)(?<!^)ï¼»', ' ï¼»', text)
    text = re.sub(r'ï¼½(?!\s)', 'ï¼½ ', text)
    text = re.sub(r'(?<!\s)(?<!^)ã€ˆ', ' ã€ˆ', text)
    text = re.sub(r'ã€‰(?!\s)', 'ã€‰ ', text)
    return text

def parse_rss_date(pub_date):
    """RSS ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ í˜•ì‹í™”ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dt = parser.parse(pub_date)
    dt_kst = dt.astimezone(gettz('Asia/Seoul'))
    return dt_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')
	
def init_db(reset=False):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        try:
            if reset:
                c.execute("DROP TABLE IF EXISTS news_items")
                logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œë¨")
            
            c.execute('''CREATE TABLE IF NOT EXISTS news_items
                         (pub_date TEXT,
                          guid TEXT PRIMARY KEY,
                          title TEXT,
                          link TEXT,
                          topic TEXT,
                          related_news TEXT)''')
            
            # í…Œì´ë¸”ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            c.execute("SELECT COUNT(*) FROM news_items")
            count = c.fetchone()[0]
            
            if reset or count == 0:
                logging.info("ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logging.info(f"ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í˜„ì¬ {count}ê°œì˜ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤.")
            
        except sqlite3.Error as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

def is_guid_posted(guid):
    try:
        with sqlite3.connect(DB_PATH) as conn:
            c = conn.cursor()
            c.execute("SELECT 1 FROM news_items WHERE guid = ?", (guid,))
            result = c.fetchone() is not None
            logging.info(f"GUID {guid} í™•ì¸ ê²°ê³¼: {'ì´ë¯¸ ê²Œì‹œë¨' if result else 'ìƒˆë¡œìš´ í•­ëª©'}")
            return result
    except sqlite3.Error as e:
        logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ (GUID í™•ì¸ ì¤‘): {e}")
        return False

def save_news_item(pub_date, guid, title, link, related_news):
    """ë‰´ìŠ¤ í•­ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” êµ¬ì¡° í™•ì¸
        c.execute("PRAGMA table_info(news_items)")
        columns = [column[1] for column in c.fetchall()]
        
        # ê´€ë ¨ ë‰´ìŠ¤ í•­ëª© ìˆ˜ í™•ì¸
        related_news_items = json.loads(related_news)
        related_news_count = len(related_news_items)
        
        # í•„ìš”í•œ ì—´ ì¶”ê°€
        for i in range(related_news_count):
            title_col = f"related_title_{i+1}"
            press_col = f"related_press_{i+1}"
            link_col = f"related_link_{i+1}"
            
            if title_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {title_col} TEXT")
            if press_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {press_col} TEXT")
            if link_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {link_col} TEXT")
        
        # ë°ì´í„° ì‚½ì…ì„ ìœ„í•œ SQL ì¿¼ë¦¬ ì¤€ë¹„
        columns = ["pub_date", "guid", "title", "link", "related_news"]
        values = [pub_date, guid, title, link, related_news]
        
        for i, item in enumerate(related_news_items):
            columns.extend([f"related_title_{i+1}", f"related_press_{i+1}", f"related_link_{i+1}"])
            values.extend([item.get('title', ''), item.get('press', ''), item.get('link', '')])
        
        placeholders = ", ".join(["?" for _ in values])
        columns_str = ", ".join(columns)
        
        c.execute(f"INSERT OR REPLACE INTO news_items ({columns_str}) VALUES ({placeholders})", values)
        
        logging.info(f"ë‰´ìŠ¤ í•­ëª© ì €ì¥/ì—…ë°ì´íŠ¸: {guid}")
		
def fetch_decoded_batch_execute(id):
    s = (
        '[[["Fbv4je","[\\"garturlreq\\",[[\\"en-US\\",\\"US\\",[\\"FINANCE_TOP_INDICES\\",\\"WEB_TEST_1_0_0\\"],'
        'null,null,1,1,\\"US:en\\",null,180,null,null,null,null,null,0,null,null,[1608992183,723341000]],'
        '\\"en-US\\",\\"US\\",1,[2,3,4,8],1,0,\\"655000234\\",0,0,null,0],\\"' +
        id +
        '\\"]",null,"generic"]]]'
    )

    headers = {
        "Content-Type": "application/x-www-form-urlencoded;charset=utf-8",
        "Referer": "https://news.google.com/"
    }

    response = requests.post(
        "https://news.google.com/_/DotsSplashUi/data/batchexecute?rpcids=Fbv4je",
        headers=headers,
        data={"f.req": s}
    )

    if response.status_code != 200:
        raise Exception("Failed to fetch data from Google.")

    text = response.text
    header = '[\\"garturlres\\",\\"'
    footer = '\\",'
    if header not in text:
        raise Exception(f"Header not found in response: {text}")
    start = text.split(header, 1)[1]
    if footer not in start:
        raise Exception("Footer not found in response.")
    url = start.split(footer, 1)[0]
    return url

def decode_base64_url_part(encoded_str):
    base64_str = encoded_str.replace("-", "+").replace("_", "/")
    base64_str += "=" * ((4 - len(base64_str) % 4) % 4)
    try:
        decoded_bytes = base64.urlsafe_b64decode(base64_str)
        decoded_str = decoded_bytes.decode('latin1')
        return decoded_str
    except Exception as e:
        return f"ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def extract_youtube_id(decoded_str):
    pattern = r'\x08 "\x0b([\w-]{11})\x98\x01\x01'
    match = re.search(pattern, decoded_str)
    if match:
        return match.group(1)
    return None

def extract_regular_url(decoded_str):
    """ë””ì½”ë”©ëœ ë¬¸ìì—´ì—ì„œ ì¼ë°˜ URL ì¶”ì¶œ"""
    parts = re.split(r'[^\x20-\x7E]+', decoded_str)
    url_pattern = r'(https?://[^\s]+)'
    for part in parts:
        match = re.search(url_pattern, part)
        if match:
            return match.group(0)
    return None

def unescape_unicode(text):
    """ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ë¥¼ ì‹¤ì œ ë¬¸ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return re.sub(
        r'\\u([0-9a-fA-F]{4})',
        lambda m: chr(int(m.group(1), 16)),
        text
    )

def clean_url(url):
    """URLì„ ì •ë¦¬í•˜ê³  ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    # ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ ì²˜ë¦¬
    url = unescape_unicode(url)
    
    # ë°±ìŠ¬ë˜ì‹œë¥¼ ì •ë¦¬
    url = url.replace('\\', '')
    
    # URL ë””ì½”ë”© (ì˜ˆ: %2F -> /, %40 -> @ ë“±)
    url = unquote(url)

    parsed_url = urlparse(url)
    
    # MSN ë§í¬ íŠ¹ë³„ ì²˜ë¦¬: HTTPSë¡œ ë³€í™˜ ë° ë¶ˆí•„ìš”í•œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
    if parsed_url.netloc.endswith('msn.com'):
        parsed_url = parsed_url._replace(scheme='https')
        query_params = parse_qs(parsed_url.query)
        cleaned_params = {k: v[0] for k, v in query_params.items() if k in ['id', 'article']}
        cleaned_query = urlencode(cleaned_params)
        parsed_url = parsed_url._replace(query=cleaned_query)
    
    # ê³µë°± ë“± ë¹„ì •ìƒì ì¸ ë¬¸ì ì²˜ë¦¬
    # safe íŒŒë¼ë¯¸í„°ì— íŠ¹ìˆ˜ ë¬¸ìë“¤ì„ í¬í•¨í•˜ì—¬ ì¸ì½”ë”©ë˜ì§€ ì•Šë„ë¡ ì„¤ì •
    safe_chars = "/:@&=+$,?#"
    cleaned_path = quote(parsed_url.path, safe=safe_chars)
    cleaned_query = quote(parsed_url.query, safe=safe_chars)
    
    # URL ì¬êµ¬ì„±
    cleaned_url = urlunparse(parsed_url._replace(path=cleaned_path, query=cleaned_query))
    
    return cleaned_url

def decode_google_news_url(source_url):
    url = urlparse(source_url)
    path = url.path.split("/")
    if url.hostname == "news.google.com" and len(path) > 1 and path[-2] == "articles":
        base64_str = path[-1]
        
        # ë¨¼ì € ìƒˆë¡œìš´ ë°©ì‹ ì‹œë„
        try:
            decoded_bytes = base64.urlsafe_b64decode(base64_str + '==')
            decoded_str = decoded_bytes.decode('latin1')

            prefix = b'\x08\x13\x22'.decode('latin1')
            if decoded_str.startswith(prefix):
                decoded_str = decoded_str[len(prefix):]

            suffix = b'\xd2\x01\x00'.decode('latin1')
            if decoded_str.endswith(suffix):
                decoded_str = decoded_str[:-len(suffix)]

            bytes_array = bytearray(decoded_str, 'latin1')
            length = bytes_array[0]
            if length >= 0x80:
                decoded_str = decoded_str[2:length+1]
            else:
                decoded_str = decoded_str[1:length+1]

            if decoded_str.startswith("AU_yqL"):
                return clean_url(fetch_decoded_batch_execute(base64_str))

            regular_url = extract_regular_url(decoded_str)
            if regular_url:
                return clean_url(regular_url)
        except Exception:
            pass  # ìƒˆë¡œìš´ ë°©ì‹ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ì¡´ ë°©ì‹ ì‹œë„

        # ê¸°ì¡´ ë°©ì‹ ì‹œë„ (ìœ íŠœë¸Œ ë§í¬ í¬í•¨)
        decoded_str = decode_base64_url_part(base64_str)
        youtube_id = extract_youtube_id(decoded_str)
        if youtube_id:
            return f"https://www.youtube.com/watch?v={youtube_id}"

        regular_url = extract_regular_url(decoded_str)
        if regular_url:
            return clean_url(regular_url)

    return clean_url(source_url)  # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ì •ë¦¬ í›„ ë°˜í™˜

def get_original_url(google_link, session, max_retries=5):
    if ORIGIN_LINK_KEYWORD:
        original_url = decode_google_news_url(google_link)
        if original_url != google_link:
            return original_url

        # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ requests ë°©ì‹ ì‹œë„
        retries = 0
        while retries < max_retries:
            try:
                response = session.get(google_link, allow_redirects=True)
                if response.status_code == 200:
                    return clean_url(response.url)
            except requests.RequestException as e:
                logging.error(f"Failed to get original URL: {e}")
            retries += 1
        
        logging.warning(f"ì˜¤ë¦¬ì§€ë„ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return clean_url(google_link)
    else:
        logging.info(f"ORIGIN_LINK_KEYWORDê°€ False, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return clean_url(google_link)

def fetch_rss_feed(url):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    response = requests.get(url)
    return response.content

def send_discord_message(webhook_url, message, avatar_url=None, username=None):
    """Discord ì›¹í›…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
    payload = {"content": message}
    
    if avatar_url and avatar_url.strip():
        payload["avatar_url"] = avatar_url
    
    if username and username.strip():
        payload["username"] = username
    
    headers = {"Content-Type": "application/json"}
    response = requests.post(webhook_url, json=payload, headers=headers)
    if response.status_code != 204:
        logging.error(f"Discordì— ë©”ì‹œì§€ë¥¼ ê²Œì‹œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        logging.error(response.text)
    else:
        logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
    time.sleep(3)

def extract_news_items(description, session):
    """HTML ì„¤ëª…ì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(description, 'html.parser')
    news_items = []
    for li in soup.find_all('li'):
        a_tag = li.find('a')
        if a_tag:
            title = replace_brackets(a_tag.text)
            google_link = a_tag['href']
            link = get_original_url(google_link, session)
            press = li.find('font', color="#6f6f6f").text if li.find('font', color="#6f6f6f") else ""
            news_items.append({"title": title, "link": link, "press": press})
    return news_items

def parse_html_description(html_desc, session, main_title, main_link):
    """HTML ì„¤ëª…ì„ íŒŒì‹±í•˜ì—¬ ê´€ë ¨ ë‰´ìŠ¤ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    news_items = extract_news_items(html_desc, session)
    
    news_items = [item for item in news_items if item['title'] != main_title or item['link'] != main_link]
    
    if len(news_items) == 0:
        return "", []  # ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ë©”ì¸ ë‰´ìŠ¤ì™€ ë™ì¼í•œ ê²½ìš°
    elif len(news_items) == 1:
        return "", news_items  # ê´€ë ¨ ë‰´ìŠ¤ê°€ 1ê°œì¸ ê²½ìš° (í‘œì‹œí•˜ì§€ ì•ŠìŒ)
    else:
        news_string = '\n'.join([f"> - [{item['title']}]({item['link']}) | {item['press']}" for item in news_items])
        return news_string, news_items

def extract_rss_feed_category(title):
    """RSS í”¼ë“œ ì œëª©ì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r'"([^"]+)', title)
    if match:
        category = match.group(1)
        if 'when:' in category:
            category = category.split('when:')[0].strip()
        return category
    return "ë””ìŠ¤ì½”ë“œ"

def apply_advanced_filter(title, description, advanced_filter):
    """ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²Œì‹œë¬¼ì„ ì „ì†¡í• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    if not advanced_filter:
        return True

    text_to_check = (title + ' ' + description).lower()

    terms = re.findall(r'([+-]?)(?:"([^"]*)"|\S+)', advanced_filter)

    for prefix, term in terms:
        term = term.lower() if term else prefix.lower()
        if prefix == '+' or not prefix:  # í¬í•¨í•´ì•¼ í•˜ëŠ” ë‹¨ì–´
            if term not in text_to_check:
                return False
        elif prefix == '-':  # ì œì™¸í•´ì•¼ í•˜ëŠ” ë‹¨ì–´ ë˜ëŠ” êµ¬ë¬¸
            exclude_terms = term.split()
            if len(exclude_terms) > 1:
                if ' '.join(exclude_terms) in text_to_check:
                    return False
            else:
                if term in text_to_check:
                    return False

    return True

def parse_date_filter(filter_string):
    since_date = None
    until_date = None
    past_date = None

    logging.info(f"íŒŒì‹± ì¤‘ì¸ ë‚ ì§œ í•„í„° ë¬¸ìì—´: {filter_string}")

    if not filter_string:
        logging.warning("ë‚ ì§œ í•„í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return since_date, until_date, past_date

    since_match = re.search(r'since:(\d{4}-\d{2}-\d{2})', filter_string)
    until_match = re.search(r'until:(\d{4}-\d{2}-\d{2})', filter_string)
    
    if since_match:
        since_date = datetime.strptime(since_match.group(1), '%Y-%m-%d').replace(tzinfo=pytz.UTC)
        logging.info(f"since_date íŒŒì‹± ê²°ê³¼: {since_date}")
    if until_match:
        until_date = datetime.strptime(until_match.group(1), '%Y-%m-%d').replace(tzinfo=pytz.UTC)
        logging.info(f"until_date íŒŒì‹± ê²°ê³¼: {until_date}")

    past_match = re.search(r'past:(\d+)([hdmy])', filter_string)
    if past_match:
        value = int(past_match.group(1))
        unit = past_match.group(2)
        now = datetime.now(pytz.UTC)
        if unit == 'h':
            past_date = now - timedelta(hours=value)
        elif unit == 'd':
            past_date = now - timedelta(days=value)
        elif unit == 'm':
            past_date = now - timedelta(days=value*30)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        elif unit == 'y':
            past_date = now - timedelta(days=value*365)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        logging.info(f"past_date íŒŒì‹± ê²°ê³¼: {past_date}")
    else:
        logging.warning("past: í˜•ì‹ì˜ ë‚ ì§œ í•„í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    logging.info(f"ìµœì¢… íŒŒì‹± ê²°ê³¼ - since_date: {since_date}, until_date: {until_date}, past_date: {past_date}")
    return since_date, until_date, past_date

def is_within_date_range(pub_date, since_date, until_date, past_date):
    pub_datetime = parser.parse(pub_date).replace(tzinfo=pytz.UTC)
    now = datetime.now(pytz.UTC)
    
    logging.info(f"ê²€ì‚¬ ì¤‘ì¸ ê¸°ì‚¬ ë‚ ì§œ: {pub_datetime}")
    logging.info(f"í˜„ì¬ ë‚ ì§œ: {now}")
    logging.info(f"ì„¤ì •ëœ í•„í„° - since_date: {since_date}, until_date: {until_date}, past_date: {past_date}")

    if past_date:
        result = pub_datetime >= past_date
        logging.info(f"past_date í•„í„° ì ìš© ê²°ê³¼: {result}")
        return result
    
    if since_date and pub_datetime < since_date:
        logging.info(f"since_date í•„í„°ì— ì˜í•´ ì œì™¸ë¨")
        return False
    if until_date and pub_datetime > until_date:
        logging.info(f"until_date í•„í„°ì— ì˜í•´ ì œì™¸ë¨")
        return False
    
    logging.info(f"ëª¨ë“  ë‚ ì§œ í•„í„°ë¥¼ í†µê³¼í•¨")
    return True
	
def main():
    """ë©”ì¸ í•¨ìˆ˜: RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ê³  Discordë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    rss_base_url = "https://news.google.com/rss/search"
    
    if KEYWORD_MODE:
        encoded_keyword = requests.utils.quote(KEYWORD)
        query_params = [f"q={encoded_keyword}"]
        
        if WHEN:
            query_params[-1] += f"+when:{WHEN}"
        elif AFTER_DATE or BEFORE_DATE:
            if AFTER_DATE:
                query_params[-1] += f"+after:{AFTER_DATE}"
            if BEFORE_DATE:
                query_params[-1] += f"+before:{BEFORE_DATE}"
        
        query_string = "+".join(query_params)
        
        if HL and GL and CEID:
            rss_url = f"{rss_base_url}?{query_string}&hl={HL}&gl={GL}&ceid={CEID}"
        else:
            rss_url = f"{rss_base_url}?{query_string}&hl=ko&gl=KR&ceid=KR:ko"
        
        category = KEYWORD
    else:
        rss_url = RSS_URL_KEYWORD
        rss_data = fetch_rss_feed(rss_url)
        root = ET.fromstring(rss_data)
        title_element = root.find('.//channel/title')
        if title_element is not None:
            category = extract_rss_feed_category(title_element.text)
        else:
            category = "ë””ìŠ¤ì½”ë“œ"

    logging.info(f"ì‚¬ìš©ëœ RSS URL: {rss_url}")

    rss_data = fetch_rss_feed(rss_url)
    root = ET.fromstring(rss_data)

    init_db(reset=INITIALIZE_KEYWORD)

    session = requests.Session()
    
    news_items = root.findall('.//item')
    news_items = sorted(news_items, key=lambda item: parser.parse(item.find('pubDate').text))

    since_date, until_date, past_date = parse_date_filter(DATE_FILTER_KEYWORD)
    logging.info(f"ì ìš©ë  ë‚ ì§œ í•„í„° - since_date: {since_date}, until_date: {until_date}, past_date: {past_date}")

    with sqlite3.connect(DB_PATH) as conn:
        for item in news_items:
            guid = item.find('guid').text

            if not INITIALIZE_KEYWORD and is_guid_posted(guid, conn):
                logging.info(f"ì´ë¯¸ ê²Œì‹œëœ í•­ëª© ê±´ë„ˆëœ€: {guid}")
                continue

            title = replace_brackets(item.find('title').text)
            google_link = item.find('link').text
            link = get_original_url(google_link, session)
            pub_date = item.find('pubDate').text
            description_html = item.find('description').text
            
            formatted_date = parse_rss_date(pub_date)

            # ë‚ ì§œ í•„í„° ì ìš© (ì¤‘ë³µ ì²´í¬ í›„)
            if not is_within_date_range(pub_date, since_date, until_date, past_date):
                logging.info(f"ë‚ ì§œ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
                continue

            logging.info(f"ë‚ ì§œ í•„í„°ë¥¼ í†µê³¼í•œ ë‰´ìŠ¤: {title}")

            description, related_news = parse_html_description(description_html, session, title, link)

            # ê³ ê¸‰ ê²€ìƒ‰ í•„í„° ì ìš©
            if not apply_advanced_filter(title, description, ADVANCED_FILTER_KEYWORD):
                logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
                continue

            discord_message = f"`Google ë‰´ìŠ¤ - {category} - í•œêµ­ ğŸ‡°ğŸ‡·`\n**{title}**\n{link}"
            if description:
                discord_message += f"\n{description}"
            discord_message += f"\n\nğŸ“… {formatted_date}"

            send_discord_message(
                DISCORD_WEBHOOK_KEYWORD,
                discord_message,
                avatar_url=DISCORD_AVATAR_KEYWORD,
                username=DISCORD_USERNAME_KEYWORD
            )

            save_news_item(pub_date, guid, title, link, json.dumps(related_news, ensure_ascii=False))

            if not INITIALIZE_KEYWORD:
                time.sleep(3)

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")