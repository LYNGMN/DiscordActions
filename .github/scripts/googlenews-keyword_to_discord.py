import xml.etree.ElementTree as ET
import requests
import re
import os
import time
import random
import logging
import json
import base64
import sqlite3
import sys
from urllib.parse import urlparse, parse_qs, unquote
from datetime import datetime, timedelta
from dateutil import parser
from dateutil.tz import gettz
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK_KEYWORD = os.environ.get('DISCORD_WEBHOOK_KEYWORD')
DISCORD_AVATAR_KEYWORD = os.environ.get('DISCORD_AVATAR_KEYWORD')
DISCORD_USERNAME_KEYWORD = os.environ.get('DISCORD_USERNAME_KEYWORD')
INITIALIZE_KEYWORD = os.environ.get('INITIALIZE_MODE_KEYWORD', 'false').lower() == 'true'
KEYWORD_MODE = os.environ.get('KEYWORD_MODE', 'false').lower() == 'true'
KEYWORD = os.environ.get('KEYWORD', '')
RSS_URL_KEYWORD = os.environ.get('RSS_URL_KEYWORD', '')
AFTER_DATE = os.environ.get('AFTER_DATE', '')
BEFORE_DATE = os.environ.get('BEFORE_DATE', '')
WHEN = os.environ.get('WHEN', '')
HL = os.environ.get('HL', '')
GL = os.environ.get('GL', '')
CEID = os.environ.get('CEID', '')
ADVANCED_FILTER_KEYWORD = os.environ.get('ADVANCED_FILTER_KEYWORD', '')
DATE_FILTER_KEYWORD = os.environ.get('DATE_FILTER_KEYWORD', '')
ORIGIN_LINK_KEYWORD = os.getenv('ORIGIN_LINK_KEYWORD', '').lower()
ORIGIN_LINK_KEYWORD = ORIGIN_LINK_KEYWORD not in ['false', 'f', '0', 'no', 'n']

# ORIGIN_LINK_KEYWORD ê°’ì„ ë¡œê·¸ì— ì¶œë ¥
logging.info(f"ORIGIN_LINK_KEYWORD ê°’: {ORIGIN_LINK_KEYWORD}")

# DB ì„¤ì •
DB_PATH = 'google_news_keyword.db'

def check_env_variables():
    """í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not DISCORD_WEBHOOK_KEYWORD:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK_KEYWORD")
    if KEYWORD_MODE and not KEYWORD:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆì§€ë§Œ KEYWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not KEYWORD_MODE and not RSS_URL_KEYWORD:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆì§€ë§Œ RSS_URL_KEYWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if AFTER_DATE and not is_valid_date(AFTER_DATE):
        raise ValueError("AFTER_DATE í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì´ ì•„ë‹™ë‹ˆë‹¤.")
    if BEFORE_DATE and not is_valid_date(BEFORE_DATE):
        raise ValueError("BEFORE_DATE í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì´ ì•„ë‹™ë‹ˆë‹¤.")
    if WHEN and (AFTER_DATE or BEFORE_DATE):
        logging.error("WHENê³¼ AFTER_DATE/BEFORE_DATEëŠ” í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. WHENì„ ì‚¬ìš©í•˜ê±°ë‚˜ AFTER_DATE/BEFORE_DATEë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        raise ValueError("ì˜ëª»ëœ ë‚ ì§œ ì¿¼ë¦¬ ì¡°í•©ì…ë‹ˆë‹¤.")
    if (HL or GL or CEID) and not (HL and GL and CEID):
        raise ValueError("HL, GL, CEID í™˜ê²½ ë³€ìˆ˜ëŠ” ëª¨ë‘ ì„¤ì •ë˜ê±°ë‚˜ ëª¨ë‘ ì„¤ì •ë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤.")
    if ADVANCED_FILTER_KEYWORD:
        logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {ADVANCED_FILTER_KEYWORD}")
    if DATE_FILTER_KEYWORD:
        logging.info(f"ë‚ ì§œ í•„í„°ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤: {DATE_FILTER_KEYWORD}")

def is_valid_date(date_string):
    """ë‚ ì§œ ë¬¸ìì—´ì´ ì˜¬ë°”ë¥¸ í˜•ì‹(YYYY-MM-DD)ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        datetime.strptime(date_string, '%Y-%m-%d')
        return True
    except ValueError:
        return False

def replace_brackets(text):
    """ëŒ€ê´„í˜¸ì™€ êº¾ì‡ ê´„í˜¸ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
    text = text.replace('[', 'ï¼»').replace(']', 'ï¼½')
    text = text.replace('<', 'ã€ˆ').replace('>', 'ã€‰')
    text = re.sub(r'(?<!\s)(?<!^)ï¼»', ' ï¼»', text)
    text = re.sub(r'ï¼½(?!\s)', 'ï¼½ ', text)
    text = re.sub(r'(?<!\s)(?<!^)ã€ˆ', ' ã€ˆ', text)
    text = re.sub(r'ã€‰(?!\s)', 'ã€‰ ', text)
    return text

def parse_rss_date(pub_date):
    """RSS ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ í˜•ì‹í™”ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dt = parser.parse(pub_date)
    dt_kst = dt.astimezone(gettz('Asia/Seoul'))
    return dt_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')
	
def init_db(reset=False):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        if reset:
            c.execute("DROP TABLE IF EXISTS news_items")
            logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œ")
        c.execute('''CREATE TABLE IF NOT EXISTS news_items
                     (pub_date TEXT,
                      guid TEXT PRIMARY KEY,
                      title TEXT,
                      link TEXT,
                      related_news TEXT)''')
        logging.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

def is_guid_posted(guid):
    """ì£¼ì–´ì§„ GUIDê°€ ì´ë¯¸ ê²Œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT 1 FROM news_items WHERE guid = ?", (guid,))
        result = c.fetchone()
        logging.info(f"GUID {guid} ì¤‘ë³µ í™•ì¸ ê²°ê³¼: {'ì¤‘ë³µ' if result else 'ìƒˆë¡œìš´ í•­ëª©'}")
        return result is not None

def save_news_item(pub_date, guid, title, link, related_news):
    """ë‰´ìŠ¤ í•­ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” êµ¬ì¡° í™•ì¸
        c.execute("PRAGMA table_info(news_items)")
        columns = [column[1] for column in c.fetchall()]
        
        # ê´€ë ¨ ë‰´ìŠ¤ í•­ëª© ìˆ˜ í™•ì¸
        related_news_items = json.loads(related_news)
        related_news_count = len(related_news_items)
        
        # í•„ìš”í•œ ì—´ ì¶”ê°€
        for i in range(related_news_count):
            title_col = f"related_title_{i+1}"
            press_col = f"related_press_{i+1}"
            link_col = f"related_link_{i+1}"
            
            if title_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {title_col} TEXT")
            if press_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {press_col} TEXT")
            if link_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {link_col} TEXT")
        
        # ë°ì´í„° ì‚½ì…ì„ ìœ„í•œ SQL ì¿¼ë¦¬ ì¤€ë¹„
        columns = ["pub_date", "guid", "title", "link", "related_news"]
        values = [pub_date, guid, title, link, related_news]
        
        for i, item in enumerate(related_news_items):
            columns.extend([f"related_title_{i+1}", f"related_press_{i+1}", f"related_link_{i+1}"])
            values.extend([item.get('title', ''), item.get('press', ''), item.get('link', '')])
        
        placeholders = ", ".join(["?" for _ in values])
        columns_str = ", ".join(columns)
        
        c.execute(f"INSERT OR REPLACE INTO news_items ({columns_str}) VALUES ({placeholders})", values)
        
        logging.info(f"ë‰´ìŠ¤ í•­ëª© ì €ì¥/ì—…ë°ì´íŠ¸: {guid}")
		
def decode_base64_url_part(encoded_str):
    """Base64ë¡œ ì¸ì½”ë”©ëœ ë¬¸ìì—´ì„ ë””ì½”ë”©"""
    base64_str = encoded_str.replace("-", "+").replace("_", "/")
    base64_str += "=" * ((4 - len(base64_str) % 4) % 4)
    try:
        decoded_bytes = base64.urlsafe_b64decode(base64_str)
        decoded_str = decoded_bytes.decode('latin1')  # latin1ì„ ì‚¬ìš©í•˜ì—¬ ë””ì½”ë”©
        return decoded_str
    except Exception as e:
        return f"ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def extract_regular_url(decoded_str):
    """ë””ì½”ë”©ëœ ë¬¸ìì—´ì—ì„œ ì²« ë²ˆì§¸ URLë§Œ ì •í™•íˆ ì¶”ì¶œ"""
    parts = re.split(r'[^\x20-\x7E]+', decoded_str)
    url_pattern = r'(https?://[^\s]+)'
    for part in parts:
        match = re.search(url_pattern, part)
        if match:
            return match.group(0)
    return None

def extract_youtube_id(decoded_str):
    """ë””ì½”ë”©ëœ ë¬¸ìì—´ì—ì„œ ìœ íŠœë¸Œ ì˜ìƒ ID ì¶”ì¶œ"""
    pattern = r'\x08 "\x0b([\w-]{11})\x98\x01\x01'
    match = re.search(pattern, decoded_str)
    if match:
        return match.group(1)
    return None

def decode_google_news_url(source_url):
    """Google ë‰´ìŠ¤ URLì„ ë””ì½”ë”©í•˜ì—¬ ì›ë³¸ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    url = urlparse(source_url)
    path = url.path.split('/')
    if url.hostname == "news.google.com" and len(path) > 1 and path[-2] == "articles":
        base64_str = path[-1]
        decoded_str = decode_base64_url_part(base64_str)
        
        regular_url = extract_regular_url(decoded_str)
        if regular_url:
            logging.info(f"ì¼ë°˜ ë§í¬ ì¶”ì¶œ ì„±ê³µ: {source_url} -> {regular_url}")
            return regular_url
        
        youtube_id = extract_youtube_id(decoded_str)
        if youtube_id:
            youtube_url = f"https://www.youtube.com/watch?v={youtube_id}"
            logging.info(f"ìœ íŠœë¸Œ ë§í¬ ì¶”ì¶œ ì„±ê³µ: {source_url} -> {youtube_url}")
            return youtube_url
    
    logging.warning(f"Google ë‰´ìŠ¤ URL ë””ì½”ë”© ì‹¤íŒ¨, ì›ë³¸ URL ë°˜í™˜: {source_url}")
    return source_url

def get_original_url(google_link, session, max_retries=5):
    """
    Google ë‰´ìŠ¤ ë§í¬ë¥¼ ì›ë³¸ URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
    ORIGIN_LINK_KEYWORD ì„¤ì •ì— ë”°ë¼ ë™ì‘ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤:
    - ì„¤ì •í•˜ì§€ ì•Šì•˜ê±°ë‚˜ True: ì˜¤ë¦¬ì§€ë„ ë§í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - False: ì› ë§í¬(êµ¬ê¸€ ë§í¬)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    logging.info(f"ORIGIN_LINK_KEYWORD ê°’ í™•ì¸: {ORIGIN_LINK_KEYWORD}")

    if ORIGIN_LINK_KEYWORD:
        # ì˜¤ë¦¬ì§€ë„ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ë ¤ê³  ì‹œë„
        original_url = decode_google_news_url(google_link)
        if original_url != google_link:
            return original_url

        # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ requests ë°©ì‹ ì‹œë„
        retries = 0
        while retries < max_retries:
            try:
                response = session.get(google_link, allow_redirects=True)
                if response.status_code == 200:
                    return response.url
            except requests.RequestException as e:
                logging.error(f"Failed to get original URL: {e}")
            retries += 1
        
        # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš° ì› ë§í¬ ë°˜í™˜
        logging.warning(f"ì˜¤ë¦¬ì§€ë„ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return google_link
    else:
        # ORIGIN_LINK_KEYWORDê°€ Falseì¸ ê²½ìš° ì› ë§í¬ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        logging.info(f"ORIGIN_LINK_KEYWORDê°€ False, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return google_link

def fetch_rss_feed(url):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    response = requests.get(url)
    return response.content

def send_discord_message(webhook_url, message, avatar_url=None, username=None):
    """Discord ì›¹í›…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
    payload = {"content": message}
    
    if avatar_url and avatar_url.strip():
        payload["avatar_url"] = avatar_url
    
    if username and username.strip():
        payload["username"] = username
    
    headers = {"Content-Type": "application/json"}
    response = requests.post(webhook_url, json=payload, headers=headers)
    if response.status_code != 204:
        logging.error(f"Discordì— ë©”ì‹œì§€ë¥¼ ê²Œì‹œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        logging.error(response.text)
    else:
        logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
    time.sleep(3)

def extract_news_items(description, session):
    """HTML ì„¤ëª…ì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(description, 'html.parser')
    news_items = []
    for li in soup.find_all('li'):
        a_tag = li.find('a')
        if a_tag:
            title = replace_brackets(a_tag.text)
            google_link = a_tag['href']
            link = get_original_url(google_link, session)
            press = li.find('font', color="#6f6f6f").text if li.find('font', color="#6f6f6f") else ""
            news_items.append({"title": title, "link": link, "press": press})
    return news_items

def parse_html_description(html_desc, session, main_title, main_link):
    """HTML ì„¤ëª…ì„ íŒŒì‹±í•˜ì—¬ ê´€ë ¨ ë‰´ìŠ¤ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    news_items = extract_news_items(html_desc, session)
    
    news_items = [item for item in news_items if item['title'] != main_title or item['link'] != main_link]
    
    if len(news_items) == 0:
        return "", []  # ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ë©”ì¸ ë‰´ìŠ¤ì™€ ë™ì¼í•œ ê²½ìš°
    elif len(news_items) == 1:
        return "", news_items  # ê´€ë ¨ ë‰´ìŠ¤ê°€ 1ê°œì¸ ê²½ìš° (í‘œì‹œí•˜ì§€ ì•ŠìŒ)
    else:
        news_string = '\n'.join([f"> - [{item['title']}]({item['link']}) | {item['press']}" for item in news_items])
        return news_string, news_items

def extract_rss_feed_category(title):
    """RSS í”¼ë“œ ì œëª©ì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r'"([^"]+)', title)
    if match:
        category = match.group(1)
        if 'when:' in category:
            category = category.split('when:')[0].strip()
        return category
    return "ë””ìŠ¤ì½”ë“œ"

def apply_advanced_filter(title, description, advanced_filter):
    """ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²Œì‹œë¬¼ì„ ì „ì†¡í• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    if not advanced_filter:
        return True

    text_to_check = (title + ' ' + description).lower()

    terms = re.findall(r'([+-]?)(?:"([^"]*)"|\S+)', advanced_filter)

    for prefix, term in terms:
        term = term.lower() if term else prefix.lower()
        if prefix == '+' or not prefix:  # í¬í•¨í•´ì•¼ í•˜ëŠ” ë‹¨ì–´
            if term not in text_to_check:
                return False
        elif prefix == '-':  # ì œì™¸í•´ì•¼ í•˜ëŠ” ë‹¨ì–´ ë˜ëŠ” êµ¬ë¬¸
            exclude_terms = term.split()
            if len(exclude_terms) > 1:
                if ' '.join(exclude_terms) in text_to_check:
                    return False
            else:
                if term in text_to_check:
                    return False

    return True

def parse_date_filter(filter_string):
    """ë‚ ì§œ í•„í„° ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ì‹œì‘ ë‚ ì§œì™€ ì¢…ë£Œ ë‚ ì§œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    since_date = None
    until_date = None
    past_date = None

    since_match = re.search(r'since:(\d{4}-\d{2}-\d{2})', filter_string)
    until_match = re.search(r'until:(\d{4}-\d{2}-\d{2})', filter_string)
    
    if since_match:
        since_date = datetime.strptime(since_match.group(1), '%Y-%m-%d')
    if until_match:
        until_date = datetime.strptime(until_match.group(1), '%Y-%m-%d')

    past_match = re.search(r'past:(\d+)([hdmy])', filter_string)
    if past_match:
        value = int(past_match.group(1))
        unit = past_match.group(2)
        now = datetime.now()
        if unit == 'h':
            past_date = now - timedelta(hours=value)
        elif unit == 'd':
            past_date = now - timedelta(days=value)
        elif unit == 'm':
            past_date = now - timedelta(days=value*30)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        elif unit == 'y':
            past_date = now - timedelta(days=value*365)  # ê·¼ì‚¬ê°’ ì‚¬ìš©

    return since_date, until_date, past_date

def is_within_date_range(pub_date, since_date, until_date, past_date):
    """ì£¼ì–´ì§„ ë‚ ì§œê°€ í•„í„° ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    pub_datetime = parser.parse(pub_date)
    
    if past_date:
        return pub_datetime >= past_date
    
    if since_date and pub_datetime < since_date:
        return False
    if until_date and pub_datetime > until_date:
        return False
    
    return True
	
def main():
    """ë©”ì¸ í•¨ìˆ˜: RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ê³  Discordë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    rss_base_url = "https://news.google.com/rss/search"
    
    if KEYWORD_MODE:
        encoded_keyword = requests.utils.quote(KEYWORD)
        query_params = [f"q={encoded_keyword}"]
        
        if WHEN:
            query_params[-1] += f"+when:{WHEN}"
        elif AFTER_DATE or BEFORE_DATE:
            if AFTER_DATE:
                query_params[-1] += f"+after:{AFTER_DATE}"
            if BEFORE_DATE:
                query_params[-1] += f"+before:{BEFORE_DATE}"
        
        query_string = "+".join(query_params)
        
        if HL and GL and CEID:
            rss_url = f"{rss_base_url}?{query_string}&hl={HL}&gl={GL}&ceid={CEID}"
        else:
            rss_url = f"{rss_base_url}?{query_string}&hl=ko&gl=KR&ceid=KR:ko"
        
        category = KEYWORD
    else:
        rss_url = RSS_URL_KEYWORD
        rss_data = fetch_rss_feed(rss_url)
        root = ET.fromstring(rss_data)
        title_element = root.find('.//channel/title')
        if title_element is not None:
            category = extract_rss_feed_category(title_element.text)
        else:
            category = "ë””ìŠ¤ì½”ë“œ"

    logging.info(f"ì‚¬ìš©ëœ RSS URL: {rss_url}")

    rss_data = fetch_rss_feed(rss_url)
    root = ET.fromstring(rss_data)

    init_db(reset=INITIALIZE_KEYWORD)

    session = requests.Session()
    
    news_items = root.findall('.//item')
    news_items = sorted(news_items, key=lambda item: parser.parse(item.find('pubDate').text))

    since_date, until_date, past_date = parse_date_filter(DATE_FILTER_KEYWORD)

    for item in news_items:
        guid = item.find('guid').text

        if not INITIALIZE_KEYWORD and is_guid_posted(guid):
            continue

        title = replace_brackets(item.find('title').text)
        google_link = item.find('link').text
        link = get_original_url(google_link, session)
        pub_date = item.find('pubDate').text
        description_html = item.find('description').text
        
        formatted_date = parse_rss_date(pub_date)

        # ë‚ ì§œ í•„í„° ì ìš©
        if not is_within_date_range(pub_date, since_date, until_date, past_date):
            logging.info(f"ë‚ ì§œ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
            continue

        description, related_news = parse_html_description(description_html, session, title, link)

        # ê³ ê¸‰ ê²€ìƒ‰ í•„í„° ì ìš©
        if not apply_advanced_filter(title, description, ADVANCED_FILTER_KEYWORD):
            logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
            continue

        discord_message = f"`Google ë‰´ìŠ¤ - {category} - í•œêµ­ ğŸ‡°ğŸ‡·`\n**{title}**\n{link}"
        if description:
            discord_message += f"\n{description}"
        discord_message += f"\n\nğŸ“… {formatted_date}"

        send_discord_message(
            DISCORD_WEBHOOK_KEYWORD,
            discord_message,
            avatar_url=DISCORD_AVATAR_KEYWORD,
            username=DISCORD_USERNAME_KEYWORD
        )

        save_news_item(pub_date, guid, title, link, json.dumps(related_news, ensure_ascii=False))

        if not INITIALIZE_KEYWORD:
            time.sleep(3)

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")