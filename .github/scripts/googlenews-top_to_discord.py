
import xml.etree.ElementTree as ET
import requests
import re
import os
import time
import random
import logging
import json
import base64
import sqlite3
import sys
import pytz
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode, unquote, quote
from datetime import datetime, timedelta
from dateutil import parser
from dateutil.tz import gettz
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK_TOP = os.environ.get('DISCORD_WEBHOOK_TOP')
DISCORD_AVATAR_TOP = os.environ.get('DISCORD_AVATAR_TOP', '').strip()
DISCORD_USERNAME_TOP = os.environ.get('DISCORD_USERNAME_TOP', '').strip()
INITIALIZE_TOP = os.environ.get('INITIALIZE_MODE_TOP', 'false').lower() == 'true'
ADVANCED_FILTER_TOP = os.environ.get('ADVANCED_FILTER_TOP', '')
DATE_FILTER_TOP = os.environ.get('DATE_FILTER_TOP', '')
ORIGIN_LINK_TOP = os.environ.get('ORIGIN_LINK_TOP', 'true').lower() == 'true'
RSS_URL_TOP = os.environ.get('RSS_URL_TOP')
TOP_MODE = os.environ.get('TOP_MODE', 'false').lower() == 'true'
TOP_COUNTRY = os.environ.get('TOP_COUNTRY')

# DB ì„¤ì •
DB_PATH = 'google_news_top.db'

def check_env_variables():
    """í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not DISCORD_WEBHOOK_TOP:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK_TOP")

def init_db(reset=False):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        try:
            if reset:
                c.execute("DROP TABLE IF EXISTS news_items")
                logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œë¨")
            
            c.execute('''CREATE TABLE IF NOT EXISTS news_items
                         (pub_date TEXT,
                          guid TEXT PRIMARY KEY,
                          title TEXT,
                          link TEXT,
                          related_news TEXT)''')
            
            # í…Œì´ë¸”ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            c.execute("SELECT COUNT(*) FROM news_items")
            count = c.fetchone()[0]
            
            if reset or count == 0:
                logging.info("ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                logging.info(f"ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í˜„ì¬ {count}ê°œì˜ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤.")
            
        except sqlite3.Error as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

def is_guid_posted(guid):
    """ì£¼ì–´ì§„ GUIDê°€ ì´ë¯¸ ê²Œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT 1 FROM news_items WHERE guid = ?", (guid,))
        return c.fetchone() is not None

def save_news_item(pub_date, guid, title, link, related_news):
    """ë‰´ìŠ¤ í•­ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” êµ¬ì¡° í™•ì¸
        c.execute("PRAGMA table_info(news_items)")
        columns = [column[1] for column in c.fetchall()]
        
        # ê´€ë ¨ ë‰´ìŠ¤ í•­ëª© ìˆ˜ í™•ì¸
        related_news_count = len(json.loads(related_news))
        
        # í•„ìš”í•œ ì—´ ì¶”ê°€
        for i in range(related_news_count):
            title_col = f"related_title_{i+1}"
            press_col = f"related_press_{i+1}"
            link_col = f"related_link_{i+1}"
            
            if title_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {title_col} TEXT")
            if press_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {press_col} TEXT")
            if link_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {link_col} TEXT")
        
        # ë°ì´í„° ì‚½ì…ì„ ìœ„í•œ SQL ì¿¼ë¦¬ ì¤€ë¹„
        columns = ["pub_date", "guid", "title", "link", "related_news"]
        values = [pub_date, guid, title, link, related_news]
        
        related_news_items = json.loads(related_news)
        for i, item in enumerate(related_news_items):
            columns.extend([f"related_title_{i+1}", f"related_press_{i+1}", f"related_link_{i+1}"])
            values.extend([item['title'], item['press'], item['link']])
        
        placeholders = ", ".join(["?" for _ in values])
        columns_str = ", ".join(columns)
        
        c.execute(f"INSERT OR REPLACE INTO news_items ({columns_str}) VALUES ({placeholders})", values)
        
        logging.info(f"ìƒˆ ë‰´ìŠ¤ í•­ëª© ì €ì¥: {guid}")

def fetch_decoded_batch_execute(id):
    s = (
        '[[["Fbv4je","[\\"garturlreq\\",[[\\"en-US\\",\\"US\\",[\\"FINANCE_TOP_INDICES\\",\\"WEB_TEST_1_0_0\\"],'
        'null,null,1,1,\\"US:en\\",null,180,null,null,null,null,null,0,null,null,[1608992183,723341000]],'
        '\\"en-US\\",\\"US\\",1,[2,3,4,8],1,0,\\"655000234\\",0,0,null,0],\\"' +
        id +
        '\\"]",null,"generic"]]]'
    )

    headers = {
        "Content-Type": "application/x-www-form-urlencoded;charset=utf-8",
        "Referer": "https://news.google.com/"
    }

    response = requests.post(
        "https://news.google.com/_/DotsSplashUi/data/batchexecute?rpcids=Fbv4je",
        headers=headers,
        data={"f.req": s}
    )

    if response.status_code != 200:
        raise Exception("Failed to fetch data from Google.")

    text = response.text
    header = '[\\"garturlres\\",\\"'
    footer = '\\",'
    if header not in text:
        raise Exception(f"Header not found in response: {text}")
    start = text.split(header, 1)[1]
    if footer not in start:
        raise Exception("Footer not found in response.")
    url = start.split(footer, 1)[0]
    return url

def decode_base64_url_part(encoded_str):
    base64_str = encoded_str.replace("-", "+").replace("_", "/")
    base64_str += "=" * ((4 - len(base64_str) % 4) % 4)
    try:
        decoded_bytes = base64.urlsafe_b64decode(base64_str)
        decoded_str = decoded_bytes.decode('latin1')
        return decoded_str
    except Exception as e:
        return f"ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def extract_youtube_id(decoded_str):
    pattern = r'\x08 "\x0b([\w-]{11})\x98\x01\x01'
    match = re.search(pattern, decoded_str)
    if match:
        return match.group(1)
    return None

def extract_regular_url(decoded_str):
    parts = re.split(r'[^\x20-\x7E]+', decoded_str)
    url_pattern = r'(https?://[^\s]+)'
    for part in parts:
        match = re.search(url_pattern, part)
        if match:
            return match.group(0)
    return None

def unescape_unicode(text):
    """ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ë¥¼ ì‹¤ì œ ë¬¸ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    return re.sub(
        r'\\u([0-9a-fA-F]{4})',
        lambda m: chr(int(m.group(1), 16)),
        text
    )

def clean_url(url):
    """URLì„ ì •ë¦¬í•˜ê³  ìœ ë‹ˆì½”ë“œ ë¬¸ìë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    # ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ ì‹œí€€ìŠ¤ ì²˜ë¦¬
    url = unescape_unicode(url)
    
    # ë°±ìŠ¬ë˜ì‹œë¥¼ ì •ë¦¬
    url = url.replace('\\', '')
    
    # URL ë””ì½”ë”© (ì˜ˆ: %2F -> /, %40 -> @ ë“±)
    url = unquote(url)

    parsed_url = urlparse(url)
    
    # MSN ë§í¬ íŠ¹ë³„ ì²˜ë¦¬: HTTPSë¡œ ë³€í™˜ ë° ë¶ˆí•„ìš”í•œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
    if parsed_url.netloc.endswith('msn.com'):
        parsed_url = parsed_url._replace(scheme='https')
        query_params = parse_qs(parsed_url.query)
        cleaned_params = {k: v[0] for k, v in query_params.items() if k in ['id', 'article']}
        cleaned_query = urlencode(cleaned_params)
        parsed_url = parsed_url._replace(query=cleaned_query)
    
    # ê³µë°± ë“± ë¹„ì •ìƒì ì¸ ë¬¸ì ì²˜ë¦¬
    # safe íŒŒë¼ë¯¸í„°ì— íŠ¹ìˆ˜ ë¬¸ìë“¤ì„ í¬í•¨í•˜ì—¬ ì¸ì½”ë”©ë˜ì§€ ì•Šë„ë¡ ì„¤ì •
    safe_chars = "/:@&=+$,?#"
    cleaned_path = quote(parsed_url.path, safe=safe_chars)
    cleaned_query = quote(parsed_url.query, safe=safe_chars)
    
    # URL ì¬êµ¬ì„±
    cleaned_url = urlunparse(parsed_url._replace(path=cleaned_path, query=cleaned_query))
    
    return cleaned_url

def decode_google_news_url(source_url):
    url = urlparse(source_url)
    path = url.path.split("/")
    if url.hostname == "news.google.com" and len(path) > 1 and path[-2] == "articles":
        base64_str = path[-1]
        
        # ë¨¼ì € ìƒˆë¡œìš´ ë°©ì‹ ì‹œë„
        try:
            decoded_bytes = base64.urlsafe_b64decode(base64_str + '==')
            decoded_str = decoded_bytes.decode('latin1')

            prefix = b'\x08\x13\x22'.decode('latin1')
            if decoded_str.startswith(prefix):
                decoded_str = decoded_str[len(prefix):]

            suffix = b'\xd2\x01\x00'.decode('latin1')
            if decoded_str.endswith(suffix):
                decoded_str = decoded_str[:-len(suffix)]

            bytes_array = bytearray(decoded_str, 'latin1')
            length = bytes_array[0]
            if length >= 0x80:
                decoded_str = decoded_str[2:length+1]
            else:
                decoded_str = decoded_str[1:length+1]

            if decoded_str.startswith("AU_yqL"):
                return clean_url(fetch_decoded_batch_execute(base64_str))

            regular_url = extract_regular_url(decoded_str)
            if regular_url:
                return clean_url(regular_url)
        except Exception:
            pass  # ìƒˆë¡œìš´ ë°©ì‹ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ì¡´ ë°©ì‹ ì‹œë„

        # ê¸°ì¡´ ë°©ì‹ ì‹œë„ (ìœ íŠœë¸Œ ë§í¬ í¬í•¨)
        decoded_str = decode_base64_url_part(base64_str)
        youtube_id = extract_youtube_id(decoded_str)
        if youtube_id:
            return f"https://www.youtube.com/watch?v={youtube_id}"

        regular_url = extract_regular_url(decoded_str)
        if regular_url:
            return clean_url(regular_url)

    return clean_url(source_url)  # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì›ë³¸ URL ì •ë¦¬ í›„ ë°˜í™˜

def get_original_url(google_link, session, max_retries=5):
    logging.info(f"ORIGIN_LINK_TOP ê°’ í™•ì¸: {ORIGIN_LINK_TOP}")

    # ORIGIN_LINK_TOP ì„¤ì •ê³¼ ìƒê´€ì—†ì´ í•­ìƒ ì›ë³¸ ë§í¬ë¥¼ ì‹œë„
    original_url = decode_google_news_url(google_link)
    if original_url != google_link:
        return original_url

    # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ requests ë°©ì‹ ì‹œë„
    retries = 0
    while retries < max_retries:
        try:
            response = session.get(google_link, allow_redirects=True)
            if response.status_code == 200:
                return clean_url(response.url)
        except requests.RequestException as e:
            logging.error(f"Failed to get original URL: {e}")
        retries += 1

    logging.warning(f"ì˜¤ë¦¬ì§€ë„ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨, ì› ë§í¬ ì‚¬ìš©: {google_link}")
    return clean_url(google_link)

def fetch_rss_feed(url, max_retries=3, retry_delay=5):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()  # 4xx, 5xx ìƒíƒœ ì½”ë“œì— ëŒ€í•´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
            return response.content
        except RequestException as e:
            logging.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt + 1 < max_retries:
                time.sleep(retry_delay)
            else:
                logging.error(f"RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {url}")
                raise

def get_rss_url():
    if TOP_MODE:
        if not TOP_COUNTRY:
            raise ValueError("TOP_MODEê°€ trueì¼ ë•Œ TOP_COUNTRYë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        country_configs = {
            # ë™ì•„ì‹œì•„
            'KR': ('ko', 'KR:ko', 'Google ë‰´ìŠ¤', 'ì£¼ìš” ë‰´ìŠ¤', 'í•œêµ­', 'South Korea', 'ğŸ‡°ğŸ‡·'),
            'JP': ('ja', 'JP:ja', 'Google ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'ãƒˆãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æ—¥æœ¬', 'Japan', 'ğŸ‡¯ğŸ‡µ'),
            'CN': ('zh-CN', 'CN:zh-Hans', 'Google æ–°é—»', 'ç„¦ç‚¹æ–°é—»', 'ä¸­å›½', 'China', 'ğŸ‡¨ğŸ‡³'),
            'TW': ('zh-TW', 'TW:zh-Hant', 'Google æ–°è', 'ç„¦é»æ–°è', 'å°ç£', 'Taiwan', 'ğŸ‡¹ğŸ‡¼'),
            'HK': ('zh-HK', 'HK:zh-Hant', 'Google æ–°è', 'ç„¦é»æ–°è', 'é¦™æ¸¯', 'Hong Kong', 'ğŸ‡­ğŸ‡°'),
            
            # ë™ë‚¨ì•„ì‹œì•„
            'VN': ('vi', 'VN:vi', 'Google Tin tá»©c', 'Tin ná»•i báº­t', 'Viá»‡t Nam', 'Vietnam', 'ğŸ‡»ğŸ‡³'),
            'TH': ('th', 'TH:th', 'Google News', 'à¹€à¸£à¸·à¹ˆà¸­à¸‡à¹€à¸”à¹ˆà¸™', 'à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢', 'Thailand', 'ğŸ‡¹ğŸ‡­'),
            'PH': ('en-PH', 'PH:en', 'Google News', 'Top stories', 'Philippines', 'Philippines', 'ğŸ‡µğŸ‡­'),
            'MY': ('ms-MY', 'MY:ms', 'Berita Google', 'Berita hangat', 'Malaysia', 'Malaysia', 'ğŸ‡²ğŸ‡¾'),
            'SG': ('en-SG', 'SG:en', 'Google News', 'Top stories', 'Singapore', 'Singapore', 'ğŸ‡¸ğŸ‡¬'),
            'ID': ('id', 'ID:id', 'Google Berita', 'Artikel populer', 'Indonesia', 'Indonesia', 'ğŸ‡®ğŸ‡©'),
            
            # ë‚¨ì•„ì‹œì•„
            'IN': ('en-IN', 'IN:en', 'Google News', 'Top stories', 'India', 'India', 'ğŸ‡®ğŸ‡³'),
            'BD': ('bn', 'BD:bn', 'Google News', 'à¦¸à§‡à¦°à¦¾ à¦–à¦¬à¦°', 'à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶', 'Bangladesh', 'ğŸ‡§ğŸ‡©'),
            'PK': ('en-PK', 'PK:en', 'Google News', 'Top stories', 'Pakistan', 'Pakistan', 'ğŸ‡µğŸ‡°'),
            
            # ì„œì•„ì‹œì•„
            'IL': ('he', 'IL:he', '×—×“×©×•×ª Google', '×”×›×ª×‘×•×ª ×”××•×‘×™×œ×•×ª', '×™×©×¨××œ', 'Israel', 'ğŸ‡®ğŸ‡±'),
            'AE': ('ar', 'AE:ar', 'Ø£Ø®Ø¨Ø§Ø± Google', 'Ø£Ù‡Ù… Ø§Ù„Ø£Ø®Ø¨Ø§Ø±', 'Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©', 'United Arab Emirates', 'ğŸ‡¦ğŸ‡ª'),
            'TR': ('tr', 'TR:tr', 'Google Haberler', 'En Ã§ok okunan haberler', 'TÃ¼rkiye', 'Turkey', 'ğŸ‡¹ğŸ‡·'),
            'LB': ('ar', 'LB:ar', 'Ø£Ø®Ø¨Ø§Ø± Google', 'Ø£Ù‡Ù… Ø§Ù„Ø£Ø®Ø¨Ø§Ø±', 'Ù„Ø¨Ù†Ø§Ù†', 'Lebanon', 'ğŸ‡±ğŸ‡§'),

            # ì˜¤ì„¸ì•„ë‹ˆì•„
            'AU': ('en-AU', 'AU:en', 'Google News', 'Top stories', 'Australia', 'Australia', 'ğŸ‡¦ğŸ‡º'),
            'NZ': ('en-NZ', 'NZ:en', 'Google News', 'Top stories', 'New Zealand', 'New Zealand', 'ğŸ‡³ğŸ‡¿'),

            # ëŸ¬ì‹œì•„ì™€ ë™ìœ ëŸ½
            'RU': ('ru', 'RU:ru', 'Google ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸', 'Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸', 'Ğ Ğ¾ÑÑĞ¸Ñ', 'Russia', 'ğŸ‡·ğŸ‡º'),
            'UA': ('uk', 'UA:uk', 'Google ĞĞ¾Ğ²Ğ¸Ğ½Ğ¸', 'Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ñ– Ğ½Ğ¾Ğ²Ğ¸Ğ½Ğ¸', 'Ğ£ĞºÑ€Ğ°Ñ—Ğ½Ğ°', 'Ukraine', 'ğŸ‡ºğŸ‡¦'),

            # ìœ ëŸ½
            'GR': ('el', 'GR:el', 'Î•Î¹Î´Î®ÏƒÎµÎ¹Ï‚ Google', 'ÎšÏ…ÏÎ¹ÏŒÏ„ÎµÏÎµÏ‚ ÎµÎ¹Î´Î®ÏƒÎµÎ¹Ï‚', 'Î•Î»Î»Î¬Î´Î±', 'Greece', 'ğŸ‡¬ğŸ‡·'),
            'DE': ('de', 'DE:de', 'Google News', 'Top-Meldungen', 'Deutschland', 'Germany', 'ğŸ‡©ğŸ‡ª'),
            'NL': ('nl', 'NL:nl', 'Google Nieuws', 'Voorpaginanieuws', 'Nederland', 'Netherlands', 'ğŸ‡³ğŸ‡±'),
            'NO': ('no', 'NO:no', 'Google Nyheter', 'Hovedoppslag', 'Norge', 'Norway', 'ğŸ‡³ğŸ‡´'),
            'LV': ('lv', 'LV:lv', 'Google ziÅ†as', 'PopulÄrÄkÄs ziÅ†as', 'Latvija', 'Latvia', 'ğŸ‡±ğŸ‡»'),
            'LT': ('lt', 'LT:lt', 'Google naujienos', 'Populiariausios naujienos', 'Lietuva', 'Lithuania', 'ğŸ‡±ğŸ‡¹'),
            'RO': ('ro', 'RO:ro', 'È˜tiri Google', 'Cele mai populare subiecte', 'RomÃ¢nia', 'Romania', 'ğŸ‡·ğŸ‡´'),
            'BE': ('fr', 'BE:fr', 'Google ActualitÃ©s', 'Ã€ la une', 'Belgique', 'Belgium', 'ğŸ‡§ğŸ‡ª'),
            'BG': ('bg', 'BG:bg', 'Google ĞĞ¾Ğ²Ğ¸Ğ½Ğ¸', 'Ğ’Ğ¾Ğ´ĞµÑ‰Ğ¸ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸', 'Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€Ğ¸Ñ', 'Bulgaria', 'ğŸ‡§ğŸ‡¬'),
            'SK': ('sk', 'SK:sk', 'SprÃ¡vy Google', 'HlavnÃ© sprÃ¡vy', 'Slovensko', 'Slovakia', 'ğŸ‡¸ğŸ‡°'),
            'SI': ('sl', 'SI:sl', 'Google News', 'NajpomembnejÅ¡e novice', 'Slovenija', 'Slovenia', 'ğŸ‡¸ğŸ‡®'),
            'CH': ('de', 'CH:de', 'Google News', 'Top-Meldungen', 'Schweiz', 'Switzerland', 'ğŸ‡¨ğŸ‡­'),
            'ES': ('es', 'ES:es', 'Google News', 'Noticias destacadas', 'EspaÃ±a', 'Spain', 'ğŸ‡ªğŸ‡¸'),
            'SE': ('sv', 'SE:sv', 'Google Nyheter', 'Huvudnyheter', 'Sverige', 'Sweden', 'ğŸ‡¸ğŸ‡ª'),
            'RS': ('sr', 'RS:sr', 'Google Ğ²ĞµÑÑ‚Ğ¸', 'ĞĞ°Ñ˜Ğ²Ğ°Ğ¶Ğ½Ğ¸Ñ˜Ğµ Ğ²ĞµÑÑ‚Ğ¸', 'Ğ¡Ñ€Ğ±Ğ¸Ñ˜Ğ°', 'Serbia', 'ğŸ‡·ğŸ‡¸'),
            'AT': ('de', 'AT:de', 'Google News', 'Top-Meldungen', 'Ã–sterreich', 'Austria', 'ğŸ‡¦ğŸ‡¹'),
            'IE': ('en-IE', 'IE:en', 'Google News', 'Top stories', 'Ireland', 'Ireland', 'ğŸ‡®ğŸ‡ª'),
            'EE': ('et-EE', 'EE:et', 'Google News', 'Populaarseimad lood', 'Eesti', 'Estonia', 'ğŸ‡ªğŸ‡ª'),
            'IT': ('it', 'IT:it', 'Google News', 'Notizie principali', 'Italia', 'Italy', 'ğŸ‡®ğŸ‡¹'),
            'CZ': ('cs', 'CZ:cs', 'ZprÃ¡vy Google', 'HlavnÃ­ udÃ¡losti', 'ÄŒesko', 'Czech Republic', 'ğŸ‡¨ğŸ‡¿'),
            'GB': ('en-GB', 'GB:en', 'Google News', 'Top stories', 'United Kingdom', 'United Kingdom', 'ğŸ‡¬ğŸ‡§'),
            'PL': ('pl', 'PL:pl', 'Google News', 'NajwaÅ¼niejsze artykuÅ‚y', 'Polska', 'Poland', 'ğŸ‡µğŸ‡±'),
            'PT': ('pt-PT', 'PT:pt-150', 'Google NotÃ­cias', 'NotÃ­cias principais', 'Portugal', 'Portugal', 'ğŸ‡µğŸ‡¹'),
            'FI': ('fi-FI', 'FI:fi', 'Google Uutiset', 'PÃ¤Ã¤uutiset', 'Suomi', 'Finland', 'ğŸ‡«ğŸ‡®'),
            'FR': ('fr', 'FR:fr', 'Google ActualitÃ©s', 'Ã€ la une', 'France', 'France', 'ğŸ‡«ğŸ‡·'),
            'HU': ('hu', 'HU:hu', 'Google HÃ­rek', 'VezetÅ‘ hÃ­rek', 'MagyarorszÃ¡g', 'Hungary', 'ğŸ‡­ğŸ‡º'),

            # ë¶ë¯¸
            'CA': ('en-CA', 'CA:en', 'Google News', 'Top stories', 'Canada', 'Canada', 'ğŸ‡¨ğŸ‡¦'),
            'MX': ('es-419', 'MX:es-419', 'Google Noticias', 'Noticias destacadas', 'MÃ©xico', 'Mexico', 'ğŸ‡²ğŸ‡½'),
            'US': ('en-US', 'US:en', 'Google News', 'Top stories', 'United States', 'United States', 'ğŸ‡ºğŸ‡¸'),
            'CU': ('es-419', 'CU:es-419', 'Google Noticias', 'Noticias destacadas', 'Cuba', 'Cuba', 'ğŸ‡¨ğŸ‡º'),

            # ë‚¨ë¯¸
            'AR': ('es-419', 'AR:es-419', 'Google Noticias', 'Noticias destacadas', 'Argentina', 'Argentina', 'ğŸ‡¦ğŸ‡·'),
            'BR': ('pt-BR', 'BR:pt-419', 'Google NotÃ­cias', 'Principais notÃ­cias', 'Brasil', 'Brazil', 'ğŸ‡§ğŸ‡·'),
            'CL': ('es-419', 'CL:es-419', 'Google Noticias', 'Noticias destacadas', 'Chile', 'Chile', 'ğŸ‡¨ğŸ‡±'),
            'CO': ('es-419', 'CO:es-419', 'Google Noticias', 'Noticias destacadas', 'Colombia', 'Colombia', 'ğŸ‡¨ğŸ‡´'),
            'PE': ('es-419', 'PE:es-419', 'Google Noticias', 'Noticias destacadas', 'PerÃº', 'Peru', 'ğŸ‡µğŸ‡ª'),
            'VE': ('es-419', 'VE:es-419', 'Google Noticias', 'Noticias destacadas', 'Venezuela', 'Venezuela', 'ğŸ‡»ğŸ‡ª'),

            # ì•„í”„ë¦¬ì¹´
            'ZA': ('en-ZA', 'ZA:en', 'Google News', 'Top stories', 'South Africa', 'South Africa', 'ğŸ‡¿ğŸ‡¦'),
            'NG': ('en-NG', 'NG:en', 'Google News', 'Top stories', 'Nigeria', 'Nigeria', 'ğŸ‡³ğŸ‡¬'),
            'EG': ('ar', 'EG:ar', 'Ø£Ø®Ø¨Ø§Ø± Google', 'Ø£Ù‡Ù… Ø§Ù„Ø£Ø®Ø¨Ø§Ø±', 'Ù…ØµØ±', 'Egypt', 'ğŸ‡ªğŸ‡¬'),
            'KE': ('en-KE', 'KE:en', 'Google News', 'Top stories', 'Kenya', 'Kenya', 'ğŸ‡°ğŸ‡ª'),
            'MA': ('fr', 'MA:fr', 'Google ActualitÃ©s', 'Ã€ la une', 'Maroc', 'Morocco', 'ğŸ‡²ğŸ‡¦'),
            'SN': ('fr', 'SN:fr', 'Google ActualitÃ©s', 'Ã€ la une', 'SÃ©nÃ©gal', 'Senegal', 'ğŸ‡¸ğŸ‡³'),
            'UG': ('en-UG', 'UG:en', 'Google News', 'Top stories', 'Uganda', 'Uganda', 'ğŸ‡ºğŸ‡¬'),
            'TZ': ('en-TZ', 'TZ:en', 'Google News', 'Top stories', 'Tanzania', 'Tanzania', 'ğŸ‡¹ğŸ‡¿'),
            'ZW': ('en-ZW', 'ZW:en', 'Google News', 'Top stories', 'Zimbabwe', 'Zimbabwe', 'ğŸ‡¿ğŸ‡¼'),
            'ET': ('en-ET', 'ET:en', 'Google News', 'Top stories', 'Ethiopia', 'Ethiopia', 'ğŸ‡ªğŸ‡¹'),
            'GH': ('en-GH', 'GH:en', 'Google News', 'Top stories', 'Ghana', 'Ghana', 'ğŸ‡¬ğŸ‡­'),
        }
        
        if TOP_COUNTRY not in country_configs:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” êµ­ê°€ ì½”ë“œ: {TOP_COUNTRY}")
        
        hl, ceid, google_news, news_type, country_name, country_name_en, flag = country_configs[TOP_COUNTRY]
        rss_url = f"https://news.google.com/rss?hl={hl}&gl={TOP_COUNTRY}&ceid={ceid}"
        
        # Discord ë©”ì‹œì§€ ì œëª© í˜•ì‹ ìƒì„±
        discord_source= f"`{google_news} - {news_type} - {country_name} {flag}`"
        
        return rss_url, discord_source
    elif RSS_URL_TOP:
        return RSS_URL_TOP, None
    else:
        raise ValueError("TOP_MODEê°€ falseì¼ ë•Œ RSS_URL_TOPë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

def replace_brackets(text):
    """ëŒ€ê´„í˜¸ì™€ êº¾ì‡ ê´„í˜¸ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
    text = text.replace('[', 'ï¼»').replace(']', 'ï¼½')
    text = text.replace('<', 'ã€ˆ').replace('>', 'ã€‰')
    text = re.sub(r'(?<!\s)(?<!^)ï¼»', ' ï¼»', text)
    text = re.sub(r'ï¼½(?!\s)', 'ï¼½ ', text)
    text = re.sub(r'(?<!\s)(?<!^)ã€ˆ', ' ã€ˆ', text)
    text = re.sub(r'ã€‰(?!\s)', 'ã€‰ ', text)
    return text

def parse_html_description(html_desc, session):
    """HTML ì„¤ëª…ì„ íŒŒì‹±í•˜ì—¬ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(html_desc, 'html.parser')
    items = soup.find_all('li')

    news_items = []
    full_content_link = ""
    for item in items:
        if 'Google ë‰´ìŠ¤ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë³´ê¸°' in item.text:
            full_content_link_match = item.find('a')
            if full_content_link_match:
                full_content_link = full_content_link_match['href']
            continue

        title_match = item.find('a')
        press_match = item.find('font', color="#6f6f6f")
        if title_match and press_match:
            google_link = title_match['href']
            link = get_original_url(google_link, session)
            title_text = replace_brackets(title_match.text)
            press_name = press_match.text
            news_item = f"- [{title_text}](<{link}>) | {press_name}"
            news_items.append(news_item)

    news_string = '\n'.join(news_items)
    if full_content_link:
        news_string += f"\n\nâ–¶ï¸ [Google ë‰´ìŠ¤ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë³´ê¸°](<{full_content_link}>)"

    return news_string

def parse_rss_date(pub_date):
    """RSS ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ í˜•ì‹í™”ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dt = parser.parse(pub_date)
    dt_kst = dt.astimezone(gettz('Asia/Seoul'))
    return dt_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')

def send_discord_message(webhook_url, message, avatar_url=None, username=None):
    """Discord ì›¹í›…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
    payload = {"content": message}
    
    # ì•„ë°”íƒ€ URLì´ ì œê³µë˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ payloadì— ì¶”ê°€
    if avatar_url and avatar_url.strip():
        payload["avatar_url"] = avatar_url
    
    # ì‚¬ìš©ì ì´ë¦„ì´ ì œê³µë˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ payloadì— ì¶”ê°€
    if username and username.strip():
        payload["username"] = username
    
    headers = {"Content-Type": "application/json"}
    response = requests.post(webhook_url, json=payload, headers=headers)
    if response.status_code != 204:
        logging.error(f"Discordì— ë©”ì‹œì§€ë¥¼ ê²Œì‹œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        logging.error(response.text)
    else:
        logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
    time.sleep(3)

def extract_news_items(description, session):
    """HTML ì„¤ëª…ì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(description, 'html.parser')
    news_items = []
    for li in soup.find_all('li'):
        a_tag = li.find('a')
        if a_tag:
            title = replace_brackets(a_tag.text)
            google_link = a_tag['href']
            link = get_original_url(google_link, session)
            press = li.find('font', color="#6f6f6f").text if li.find('font', color="#6f6f6f") else ""
            news_items.append({"title": title, "link": link, "press": press})
    return news_items

def apply_advanced_filter(title, description, advanced_filter):
    """ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²Œì‹œë¬¼ì„ ì „ì†¡í• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    if not advanced_filter:
        return True

    text_to_check = (title + ' ' + description).lower()

    # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ê²€ìƒ‰ ì¿¼ë¦¬ íŒŒì‹±
    terms = re.findall(r'([+-]?)(?:"([^"]*)"|\S+)', advanced_filter)

    for prefix, term in terms:
        term = term.lower() if term else prefix.lower()
        if prefix == '+' or not prefix:  # í¬í•¨í•´ì•¼ í•˜ëŠ” ë‹¨ì–´
            if term not in text_to_check:
                return False
        elif prefix == '-':  # ì œì™¸í•´ì•¼ í•˜ëŠ” ë‹¨ì–´ ë˜ëŠ” êµ¬ë¬¸
            # ì—¬ëŸ¬ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ì œì™¸ êµ¬ë¬¸ ì²˜ë¦¬
            exclude_terms = term.split()
            if len(exclude_terms) > 1:
                if ' '.join(exclude_terms) in text_to_check:
                    return False
            else:
                if term in text_to_check:
                    return False

    return True

def parse_date_filter(filter_string):
    since_date = None
    until_date = None
    past_date = None

    logging.info(f"íŒŒì‹± ì¤‘ì¸ ë‚ ì§œ í•„í„° ë¬¸ìì—´: {filter_string}")

    if not filter_string:
        logging.warning("ë‚ ì§œ í•„í„° ë¬¸ìì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return since_date, until_date, past_date

    since_match = re.search(r'since:(\d{4}-\d{2}-\d{2})', filter_string)
    until_match = re.search(r'until:(\d{4}-\d{2}-\d{2})', filter_string)
    
    if since_match:
        since_date = datetime.strptime(since_match.group(1), '%Y-%m-%d').replace(tzinfo=pytz.UTC)
        logging.info(f"since_date íŒŒì‹± ê²°ê³¼: {since_date}")
    if until_match:
        until_date = datetime.strptime(until_match.group(1), '%Y-%m-%d').replace(tzinfo=pytz.UTC)
        logging.info(f"until_date íŒŒì‹± ê²°ê³¼: {until_date}")

    past_match = re.search(r'past:(\d+)([hdmy])', filter_string)
    if past_match:
        value = int(past_match.group(1))
        unit = past_match.group(2)
        now = datetime.now(pytz.UTC)
        if unit == 'h':
            past_date = now - timedelta(hours=value)
        elif unit == 'd':
            past_date = now - timedelta(days=value)
        elif unit == 'm':
            past_date = now - timedelta(days=value*30)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        elif unit == 'y':
            past_date = now - timedelta(days=value*365)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        logging.info(f"past_date íŒŒì‹± ê²°ê³¼: {past_date}")
    else:
        logging.warning("past: í˜•ì‹ì˜ ë‚ ì§œ í•„í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    logging.info(f"ìµœì¢… íŒŒì‹± ê²°ê³¼ - since_date: {since_date}, until_date: {until_date}, past_date: {past_date}")
    return since_date, until_date, past_date

def is_within_date_range(pub_date, since_date, until_date, past_date):
    pub_datetime = parser.parse(pub_date).replace(tzinfo=pytz.UTC)
    now = datetime.now(pytz.UTC)
    
    logging.info(f"ê²€ì‚¬ ì¤‘ì¸ ê¸°ì‚¬ ë‚ ì§œ: {pub_datetime}")
    logging.info(f"í˜„ì¬ ë‚ ì§œ: {now}")
    logging.info(f"ì„¤ì •ëœ í•„í„° - since_date: {since_date}, until_date: {until_date}, past_date: {past_date}")

    if past_date:
        result = pub_datetime >= past_date
        logging.info(f"past_date í•„í„° ì ìš© ê²°ê³¼: {result}")
        return result
    
    if since_date and pub_datetime < since_date:
        logging.info(f"since_date í•„í„°ì— ì˜í•´ ì œì™¸ë¨")
        return False
    if until_date and pub_datetime > until_date:
        logging.info(f"until_date í•„í„°ì— ì˜í•´ ì œì™¸ë¨")
        return False
    
    logging.info(f"ëª¨ë“  ë‚ ì§œ í•„í„°ë¥¼ í†µê³¼í•¨")
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜: RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ê³  Discordë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    rss_url, discord_source = get_rss_url()
    rss_data = fetch_rss_feed(rss_url)
    root = ET.fromstring(rss_data)

    init_db(reset=INITIALIZE_TOP)

    session = requests.Session()
    
    news_items = root.findall('.//item')
    if INITIALIZE_TOP:
        news_items = list(news_items)
    else:
        news_items = reversed(news_items)

    since_date, until_date, past_date = parse_date_filter(DATE_FILTER_TOP)

    for item in news_items:
        guid = item.find('guid').text

        if not INITIALIZE_TOP and is_guid_posted(guid):
            continue

        title = replace_brackets(item.find('title').text)
        google_link = item.find('link').text
        link = get_original_url(google_link, session)
        pub_date = item.find('pubDate').text
        description_html = item.find('description').text
        
        formatted_date = parse_rss_date(pub_date)

        # ë‚ ì§œ í•„í„° ì ìš©
        if not is_within_date_range(pub_date, since_date, until_date, past_date):
            logging.info(f"ë‚ ì§œ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
            continue

        related_news = extract_news_items(description_html, session)
        related_news_json = json.dumps(related_news, ensure_ascii=False)

        description = parse_html_description(description_html, session)

        # ê³ ê¸‰ ê²€ìƒ‰ í•„í„° ì ìš©
        if not apply_advanced_filter(title, description, ADVANCED_FILTER_TOP):
            logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
            continue

        # Discord ë©”ì‹œì§€ êµ¬ì„±
        if discord_source:
            discord_message = f"{discord_source}\n**{title}**\n{link}"
        else:
            discord_message = f"**{title}**\n{link}"
        if description:
            discord_message += f"\n>>> {description}\n\n"
        else:
            discord_message += "\n\n"
        discord_message += f"ğŸ“… {formatted_date}"

        send_discord_message(
            DISCORD_WEBHOOK_TOP,
            discord_message,
            avatar_url=DISCORD_AVATAR_TOP,
            username=DISCORD_USERNAME_TOP
        )

        save_news_item(pub_date, guid, title, link, related_news_json)

        if not INITIALIZE_TOP:
            time.sleep(3)

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")
