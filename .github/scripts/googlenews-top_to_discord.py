import xml.etree.ElementTree as ET
import requests
import re
import os
import time
import random
import logging
import json
import base64
import sqlite3
import sys
from urllib.parse import urlparse
from datetime import datetime, timedelta
from dateutil import parser
from dateutil.tz import gettz
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK_TOP = os.environ.get('DISCORD_WEBHOOK_TOP')
DISCORD_AVATAR_TOP = os.environ.get('DISCORD_AVATAR_TOP', '').strip()
DISCORD_USERNAME_TOP = os.environ.get('DISCORD_USERNAME_TOP', '').strip()
INITIALIZE_TOP = os.environ.get('INITIALIZE_MODE_TOP', 'false').lower() == 'true'
ADVANCED_FILTER_TOP = os.environ.get('ADVANCED_FILTER_TOP', '')
DATE_FILTER_TOP = os.environ.get('DATE_FILTER_TOP', '')
ORIGIN_LINK_TOP = os.environ.get('ORIGIN_LINK_TOP', 'true').lower() == 'true'

# DB ì„¤ì •
DB_PATH = 'google_news_top.db'

def check_env_variables():
    """í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not DISCORD_WEBHOOK_TOP:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK_TOP")

def init_db(reset=False):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        if reset:
            c.execute("DROP TABLE IF EXISTS news_items")
            logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œ")
        c.execute('''CREATE TABLE IF NOT EXISTS news_items
                     (pub_date TEXT,
                      guid TEXT PRIMARY KEY,
                      title TEXT,
                      link TEXT,
                      related_news TEXT)''')
        logging.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

def is_guid_posted(guid):
    """ì£¼ì–´ì§„ GUIDê°€ ì´ë¯¸ ê²Œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT 1 FROM news_items WHERE guid = ?", (guid,))
        return c.fetchone() is not None

def save_news_item(pub_date, guid, title, link, related_news):
    """ë‰´ìŠ¤ í•­ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” êµ¬ì¡° í™•ì¸
        c.execute("PRAGMA table_info(news_items)")
        columns = [column[1] for column in c.fetchall()]
        
        # ê´€ë ¨ ë‰´ìŠ¤ í•­ëª© ìˆ˜ í™•ì¸
        related_news_count = len(json.loads(related_news))
        
        # í•„ìš”í•œ ì—´ ì¶”ê°€
        for i in range(related_news_count):
            title_col = f"related_title_{i+1}"
            press_col = f"related_press_{i+1}"
            link_col = f"related_link_{i+1}"
            
            if title_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {title_col} TEXT")
            if press_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {press_col} TEXT")
            if link_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {link_col} TEXT")
        
        # ë°ì´í„° ì‚½ì…ì„ ìœ„í•œ SQL ì¿¼ë¦¬ ì¤€ë¹„
        columns = ["pub_date", "guid", "title", "link", "related_news"]
        values = [pub_date, guid, title, link, related_news]
        
        related_news_items = json.loads(related_news)
        for i, item in enumerate(related_news_items):
            columns.extend([f"related_title_{i+1}", f"related_press_{i+1}", f"related_link_{i+1}"])
            values.extend([item['title'], item['press'], item['link']])
        
        placeholders = ", ".join(["?" for _ in values])
        columns_str = ", ".join(columns)
        
        c.execute(f"INSERT OR REPLACE INTO news_items ({columns_str}) VALUES ({placeholders})", values)
        
        logging.info(f"ìƒˆ ë‰´ìŠ¤ í•­ëª© ì €ì¥: {guid}")

def fetch_decoded_batch_execute(id):
    s = (
        '[[["Fbv4je","[\\"garturlreq\\",[[\\"en-US\\",\\"US\\",[\\"FINANCE_TOP_INDICES\\",\\"WEB_TEST_1_0_0\\"],'
        'null,null,1,1,\\"US:en\\",null,180,null,null,null,null,null,0,null,null,[1608992183,723341000]],'
        '\\"en-US\\",\\"US\\",1,[2,3,4,8],1,0,\\"655000234\\",0,0,null,0],\\"' +
        id +
        '\\"]",null,"generic"]]]'
    )

    headers = {
        "Content-Type": "application/x-www-form-urlencoded;charset=utf-8",
        "Referer": "https://news.google.com/"
    }

    response = requests.post(
        "https://news.google.com/_/DotsSplashUi/data/batchexecute?rpcids=Fbv4je",
        headers=headers,
        data={"f.req": s}
    )

    if response.status_code != 200:
        raise Exception("Failed to fetch data from Google.")

    text = response.text
    header = '[\\"garturlres\\",\\"'
    footer = '\\",'
    if header not in text:
        raise Exception(f"Header not found in response: {text}")
    start = text.split(header, 1)[1]
    if footer not in start:
        raise Exception("Footer not found in response.")
    url = start.split(footer, 1)[0]
    return url

def decode_base64_url_part(encoded_str):
    base64_str = encoded_str.replace("-", "+").replace("_", "/")
    base64_str += "=" * ((4 - len(base64_str) % 4) % 4)
    try:
        decoded_bytes = base64.urlsafe_b64decode(base64_str)
        decoded_str = decoded_bytes.decode('latin1')
        return decoded_str
    except Exception as e:
        return f"ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def extract_youtube_id(decoded_str):
    pattern = r'\x08 "\x0b([\w-]{11})\x98\x01\x01'
    match = re.search(pattern, decoded_str)
    if match:
        return match.group(1)
    return None

def extract_regular_url(decoded_str):
    parts = re.split(r'[^\x20-\x7E]+', decoded_str)
    url_pattern = r'(https?://[^\s]+)'
    for part in parts:
        match = re.search(url_pattern, part)
        if match:
            return match.group(0)
    return None

def clean_url(url):
    """URLì„ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    parsed_url = urlparse(url)
    
    # MSN ë§í¬ íŠ¹ë³„ ì²˜ë¦¬
    if parsed_url.netloc.endswith('msn.com'):
        parsed_url = parsed_url._replace(scheme='https')
        query_params = parse_qs(parsed_url.query)
        cleaned_params = {k: v[0] for k, v in query_params.items() if k in ['id', 'article']}
        cleaned_query = urlencode(cleaned_params)
        return urlunparse(parsed_url._replace(query=cleaned_query))
    
    # ë‹¤ë¥¸ ëª¨ë“  URLì€ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return url

def decode_google_news_url(source_url):
    url = urlparse(source_url)
    path = url.path.split("/")
    if url.hostname == "news.google.com" and len(path) > 1 and path[-2] == "articles":
        base64_str = path[-1]
        
        try:
            decoded_bytes = base64.urlsafe_b64decode(base64_str + '==')
            decoded_str = decoded_bytes.decode('latin1')
            
            # URL ì¶”ì¶œ
            url_match = re.search(r'(https?://[^\s]+)', decoded_str)
            if url_match:
                extracted_url = url_match.group(1)
                return clean_url(extracted_url)
        except Exception as e:
            logging.error(f"URL ë””ì½”ë”© ì‹¤íŒ¨: {e}")
    
    return clean_url(source_url)

def get_original_url(google_link, session, max_retries=5):
    logging.info(f"ORIGIN_LINK_TOP ê°’ í™•ì¸: {ORIGIN_LINK_TOP}")

    if ORIGIN_LINK_TOP:
        original_url = decode_google_news_url(google_link)
        if original_url != google_link:
            return original_url

        # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ requests ë°©ì‹ ì‹œë„
        retries = 0
        while retries < max_retries:
            try:
                response = session.get(google_link, allow_redirects=True)
                if response.status_code == 200:
                    return clean_url(response.url)
            except requests.RequestException as e:
                logging.error(f"Failed to get original URL: {e}")
            retries += 1
        
        logging.warning(f"ì˜¤ë¦¬ì§€ë„ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return clean_url(google_link)
    else:
        logging.info(f"ORIGIN_LINK_TOPê°€ False, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return clean_url(google_link)

def fetch_rss_feed(url):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    response = requests.get(url)
    return response.content

def replace_brackets(text):
    """ëŒ€ê´„í˜¸ì™€ êº¾ì‡ ê´„í˜¸ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
    text = text.replace('[', 'ï¼»').replace(']', 'ï¼½')
    text = text.replace('<', 'ã€ˆ').replace('>', 'ã€‰')
    text = re.sub(r'(?<!\s)(?<!^)ï¼»', ' ï¼»', text)
    text = re.sub(r'ï¼½(?!\s)', 'ï¼½ ', text)
    text = re.sub(r'(?<!\s)(?<!^)ã€ˆ', ' ã€ˆ', text)
    text = re.sub(r'ã€‰(?!\s)', 'ã€‰ ', text)
    return text

def parse_html_description(html_desc, session):
    """HTML ì„¤ëª…ì„ íŒŒì‹±í•˜ì—¬ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(html_desc, 'html.parser')
    items = soup.find_all('li')

    news_items = []
    full_content_link = ""
    for item in items:
        if 'Google ë‰´ìŠ¤ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë³´ê¸°' in item.text:
            full_content_link_match = item.find('a')
            if full_content_link_match:
                full_content_link = full_content_link_match['href']
            continue

        title_match = item.find('a')
        press_match = item.find('font', color="#6f6f6f")
        if title_match and press_match:
            google_link = title_match['href']
            link = get_original_url(google_link, session)
            title_text = replace_brackets(title_match.text)
            press_name = press_match.text
            news_item = f"- [{title_text}](<{link}>) | {press_name}"
            news_items.append(news_item)

    news_string = '\n'.join(news_items)
    if full_content_link:
        news_string += f"\n\nâ–¶ï¸ [Google ë‰´ìŠ¤ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë³´ê¸°](<{full_content_link}>)"

    return news_string

def parse_rss_date(pub_date):
    """RSS ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ í˜•ì‹í™”ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dt = parser.parse(pub_date)
    dt_kst = dt.astimezone(gettz('Asia/Seoul'))
    return dt_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')

def send_discord_message(webhook_url, message, avatar_url=None, username=None):
    """Discord ì›¹í›…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
    payload = {"content": message}
    
    # ì•„ë°”íƒ€ URLì´ ì œê³µë˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ payloadì— ì¶”ê°€
    if avatar_url and avatar_url.strip():
        payload["avatar_url"] = avatar_url
    
    # ì‚¬ìš©ì ì´ë¦„ì´ ì œê³µë˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ payloadì— ì¶”ê°€
    if username and username.strip():
        payload["username"] = username
    
    headers = {"Content-Type": "application/json"}
    response = requests.post(webhook_url, json=payload, headers=headers)
    if response.status_code != 204:
        logging.error(f"Discordì— ë©”ì‹œì§€ë¥¼ ê²Œì‹œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        logging.error(response.text)
    else:
        logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
    time.sleep(3)

def extract_news_items(description, session):
    """HTML ì„¤ëª…ì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(description, 'html.parser')
    news_items = []
    for li in soup.find_all('li'):
        a_tag = li.find('a')
        if a_tag:
            title = replace_brackets(a_tag.text)
            google_link = a_tag['href']
            link = get_original_url(google_link, session)
            press = li.find('font', color="#6f6f6f").text if li.find('font', color="#6f6f6f") else ""
            news_items.append({"title": title, "link": link, "press": press})
    return news_items

def apply_advanced_filter(title, description, advanced_filter):
    """ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²Œì‹œë¬¼ì„ ì „ì†¡í• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    if not advanced_filter:
        return True

    text_to_check = (title + ' ' + description).lower()

    # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ê²€ìƒ‰ ì¿¼ë¦¬ íŒŒì‹±
    terms = re.findall(r'([+-]?)(?:"([^"]*)"|\S+)', advanced_filter)

    for prefix, term in terms:
        term = term.lower() if term else prefix.lower()
        if prefix == '+' or not prefix:  # í¬í•¨í•´ì•¼ í•˜ëŠ” ë‹¨ì–´
            if term not in text_to_check:
                return False
        elif prefix == '-':  # ì œì™¸í•´ì•¼ í•˜ëŠ” ë‹¨ì–´ ë˜ëŠ” êµ¬ë¬¸
            # ì—¬ëŸ¬ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ì œì™¸ êµ¬ë¬¸ ì²˜ë¦¬
            exclude_terms = term.split()
            if len(exclude_terms) > 1:
                if ' '.join(exclude_terms) in text_to_check:
                    return False
            else:
                if term in text_to_check:
                    return False

    return True

def parse_date_filter(filter_string):
    """ë‚ ì§œ í•„í„° ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ê¸°ì¤€ ë‚ ì§œì™€ ê¸°ê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    since_date = None
    until_date = None
    past_date = None

    # since ë˜ëŠ” until íŒŒì‹±
    since_match = re.search(r'since:(\d{4}-\d{2}-\d{2})', filter_string)
    until_match = re.search(r'until:(\d{4}-\d{2}-\d{2})', filter_string)
    
    if since_match:
        since_date = datetime.strptime(since_match.group(1), '%Y-%m-%d')
    elif until_match:
        until_date = datetime.strptime(until_match.group(1), '%Y-%m-%d')

    # past íŒŒì‹±
    past_match = re.search(r'past:(\d+)([hdmy])', filter_string)
    if past_match:
        value = int(past_match.group(1))
        unit = past_match.group(2)
        now = datetime.now()
        if unit == 'h':
            past_date = now - timedelta(hours=value)
        elif unit == 'd':
            past_date = now - timedelta(days=value)
        elif unit == 'm':
            past_date = now - timedelta(days=value*30)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        elif unit == 'y':
            past_date = now - timedelta(days=value*365)  # ê·¼ì‚¬ê°’ ì‚¬ìš©

    return since_date, until_date, past_date

def is_within_date_range(pub_date, since_date, until_date, past_date):
    """ì£¼ì–´ì§„ ë‚ ì§œê°€ í•„í„° ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    pub_datetime = parser.parse(pub_date)
    
    if past_date:
        return pub_datetime >= past_date
    
    if since_date:
        return pub_datetime >= since_date
    if until_date:
        return pub_datetime <= until_date
    
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜: RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ê³  Discordë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    rss_url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"
    rss_data = fetch_rss_feed(rss_url)
    root = ET.fromstring(rss_data)

    init_db(reset=INITIALIZE_TOP)

    session = requests.Session()
    
    news_items = root.findall('.//item')
    if INITIALIZE_TOP:
        news_items = list(news_items)
    else:
        news_items = reversed(news_items)

    since_date, until_date, past_date = parse_date_filter(DATE_FILTER_TOP)

    for item in news_items:
        guid = item.find('guid').text

        if not INITIALIZE_TOP and is_guid_posted(guid):
            continue

        title = replace_brackets(item.find('title').text)
        google_link = item.find('link').text
        link = get_original_url(google_link, session)
        pub_date = item.find('pubDate').text
        description_html = item.find('description').text
        
        formatted_date = parse_rss_date(pub_date)

        # ë‚ ì§œ í•„í„° ì ìš©
        if not is_within_date_range(pub_date, since_date, until_date, past_date):
            logging.info(f"ë‚ ì§œ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
            continue

        related_news = extract_news_items(description_html, session)
        related_news_json = json.dumps(related_news, ensure_ascii=False)

        description = parse_html_description(description_html, session)

        # ê³ ê¸‰ ê²€ìƒ‰ í•„í„° ì ìš©
        if not apply_advanced_filter(title, description, ADVANCED_FILTER_TOP):
            logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
            continue

        discord_message = f"`Google ë‰´ìŠ¤ - ì£¼ìš” ë‰´ìŠ¤ - í•œêµ­ ğŸ‡°ğŸ‡·`\n**{title}**\n{link}"
        if description:
            discord_message += f"\n>>> {description}\n\n"
        else:
            discord_message += "\n\n"
        discord_message += f"ğŸ“… {formatted_date}"

        send_discord_message(
            DISCORD_WEBHOOK_TOP,
            discord_message,
            avatar_url=DISCORD_AVATAR_TOP,
            username=DISCORD_USERNAME_TOP
        )

        save_news_item(pub_date, guid, title, link, related_news_json)

        if not INITIALIZE_TOP:
            time.sleep(3)

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")
