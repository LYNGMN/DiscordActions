import xml.etree.ElementTree as ET
import requests
from html import unescape
import re
import os
import time
import random
from datetime import datetime
from dateutil import parser
from dateutil.tz import gettz
import sqlite3
import logging
from bs4 import BeautifulSoup
import json
import urllib.parse

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK_TOPICS = os.environ.get('DISCORD_WEBHOOK_TOPICS')
INITIALIZE = os.environ.get('INITIALIZE', 'false').lower() == 'true'

# DB ì„¤ì •
DB_PATH = 'google_news_topic.db'

def check_env_variables():
    if not DISCORD_WEBHOOK_TOPICS:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK_TOPICS")

def init_db(reset=False):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    if reset:
        c.execute("DROP TABLE IF EXISTS news_items")
        logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œ")
    c.execute('''CREATE TABLE IF NOT EXISTS news_items
                 (pub_date TEXT,
                  guid TEXT PRIMARY KEY,
                  title TEXT,
                  link TEXT,
                  related_news TEXT)''')
    conn.commit()
    conn.close()
    logging.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

def is_guid_posted(guid):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT * FROM news_items WHERE guid = ?", (guid,))
    result = c.fetchone()
    conn.close()
    return result is not None

def save_news_item(pub_date, guid, title, link, related_news):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    
    # ê¸°ì¡´ í…Œì´ë¸” êµ¬ì¡° í™•ì¸
    c.execute("PRAGMA table_info(news_items)")
    columns = [column[1] for column in c.fetchall()]
    
    # ê´€ë ¨ ë‰´ìŠ¤ í•­ëª© ìˆ˜ í™•ì¸
    related_news_count = len(json.loads(related_news))
    
    # í•„ìš”í•œ ì—´ ì¶”ê°€
    for i in range(related_news_count):
        title_col = f"related_title_{i+1}"
        press_col = f"related_press_{i+1}"
        link_col = f"related_link_{i+1}"
        
        if title_col not in columns:
            c.execute(f"ALTER TABLE news_items ADD COLUMN {title_col} TEXT")
        if press_col not in columns:
            c.execute(f"ALTER TABLE news_items ADD COLUMN {press_col} TEXT")
        if link_col not in columns:
            c.execute(f"ALTER TABLE news_items ADD COLUMN {link_col} TEXT")
    
    # ë°ì´í„° ì‚½ì…ì„ ìœ„í•œ SQL ì¿¼ë¦¬ ì¤€ë¹„
    columns = ["pub_date", "guid", "title", "link", "related_news"]
    values = [pub_date, guid, title, link, related_news]
    
    related_news_items = json.loads(related_news)
    for i, item in enumerate(related_news_items):
        columns.extend([f"related_title_{i+1}", f"related_press_{i+1}", f"related_link_{i+1}"])
        values.extend([item['title'], item['press'], item['link']])
    
    placeholders = ", ".join(["?" for _ in values])
    columns_str = ", ".join(columns)
    
    c.execute(f"INSERT OR REPLACE INTO news_items ({columns_str}) VALUES ({placeholders})", values)
    
    conn.commit()
    conn.close()
    logging.info(f"ìƒˆ ë‰´ìŠ¤ í•­ëª© ì €ì¥: {guid}")

def fetch_rss_feed(url):
    response = requests.get(url)
    return response.content

def replace_brackets(text):
    return text.replace("[", "ã€”").replace("]", "ã€•")

def get_original_link(google_link, max_retries=5):
    session = requests.Session()
    wait_times = [5, 10, 30, 45, 60]  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    # MSN ë§í¬ íŠ¹ë³„ ì²˜ë¦¬
    if 'news.google.com/rss/articles/' in google_link and 'msn.com' in google_link:
        try:
            # Google News RSS ë§í¬ì—ì„œ ì‹¤ì œ MSN ë§í¬ ì¶”ì¶œ
            parsed_url = urllib.parse.urlparse(google_link)
            query_params = urllib.parse.parse_qs(parsed_url.query)
            if 'url' in query_params:
                msn_link = query_params['url'][0]
                # URL ë””ì½”ë”©
                msn_link = urllib.parse.unquote(msn_link)
                # ì¶”ê°€ ë””ì½”ë”© ì²˜ë¦¬
                msn_link = urllib.parse.unquote(msn_link)
                logging.info(f"ì¶”ì¶œëœ MSN ë§í¬: {msn_link}")
                return msn_link
        except Exception as e:
            logging.error(f"MSN ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    for attempt in range(max_retries):
        try:
            response = session.get(google_link, allow_redirects=True, timeout=10)
            final_url = response.url
            # URL ë””ì½”ë”©
            final_url = urllib.parse.unquote(final_url)
            logging.info(f"Google ë§í¬: {google_link}")
            logging.info(f"ìµœì¢… URL: {final_url}")
            
            if 'news.google.com' not in final_url:
                return final_url
            else:
                base_wait_time = wait_times[min(attempt, len(wait_times) - 1)]
                wait_time = base_wait_time + random.uniform(0, 5)  # 0-5ì´ˆì˜ ëœë¤ ì‹œê°„ ì¶”ê°€
                logging.warning(f"ì‹œë„ {attempt + 1}/{max_retries}: ì›ë³¸ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                time.sleep(wait_time)
        except requests.RequestException as e:
            base_wait_time = wait_times[min(attempt, len(wait_times) - 1)]
            wait_time = base_wait_time + random.uniform(0, 5)
            logging.warning(f"ì‹œë„ {attempt + 1}/{max_retries}: ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}")
            time.sleep(wait_time)
    
    logging.error(f"ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì›ë³¸ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ë˜ì˜ Google ë§í¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {google_link}")
    return google_link

def parse_html_description(html_desc):
    html_desc = unescape(html_desc)
    items = re.findall(r'<li>(.*?)</li>', html_desc, re.DOTALL)

    news_items = []
    full_content_link = ""
    for item in items:
        if 'Google ë‰´ìŠ¤ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë³´ê¸°' in item:
            full_content_link_match = re.search(r'<a href="(https://news\.google\.com/stories/.*?)"', item)
            if full_content_link_match:
                full_content_link = full_content_link_match.group(1)
            continue

        title_match = re.search(r'<a href="(.*?)".*?>(.*?)</a>', item)
        press_match = re.search(r'<font color="#6f6f6f">(.*?)</font>', item)
        if title_match and press_match:
            google_link, title_text = title_match.groups()
            link = get_original_link(google_link)
            title_text = replace_brackets(title_text)
            press_name = press_match.group(1)
            news_item = f"- [{title_text}](<{link}>) | {press_name}"
            news_items.append(news_item)

    news_string = '\n'.join(news_items)
    if full_content_link:
        news_string += f"\n\nâ–¶ï¸ [Google ë‰´ìŠ¤ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë³´ê¸°](<{full_content_link}>)"

    return news_string

def parse_rss_date(pub_date):
    dt = parser.parse(pub_date)
    dt_kst = dt.astimezone(gettz('Asia/Seoul'))
    return dt_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')

def send_discord_message(webhook_url, message):
    payload = {"content": message}
    headers = {"Content-Type": "application/json; charset=utf-8"}
    response = requests.post(webhook_url, json=payload, headers=headers)
    if response.status_code != 204:
        logging.error(f"Discordì— ë©”ì‹œì§€ë¥¼ ê²Œì‹œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        logging.error(response.text)
    else:
        logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
    time.sleep(3)

def extract_news_items(description):
    news_items = []
    soup = BeautifulSoup(description, 'html.parser')
    for li in soup.find_all('li'):
        a_tag = li.find('a')
        if a_tag:
            title = a_tag.text
            google_link = a_tag['href']
            link = get_original_link(google_link)
            press = li.find('font', color="#6f6f6f").text if li.find('font', color="#6f6f6f") else ""
            news_items.append({"title": title, "link": link, "press": press})
    return news_items

def main():
    rss_url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"
    rss_data = fetch_rss_feed(rss_url)
    root = ET.fromstring(rss_data)

    if INITIALIZE:
        init_db(reset=True)  # DB ì´ˆê¸°í™”
        logging.info("ì´ˆê¸°í™” ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘: ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¬ì„¤ì •í•˜ê³  ëª¨ë“  ë‰´ìŠ¤ í•­ëª©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    else:
        init_db()

    news_items = root.findall('.//item')
    if INITIALIZE:
        news_items = list(news_items)  # ì´ˆê¸°í™” ì‹¤í–‰ ì‹œ ëª¨ë“  í•­ëª© ì²˜ë¦¬ (ì˜¤ë˜ëœ ìˆœ)
    else:
        news_items = reversed(news_items)  # ì¼ë°˜ ì‹¤í–‰ ì‹œ ìµœì‹  í•­ëª©ë¶€í„° ì²˜ë¦¬

    for item in news_items:
        guid = item.find('guid').text

        if not INITIALIZE and is_guid_posted(guid):
            continue

        title = item.find('title').text
        google_link = item.find('link').text
        link = get_original_link(google_link)
        pub_date = item.find('pubDate').text
        description_html = item.find('description').text
        
        title = replace_brackets(title)
        formatted_date = parse_rss_date(pub_date)

        related_news = extract_news_items(description_html)
        related_news_json = json.dumps(related_news, ensure_ascii=False)

        description = parse_html_description(description_html)
        discord_message = f"`Google ë‰´ìŠ¤ - ì£¼ìš” ë‰´ìŠ¤ - í•œêµ­ ğŸ‡°ğŸ‡·`\n**[{title}](<{link}>)**\n>>> {description}\n\nğŸ“… {formatted_date}"
        send_discord_message(DISCORD_WEBHOOK_TOPICS, discord_message)

        save_news_item(pub_date, guid, title, link, related_news_json)

        if not INITIALIZE:
            time.sleep(3)  # ì¼ë°˜ ì‹¤í–‰ ì‹œì—ë§Œ ë”œë ˆì´ ì ìš©

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    finally:
        logging.info("í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¢…ë£Œ")
