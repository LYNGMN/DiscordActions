import xml.etree.ElementTree as ET
import requests
import re
import os
import time
import random
import logging
import sqlite3
import sys
from urllib.parse import urlparse, unquote, parse_qs
from datetime import datetime
from dateutil import parser
from dateutil.tz import gettz
import base64

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK_TOPICS = os.environ.get('DISCORD_WEBHOOK_TOPICS')
DISCORD_AVATAR_TOPICS = os.environ.get('DISCORD_AVATAR_TOPICS')
DISCORD_USERNAME_TOPICS = os.environ.get('DISCORD_USERNAME_TOPICS')
INITIALIZE = os.environ.get('INITIALIZE', 'false').lower() == 'true'
KEYWORD_MODE = os.environ.get('KEYWORD_MODE', 'false').lower() == 'true'
KEYWORD = os.environ.get('KEYWORD', '')
RSS_URL = os.environ.get('RSS_URL', '')

# DB ì„¤ì •
DB_PATH = 'google_news.db'

def check_env_variables():
    """í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not DISCORD_WEBHOOK_TOPICS:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK_TOPICS")
    if KEYWORD_MODE and not KEYWORD:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆì§€ë§Œ KEYWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not KEYWORD_MODE and not RSS_URL:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆì§€ë§Œ RSS_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def init_db(reset=False):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        if reset:
            c.execute("DROP TABLE IF EXISTS news_items")
            logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œ")
        c.execute('''CREATE TABLE IF NOT EXISTS news_items
                     (pub_date TEXT,
                      guid TEXT PRIMARY KEY,
                      title TEXT,
                      link TEXT)''')
        logging.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

def is_guid_posted(guid):
    """ì£¼ì–´ì§„ GUIDê°€ ì´ë¯¸ ê²Œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT 1 FROM news_items WHERE guid = ?", (guid,))
        return c.fetchone() is not None

def save_news_item(pub_date, guid, title, link):
    """ë‰´ìŠ¤ í•­ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("INSERT OR REPLACE INTO news_items (pub_date, guid, title, link) VALUES (?, ?, ?, ?)",
                  (pub_date, guid, title, link))
        logging.info(f"ìƒˆ ë‰´ìŠ¤ í•­ëª© ì €ì¥: {guid}")

def decode_google_news_url(source_url):
    """Google ë‰´ìŠ¤ URLì„ ë””ì½”ë”©í•©ë‹ˆë‹¤."""
    url = urlparse(source_url)
    path = url.path.split('/')
    if (
        url.hostname == "news.google.com" and
        len(path) > 1 and
        path[len(path) - 2] == "articles"
    ):
        base64_str = path[len(path) - 1]
        try:
            decoded_bytes = base64.urlsafe_b64decode(base64_str + '==')
            decoded_str = decoded_bytes.decode('latin1')

            prefix = bytes([0x08, 0x13, 0x22]).decode('latin1')
            if decoded_str.startswith(prefix):
                decoded_str = decoded_str[len(prefix):]

            suffix = bytes([0xd2, 0x01, 0x00]).decode('latin1')
            if decoded_str.endswith(suffix):
                decoded_str = decoded_str[:-len(suffix)]

            bytes_array = bytearray(decoded_str, 'latin1')
            length = bytes_array[0]
            if length >= 0x80:
                decoded_str = decoded_str[2:length+1]
            else:
                decoded_str = decoded_str[1:length+1]

            logging.info(f"Google News URL ë””ì½”ë”© ì„±ê³µ: {source_url} -> {decoded_str}")
            return decoded_str
        except Exception as e:
            logging.error(f"Google News URL ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    logging.warning(f"Google News URL ë””ì½”ë”© ì‹¤íŒ¨, ì›ë³¸ URL ë°˜í™˜: {source_url}")
    return source_url

def get_original_link(google_link, session, max_retries=5):
    """ì›ë³¸ ë§í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    decoded_url = decode_google_news_url(google_link)
    
    if decoded_url.startswith('http'):
        return decoded_url

    # ë””ì½”ë”© ì‹¤íŒ¨ ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ URLì¼ ê²½ìš° request ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
    logging.info(f"ìœ íš¨í•˜ì§€ ì•Šì€ URL. request ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„: {google_link}")
    
    wait_times = [5, 10, 30, 45, 60]
    for attempt in range(max_retries):
        try:
            response = session.get(google_link, allow_redirects=True, timeout=10)
            final_url = response.url
            if 'news.google.com' not in final_url:
                logging.info(f"Request ë°©ì‹ ì„±ê³µ - Google ë§í¬: {google_link}")
                logging.info(f"ìµœì¢… URL: {final_url}")
                return final_url
        except requests.RequestException as e:
            if attempt == max_retries - 1:
                logging.error(f"ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì›ë³¸ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")
                return google_link
            wait_time = wait_times[min(attempt, len(wait_times) - 1)] + random.uniform(0, 5)
            logging.warning(f"ì‹œë„ {attempt + 1}/{max_retries}: ìš”ì²­ ì‹¤íŒ¨. {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}")
            time.sleep(wait_time)

    logging.error(f"ëª¨ë“  ë°©ë²• ì‹¤íŒ¨. ì›ë˜ì˜ Google ë§í¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {google_link}")
    return google_link

def fetch_rss_feed(url):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    response = requests.get(url)
    return response.content

def replace_brackets(text):
    """ëŒ€ê´„í˜¸ì™€ êº¾ì‡ ê´„í˜¸ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
    text = text.replace('[', 'ï¼»').replace(']', 'ï¼½')
    text = text.replace('<', 'ã€ˆ').replace('>', 'ã€‰')
    text = re.sub(r'(?<!\s)(?<!^)ï¼»', ' ï¼»', text)
    text = re.sub(r'ï¼½(?!\s)', 'ï¼½ ', text)
    text = re.sub(r'(?<!\s)(?<!^)ã€ˆ', ' ã€ˆ', text)
    text = re.sub(r'ã€‰(?!\s)', 'ã€‰ ', text)
    return text

def parse_rss_date(pub_date):
    """RSS ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ í˜•ì‹í™”ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dt = parser.parse(pub_date)
    dt_kst = dt.astimezone(gettz('Asia/Seoul'))
    return dt_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')

def send_discord_message(webhook_url, message, avatar_url=None, username=None):
    """Discord ì›¹í›…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
    payload = {"content": message}
    
    if avatar_url and avatar_url.strip():
        payload["avatar_url"] = avatar_url
    
    if username and username.strip():
        payload["username"] = username
    
    headers = {"Content-Type": "application/json"}
    response = requests.post(webhook_url, json=payload, headers=headers)
    if response.status_code != 204:
        logging.error(f"Discordì— ë©”ì‹œì§€ë¥¼ ê²Œì‹œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        logging.error(response.text)
    else:
        logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
    time.sleep(3)

def extract_keyword_from_url(url):
    """RSS URLì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ë””ì½”ë”©í•©ë‹ˆë‹¤."""
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    if 'q' in query_params:
        encoded_keyword = query_params['q'][0]
        return unquote(encoded_keyword)
    return "ì£¼ìš” ë‰´ìŠ¤"  # ê¸°ë³¸ê°’

def main():
    """ë©”ì¸ í•¨ìˆ˜: RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ê³  Discordë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    rss_base_url = "https://news.google.com/rss"
    
    if KEYWORD_MODE:
        encoded_keyword = requests.utils.quote(KEYWORD)
        rss_url = f"{rss_base_url}?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
        category = KEYWORD
    else:
        rss_url = RSS_URL
        category = extract_keyword_from_url(rss_url)

    rss_data = fetch_rss_feed(rss_url)
    root = ET.fromstring(rss_data)

    init_db(reset=INITIALIZE)

    session = requests.Session()
    
    news_items = root.findall('.//item')
    if INITIALIZE:
        news_items = list(news_items)
    else:
        news_items = reversed(news_items)

    for item in news_items:
        guid = item.find('guid').text

        if not INITIALIZE and is_guid_posted(guid):
            continue

        title = replace_brackets(item.find('title').text)
        google_link = item.find('link').text
        link = get_original_link(google_link, session)
        pub_date = item.find('pubDate').text
        
        formatted_date = parse_rss_date(pub_date)

        discord_message = f"`Google ë‰´ìŠ¤ - {category} - í•œêµ­ ğŸ‡°ğŸ‡·`\n**{title}**\n{link}"
        discord_message += f"\n\nğŸ“… {formatted_date}"

        send_discord_message(
            DISCORD_WEBHOOK_TOPICS,
            discord_message,
            avatar_url=DISCORD_AVATAR_TOPICS,
            username=DISCORD_USERNAME_TOPICS
        )

        save_news_item(pub_date, guid, title, link)

        if not INITIALIZE:
            time.sleep(3)

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")
