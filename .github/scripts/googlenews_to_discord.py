import xml.etree.ElementTree as ET
import requests
import re
import os
import time
import random
import logging
import json
import base64
import sqlite3
import sys
from urllib.parse import urlparse
from datetime import datetime
from dateutil import parser
from dateutil.tz import gettz
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK = os.environ.get('DISCORD_WEBHOOK')
DISCORD_AVATAR = os.environ.get('DISCORD_AVATAR')
DISCORD_USERNAME = os.environ.get('DISCORD_USERNAME')
INITIALIZE = os.environ.get('INITIALIZE', 'false').lower() == 'true'
KEYWORD_MODE = os.environ.get('KEYWORD_MODE', 'false').lower() == 'true'
KEYWORD = os.environ.get('KEYWORD', '')
RSS_URL = os.environ.get('RSS_URL', '')

# DB ì„¤ì •
DB_PATH = 'google_news.db'

def check_env_variables():
    """í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not DISCORD_WEBHOOK:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK")
    if KEYWORD_MODE and not KEYWORD:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆì§€ë§Œ KEYWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not KEYWORD_MODE and not RSS_URL:
        raise ValueError("í‚¤ì›Œë“œ ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆì§€ë§Œ RSS_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def init_db(reset=False):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        if reset:
            c.execute("DROP TABLE IF EXISTS news_items")
            logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œ")
        c.execute('''CREATE TABLE IF NOT EXISTS news_items
                     (pub_date TEXT,
                      guid TEXT PRIMARY KEY,
                      title TEXT,
                      link TEXT,
                      related_news TEXT)''')
        logging.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

def is_guid_posted(guid):
    """ì£¼ì–´ì§„ GUIDê°€ ì´ë¯¸ ê²Œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT 1 FROM news_items WHERE guid = ?", (guid,))
        return c.fetchone() is not None

def save_news_item(pub_date, guid, title, link, related_news):
    """ë‰´ìŠ¤ í•­ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("INSERT OR REPLACE INTO news_items (pub_date, guid, title, link, related_news) VALUES (?, ?, ?, ?, ?)",
                  (pub_date, guid, title, link, related_news))
        logging.info(f"ìƒˆ ë‰´ìŠ¤ í•­ëª© ì €ì¥: {guid}")

def decode_google_news_url(source_url):
    """Google ë‰´ìŠ¤ URLì„ ë””ì½”ë”©í•©ë‹ˆë‹¤."""
    url = urlparse(source_url)
    path = url.path.split('/')
    if (
        url.hostname == "news.google.com" and
        len(path) > 1 and
        path[len(path) - 2] == "articles"
    ):
        base64_str = path[len(path) - 1]
        try:
            decoded_bytes = base64.urlsafe_b64decode(base64_str + '==')
            decoded_str = decoded_bytes.decode('latin1')

            prefix = bytes([0x08, 0x13, 0x22]).decode('latin1')
            if decoded_str.startswith(prefix):
                decoded_str = decoded_str[len(prefix):]

            suffix = bytes([0xd2, 0x01, 0x00]).decode('latin1')
            if decoded_str.endswith(suffix):
                decoded_str = decoded_str[:-len(suffix)]

            bytes_array = bytearray(decoded_str, 'latin1')
            length = bytes_array[0]
            if length >= 0x80:
                decoded_str = decoded_str[2:length+1]
            else:
                decoded_str = decoded_str[1:length+1]

            logging.info(f"Google News URL ë””ì½”ë”© ì„±ê³µ: {source_url} -> {decoded_str}")
            return decoded_str
        except Exception as e:
            logging.error(f"Google News URL ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    logging.warning(f"Google News URL ë””ì½”ë”© ì‹¤íŒ¨, ì›ë³¸ URL ë°˜í™˜: {source_url}")
    return source_url

def get_original_link(google_link, session, max_retries=5):
    """ì›ë³¸ ë§í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    decoded_url = decode_google_news_url(google_link)
    
    if decoded_url.startswith('http'):
        return decoded_url

    # ë””ì½”ë”© ì‹¤íŒ¨ ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ URLì¼ ê²½ìš° request ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
    logging.info(f"ìœ íš¨í•˜ì§€ ì•Šì€ URL. request ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„: {google_link}")
    
    wait_times = [5, 10, 30, 45, 60]
    for attempt in range(max_retries):
        try:
            response = session.get(google_link, allow_redirects=True, timeout=10)
            final_url = response.url
            if 'news.google.com' not in final_url:
                logging.info(f"Request ë°©ì‹ ì„±ê³µ - Google ë§í¬: {google_link}")
                logging.info(f"ìµœì¢… URL: {final_url}")
                return final_url
        except requests.RequestException as e:
            if attempt == max_retries - 1:
                logging.error(f"ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì›ë³¸ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")
                return google_link
            wait_time = wait_times[min(attempt, len(wait_times) - 1)] + random.uniform(0, 5)
            logging.warning(f"ì‹œë„ {attempt + 1}/{max_retries}: ìš”ì²­ ì‹¤íŒ¨. {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}")
            time.sleep(wait_time)

    logging.error(f"ëª¨ë“  ë°©ë²• ì‹¤íŒ¨. ì›ë˜ì˜ Google ë§í¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {google_link}")
    return google_link

def fetch_rss_feed(url):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    response = requests.get(url)
    return response.content

def replace_brackets(text):
    """ëŒ€ê´„í˜¸ì™€ êº¾ì‡ ê´„í˜¸ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
    text = text.replace('[', 'ï¼»').replace(']', 'ï¼½')
    text = text.replace('<', 'ã€ˆ').replace('>', 'ã€‰')
    text = re.sub(r'(?<!\s)(?<!^)ï¼»', ' ï¼»', text)
    text = re.sub(r'ï¼½(?!\s)', 'ï¼½ ', text)
    text = re.sub(r'(?<!\s)(?<!^)ã€ˆ', ' ã€ˆ', text)
    text = re.sub(r'ã€‰(?!\s)', 'ã€‰ ', text)
    return text

def parse_rss_date(pub_date):
    """RSS ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ í˜•ì‹í™”ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dt = parser.parse(pub_date)
    dt_kst = dt.astimezone(gettz('Asia/Seoul'))
    return dt_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')

def send_discord_message(webhook_url, message, avatar_url=None, username=None):
    """Discord ì›¹í›…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
    payload = {"content": message}
    
    if avatar_url and avatar_url.strip():
        payload["avatar_url"] = avatar_url
    
    if username and username.strip():
        payload["username"] = username
    
    headers = {"Content-Type": "application/json"}
    response = requests.post(webhook_url, json=payload, headers=headers)
    if response.status_code != 204:
        logging.error(f"Discordì— ë©”ì‹œì§€ë¥¼ ê²Œì‹œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        logging.error(response.text)
    else:
        logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
    time.sleep(3)

def extract_news_items(description, session):
    """HTML ì„¤ëª…ì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(description, 'html.parser')
    news_items = []
    for li in soup.find_all('li'):
        a_tag = li.find('a')
        if a_tag:
            title = replace_brackets(a_tag.text)
            google_link = a_tag['href']
            link = get_original_link(google_link, session)
            press = li.find('font', color="#6f6f6f").text if li.find('font', color="#6f6f6f") else ""
            news_items.append({"title": title, "link": link, "press": press})
    return news_items

def parse_html_description(html_desc, session, main_title, main_link):
    """HTML ì„¤ëª…ì„ íŒŒì‹±í•˜ì—¬ ê´€ë ¨ ë‰´ìŠ¤ ë¬¸ìì—´ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    news_items = extract_news_items(html_desc, session)
    
    # ë©”ì¸ ë‰´ìŠ¤ì™€ ë™ì¼í•œ ì œëª©ê³¼ ë§í¬ë¥¼ ê°€ì§„ í•­ëª© ì œê±°
    news_items = [item for item in news_items if item['title'] != main_title or item['link'] != main_link]
    
    if len(news_items) == 0:
        return "", []  # ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ê±°ë‚˜ ë©”ì¸ ë‰´ìŠ¤ì™€ ë™ì¼í•œ ê²½ìš°
    elif len(news_items) == 1:
        return "", news_items  # ê´€ë ¨ ë‰´ìŠ¤ê°€ 1ê°œì¸ ê²½ìš° (í‘œì‹œí•˜ì§€ ì•ŠìŒ)
    else:
        news_string = '\n'.join([f"> - [{item['title']}]({item['link']}) | {item['press']}" for item in news_items])
        return news_string, news_items

def extract_keyword_from_url(url):
    """RSS URLì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ë””ì½”ë”©í•©ë‹ˆë‹¤."""
    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    if 'q' in query_params:
        encoded_keyword = query_params['q'][0]
        return unquote(encoded_keyword)
    return "ë””ìŠ¤ì½”ë“œ"  # ê¸°ë³¸ê°’

def main():
    """ë©”ì¸ í•¨ìˆ˜: RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ê³  Discordë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    rss_base_url = "https://news.google.com/rss"
    
    if KEYWORD_MODE:
        encoded_keyword = requests.utils.quote(KEYWORD)
        rss_url = f"{rss_base_url}?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
        category = KEYWORD
    else:
        rss_url = RSS_URL
        category = extract_keyword_from_url(rss_url)

    rss_data = fetch_rss_feed(rss_url)
    root = ET.fromstring(rss_data)

    init_db(reset=INITIALIZE)

    session = requests.Session()
    
    news_items = root.findall('.//item')
    if INITIALIZE:
        news_items = list(news_items)
    else:
        news_items = reversed(news_items)

    for item in news_items:
        guid = item.find('guid').text

        if not INITIALIZE and is_guid_posted(guid):
            continue

        title = replace_brackets(item.find('title').text)
        google_link = item.find('link').text
        link = get_original_link(google_link, session)
        pub_date = item.find('pubDate').text
        description_html = item.find('description').text
        
        formatted_date = parse_rss_date(pub_date)

        description, related_news = parse_html_description(description_html, session, title, link)

        discord_message = f"`Google ë‰´ìŠ¤ - {category} - í•œêµ­ ğŸ‡°ğŸ‡·`\n**{title}**\n{link}"
        if description:
            discord_message += f"\n{description}"
        discord_message += f"\n\nğŸ“… {formatted_date}"

        send_discord_message(
            DISCORD_WEBHOOK,
            discord_message,
            avatar_url=DISCORD_AVATAR,
            username=DISCORD_USERNAME
        )

        save_news_item(pub_date, guid, title, link, json.dumps(related_news, ensure_ascii=False))

        if not INITIALIZE:
            time.sleep(3)

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")
