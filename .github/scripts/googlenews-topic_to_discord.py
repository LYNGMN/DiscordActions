import xml.etree.ElementTree as ET
import requests
import re
import os
import time
import random
import logging
import json
import base64
import sqlite3
import sys
from urllib.parse import urlparse
from datetime import datetime, timedelta
from dateutil import parser
from dateutil.tz import gettz
from bs4 import BeautifulSoup

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
DISCORD_WEBHOOK_TOPIC = os.environ.get('DISCORD_WEBHOOK_TOPIC')
DISCORD_AVATAR_TOPIC = os.environ.get('DISCORD_AVATAR_TOPIC', '').strip()
DISCORD_USERNAME_TOPIC = os.environ.get('DISCORD_USERNAME_TOPIC', '').strip()
INITIALIZE_TOPIC = os.environ.get('INITIALIZE_MODE_TOPIC', 'false').lower() == 'true'
ADVANCED_FILTER_TOPIC = os.environ.get('ADVANCED_FILTER_TOPIC', '')
DATE_FILTER_TOPIC = os.environ.get('DATE_FILTER_TOPIC', '')
ORIGIN_LINK_TOPIC = os.getenv('ORIGIN_LINK_TOPIC', '').lower()
ORIGIN_LINK_TOPIC = ORIGIN_LINK_TOPIC not in ['false', 'f', '0', 'no', 'n']
TOPIC_MODE = os.environ.get('TOPIC_MODE', 'false').lower() == 'true'
TOPIC_KEYWORD = os.environ.get('TOPIC_KEYWORD', '')
TOPIC_PARAMS = os.environ.get('TOPIC_PARAMS', '?hl=ko&gl=KR&ceid=KR%3Ako')
RSS_URL_TOPIC = os.environ.get('RSS_URL_TOPIC', '')

# DB ì„¤ì •
DB_PATH = 'google_news_topic.db'

# í† í”½ ID ë§¤í•‘
TOPIC_MAP = {
    # í—¤ë“œë¼ì¸ ë‰´ìŠ¤
    "headlines": ("í—¤ë“œë¼ì¸", "CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FtdHZHZ0pMVWlnQVAB"),
    "korea": ("ëŒ€í•œë¯¼êµ­", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFp4WkRNU0FtdHZLQUFQAQ"),
    "world": ("ì„¸ê³„", "CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx1YlY4U0FtdHZHZ0pMVWlnQVAB"),
    "politics": ("ì •ì¹˜", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFZ4ZERBU0FtdHZLQUFQAQ"),

    # ì—°ì˜ˆ ë‰´ìŠ¤
    "entertainment": ("ì—”í„°í…Œì¸ë¨¼íŠ¸", "CAAqJggKIiBDQkFTRWdvSUwyMHZNREpxYW5RU0FtdHZHZ0pMVWlnQVAB"),
    "celebrity": ("ì—°ì˜ˆ", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREZ5Wm5vU0FtdHZLQUFQAQ"),
    "tv": ("TV", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRGRqTlRJU0FtdHZLQUFQAQ"),
    "music": ("ìŒì•…", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFJ5YkdZU0FtdHZLQUFQAQ"),
    "movies": ("ì˜í™”", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREoyZUc0U0FtdHZLQUFQAQ"),
    "theater": ("ì—°ê·¹", "CAAqJAgKIh5DQkFTRUFvS0wyMHZNRE54YzJSd2F4SUNhMjhvQUFQAQ"),

    # ìŠ¤í¬ì¸  ë‰´ìŠ¤
    "sports": ("ìŠ¤í¬ì¸ ", "CAAqJggKIiBDQkFTRWdvSUwyMHZNRFp1ZEdvU0FtdHZHZ0pMVWlnQVAB"),
    "soccer": ("ì¶•êµ¬", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREoyZURRU0FtdHZLQUFQAQ"),
    "cycling": ("ìì „ê±°", "PLACEHOLDER_ID_CYCLING"),
    "motorsports": ("ëª¨í„°ìŠ¤í¬ì¸ ", "PLACEHOLDER_ID_MOTORSPORTS"),
    "tennis": ("í…Œë‹ˆìŠ¤", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRGRpY3pBU0FtdHZLQUFQAQ"),
    "martial_arts": ("ê²©íˆ¬ê¸°", "CAAqIggKIhxDQkFTRHdvSkwyMHZNRFZyWXpJNUVnSnJieWdBUAE"),
    "basketball": ("ë†êµ¬", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREU0ZHpnU0FtdHZLQUFQAQ"),
    "baseball": ("ì•¼êµ¬", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREU0YW5vU0FtdHZLQUFQAQ"),
    "american_football": ("ë¯¸ì‹ì¶•êµ¬", "PLACEHOLDER_ID_AMERICAN_FOOTBALL"),
    "sports_betting": ("ìŠ¤í¬ì¸  ë² íŒ…", "PLACEHOLDER_ID_SPORTS_BETTING"),
    "water_sports": ("ìˆ˜ìƒ ìŠ¤í¬ì¸ ", "CAAqIggKIhxDQkFTRHdvSkwyMHZNREptYUdSbUVnSnJieWdBUAE"),
    "hockey": ("í•˜í‚¤", "PLACEHOLDER_ID_HOCKEY"),
    "golf": ("ê³¨í”„", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRE0zYUhvU0FtdHZLQUFQAQ"),
    "cricket": ("í¬ë¦¬ì¼“", "PLACEHOLDER_ID_CRICKET"),
    "rugby": ("ëŸ­ë¹„", "PLACEHOLDER_ID_RUGBY"),

    # ë¹„ì¦ˆë‹ˆìŠ¤ ë‰´ìŠ¤
    "business": ("ë¹„ì¦ˆë‹ˆìŠ¤", "CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FtdHZHZ0pMVWlnQVAB"),
    "economy": ("ê²½ì œ", "CAAqIggKIhxDQkFTRHdvSkwyMHZNR2RtY0hNekVnSnJieWdBUAE"),
    "personal_finance": ("ê°œì¸ ê¸ˆìœµ", "CAAqIggKIhxDQkFTRHdvSkwyMHZNREY1Tm1OeEVnSnJieWdBUAE"),
    "finance": ("ê¸ˆìœµ", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREpmTjNRU0FtdHZLQUFQAQ"),
    "digital_currency": ("ë””ì§€í„¸ í†µí™”", "CAAqJAgKIh5DQkFTRUFvS0wyMHZNSEk0YkhsM054SUNhMjhvQUFQAQ"),

    # ê¸°ìˆ  ë‰´ìŠ¤
    "technology": ("ê³¼í•™/ê¸°ìˆ ", "CAAqKAgKIiJDQkFTRXdvSkwyMHZNR1ptZHpWbUVnSnJieG9DUzFJb0FBUAE"),
    "mobile": ("ëª¨ë°”ì¼", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFV3YXpnU0FtdHZLQUFQAQ"),
    "energy": ("ì—ë„ˆì§€", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREp0YlY4U0FtdHZLQUFQAQ"),
    "games": ("ê²Œì„", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREZ0ZHpFU0FtdHZLQUFQAQ"),
    "internet_security": ("ì¸í„°ë„· ë³´ì•ˆ", "CAAqIggKIhxDQkFTRHdvSkwyMHZNRE5xWm01NEVnSnJieWdBUAE"),
    "electronics": ("ì „ìê¸°ê¸°", "PLACEHOLDER_ID_ELECTRONICS"),
    "virtual_reality": ("ê°€ìƒ í˜„ì‹¤", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRGRmYm5rU0FtdHZLQUFQAQ"),
    "robotics": ("ë¡œë´‡", "CAAqJAgKIh5DQkFTRUFvS0wyMHZNREp3TUhRMVpoSUNhMjhvQUFQAQ"),

    # ê±´ê°• ë‰´ìŠ¤
    "health": ("ê±´ê°•", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNR3QwTlRFU0FtdHZLQUFQAQ"),
    "nutrition": ("ì˜ì–‘", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFZrYW1NU0FtdHZLQUFQAQ"),
    "public_health": ("ê³µê³µë³´ê±´í•™", "CAAqIggKIhxDQkFTRHdvSkwyMHZNREpqYlRZeEVnSnJieWdBUAE"),
    "mental_health": ("ì •ì‹  ê±´ê°•", "CAAqIggKIhxDQkFTRHdvSkwyMHZNRE40TmpsbkVnSnJieWdBUAE"),
    "medicine": ("ì˜ì•½í’ˆ", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFJ6YURNU0FtdHZLQUFQAQ"),

    # ê³¼í•™ ë‰´ìŠ¤
    "science": ("ê³¼í•™", "CAAqJggKIiBDQkFTRWdvSUwyMHZNRFp0Y1RjU0FtdHZHZ0pMVWlnQVAB"),
    "space": ("ìš°ì£¼", "CAAqIggKIhxDQkFTRHdvSkwyMHZNREU0TXpOM0VnSnJieWdBUAE"),
    "wildlife": ("ì•¼ìƒë™ë¬¼", "CAAqJAgKIh5DQkFTRUFvS0wyY3ZNVE5pWWw5MGN4SUNhMjhvQUFQAQ"),
    "environment": ("í™˜ê²½", "CAAqIggKIhxDQkFTRHdvSkwyMHZNREp3ZVRBNUVnSnJieWdBUAE"),
    "neuroscience": ("ì‹ ê²½ê³¼í•™", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFZpTm1NU0FtdHZLQUFQAQ"),
    "physics": ("ë¬¼ë¦¬í•™", "PLACEHOLDER_ID_PHYSICS"),
    "geography": ("ì§€ë¦¬í•™", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRE0yYUhZU0FtdHZLQUFQAQ"),
    "paleontology": ("ê³ ìƒë¬¼í•™", "PLACEHOLDER_ID_PALEONTOLOGY"),
    "social_science": ("ì‚¬íšŒ ê³¼í•™", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRFp1Tm5BU0FtdHZLQUFQAQ"),

    # êµìœ¡ ë‰´ìŠ¤
    "education": ("êµìœ¡", "CAAqJQgKIh9DQkFTRVFvTEwyY3ZNVEl4Y0Raa09UQVNBbXR2S0FBUAE"),
    "job_market": ("ì±„ìš©ì •ë³´", "CAAqJAgKIh5DQkFTRUFvS0wyMHZNRFF4TVRWME1oSUNhMjhvQUFQAQ"),
    "online_education": ("ì˜¨ë¼ì¸ êµìœ¡", "PLACEHOLDER_ID_ONLINE_EDUCATION"),
    "higher_education": ("ê³ ë“±êµìœ¡", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRE55TlRVU0FtdHZLQUFQAQ"),

    # ë¼ì´í”„ìŠ¤íƒ€ì¼ ë‰´ìŠ¤
    "lifestyle": ("ë¼ì´í”„ìŠ¤íƒ€ì¼", "CAAqJggKIiBDQkFTRWdvSUwyMHZNRE55YXpBU0FtdHZHZ0pMVWlnQVAB"),
    "automotive": ("ì°¨ëŸ‰", "CAAqIAgKIhpDQkFTRFFvSEwyMHZNR3MwYWhJQ2EyOG9BQVAB"),
    "art_design": ("ì˜ˆìˆ /ë””ìì¸", "CAAqIAgKIhpDQkFTRFFvSEwyMHZNR3BxZHhJQ2EyOG9BQVAB"),
    "beauty": ("ë¯¸ìš©", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREZtTkRNU0FtdHZLQUFQAQ"),
    "food": ("ìŒì‹", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNREozWW0wU0FtdHZLQUFQAQ"),
    "travel": ("ì—¬í–‰", "CAAqIggKIhxDQkFTRHdvSkwyMHZNREUwWkhONEVnSnJieWdBUAE"),
    "shopping": ("ì‡¼í•‘", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNR2hvWkdJU0FtdHZLQUFQAQ"),
    "home": ("í™ˆ", "CAAqIggKIhxDQkFTRHdvSkwyMHZNREZzTUcxM0VnSnJieWdBUAE"),
    "outdoor": ("ì•¼ì™¸ í™œë™", "CAAqJAgKIh5DQkFTRUFvS0wyMHZNRFZpTUc0M2F4SUNhMjhvQUFQAQ"),
    "fashion": ("íŒ¨ì…˜", "CAAqIQgKIhtDQkFTRGdvSUwyMHZNRE15ZEd3U0FtdHZLQUFQAQ")
}

def get_news_prefix(lang):
    """ì–¸ì–´ì— ë”°ë¼ ë‰´ìŠ¤ ì ‘ë‘ì–´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    news_prefix_map = {
        'bn': "Google à¦¸à¦‚à¦¬à¦¾à¦¦",
        'zh': "Google æ–°é—»",
        'en': "Google News",
        'id': "Google Berita",
        'iw': "Google ×—×“×©×•×ª",
        'ja': "Google ãƒ‹ãƒ¥ãƒ¼ã‚¹",
        'ar': "Google Ø£Ø®Ø¨Ø§Ø±",
        'ms': "Google Berita",
        'ko': "Google ë‰´ìŠ¤",
        'th': "Google à¸‚à¹ˆà¸²à¸§",
        'tr': "Google Haberler",
        'vi': "Google Tin tá»©c",
        'ru': "Google ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
        'de': "Google Nachrichten",
        'fr': "Google ActualitÃ©s",
        'es': "Google Noticias",
        'it': "Google Notizie",
        'nl': "Google Nieuws",
        'no': "Google Nyheter",
        'pl': "Google WiadomoÅ›ci",
        'ro': "Google È˜tiri",
        'hu': "Google HÃ­rek",
        'cs': "Google ZprÃ¡vy",
        'fi': "Google Uutiset",
        'da': "Google Nyheder",
        'el': "Google Î•Î¹Î´Î®ÏƒÎµÎ¹Ï‚",
        'sv': "Google Nyheter",
        'pt': "Google NotÃ­cias",
        # ì¶”ê°€ ì–¸ì–´...
    }
    return news_prefix_map.get(lang, "Google News")

def check_env_variables():
    """í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not DISCORD_WEBHOOK_TOPIC:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: DISCORD_WEBHOOK_TOPIC")
    if TOPIC_MODE:
        if TOPIC_KEYWORD not in TOPIC_MAP:
            raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ í† í”½ í‚¤ì›Œë“œì…ë‹ˆë‹¤: {TOPIC_KEYWORD}")
        logging.info(f"í† í”½ ëª¨ë“œ í™œì„±í™”: {TOPIC_KEYWORD}, íŒŒë¼ë¯¸í„°: {TOPIC_PARAMS}")
    else:
        if not RSS_URL_TOPIC:
            raise ValueError("í† í”½ ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆì„ ë•ŒëŠ” RSS_URL_TOPICì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        logging.info(f"ì¼ë°˜ ëª¨ë“œ í™œì„±í™”, RSS í”¼ë“œ URL: {RSS_URL_TOPIC}")

def init_db(reset=False):
    """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        if reset:
            c.execute("DROP TABLE IF EXISTS news_items")
            logging.info("ê¸°ì¡´ news_items í…Œì´ë¸” ì‚­ì œ")
        c.execute('''CREATE TABLE IF NOT EXISTS news_items
                     (pub_date TEXT,
                      guid TEXT PRIMARY KEY,
                      title TEXT,
                      link TEXT,
                      topic TEXT,
                      related_news TEXT)''')
        logging.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

def is_guid_posted(guid):
    """ì£¼ì–´ì§„ GUIDê°€ ì´ë¯¸ ê²Œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        c.execute("SELECT 1 FROM news_items WHERE guid = ?", (guid,))
        return c.fetchone() is not None

def save_news_item(pub_date, guid, title, link, topic, related_news):
    """ë‰´ìŠ¤ í•­ëª©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    with sqlite3.connect(DB_PATH) as conn:
        c = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸” êµ¬ì¡° í™•ì¸
        c.execute("PRAGMA table_info(news_items)")
        columns = [column[1] for column in c.fetchall()]
        
        # ê´€ë ¨ ë‰´ìŠ¤ í•­ëª© ìˆ˜ í™•ì¸
        related_news_count = len(json.loads(related_news))
        
        # í•„ìš”í•œ ì—´ ì¶”ê°€
        for i in range(related_news_count):
            title_col = f"related_title_{i+1}"
            press_col = f"related_press_{i+1}"
            link_col = f"related_link_{i+1}"
            
            if title_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {title_col} TEXT")
            if press_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {press_col} TEXT")
            if link_col not in columns:
                c.execute(f"ALTER TABLE news_items ADD COLUMN {link_col} TEXT")
        
        # ë°ì´í„° ì‚½ì…ì„ ìœ„í•œ SQL ì¿¼ë¦¬ ì¤€ë¹„
        columns = ["pub_date", "guid", "title", "link", "topic", "related_news"]
        values = [pub_date, guid, title, link, topic, related_news]
        
        related_news_items = json.loads(related_news)
        for i, item in enumerate(related_news_items):
            columns.extend([f"related_title_{i+1}", f"related_press_{i+1}", f"related_link_{i+1}"])
            values.extend([item['title'], item['press'], item['link']])
        
        placeholders = ", ".join(["?" for _ in values])
        columns_str = ", ".join(columns)
        
        c.execute(f"INSERT OR REPLACE INTO news_items ({columns_str}) VALUES ({placeholders})", values)
        
        logging.info(f"ìƒˆ ë‰´ìŠ¤ í•­ëª© ì €ì¥: {guid}")

def decode_base64_url_part(encoded_str):
    """base64ë¡œ ì¸ì½”ë”©ëœ ë¬¸ìì—´ì„ ë””ì½”ë”©"""
    base64_str = encoded_str + "=" * ((4 - len(encoded_str) % 4) % 4)
    try:
        decoded_bytes = base64.urlsafe_b64decode(base64_str)
        decoded_str = decoded_bytes.decode('latin1')
        return decoded_str
    except Exception as e:
        return f"ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def extract_regular_url(decoded_str):
    """ë””ì½”ë”©ëœ ë¬¸ìì—´ì—ì„œ ì¼ë°˜ URL ì¶”ì¶œ"""
    parts = re.split(r'[^\x20-\x7E]+', decoded_str)
    url_pattern = r'(https?://[^\s]+)'
    for part in parts:
        match = re.search(url_pattern, part)
        if match:
            return match.group(0)
    return None

def extract_youtube_id(decoded_str):
    """ë””ì½”ë”©ëœ ë¬¸ìì—´ì—ì„œ ìœ íŠœë¸Œ ì˜ìƒ ID ì¶”ì¶œ"""
    pattern = r'\x08 "\x0b([\w-]{11})\x98\x01\x01'
    match = re.search(pattern, decoded_str)
    if match:
        return match.group(1)
    return None

def fetch_original_url_via_request(google_link, session, max_retries=5):
    """ì›ë³¸ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ requestsë¥¼ ì‚¬ìš©"""
    wait_times = [5, 10, 30, 45, 60]
    for attempt in range(max_retries):
        try:
            response = session.get(google_link, allow_redirects=True, timeout=10)
            final_url = response.url
            logging.info(f"Requests ë°©ì‹ ì„±ê³µ - Google ë§í¬: {google_link}")
            logging.info(f"ìµœì¢… URL: {final_url}")
            return final_url
        except requests.RequestException as e:
            if attempt == max_retries - 1:
                logging.error(f"ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ì›ë³¸ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")
                return google_link
            wait_time = wait_times[min(attempt, len(wait_times) - 1)] + random.uniform(0, 5)
            logging.warning(f"ì‹œë„ {attempt + 1}/{max_retries}: ìš”ì²­ ì‹¤íŒ¨. {wait_time:.2f}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}")
            time.sleep(wait_time)

    logging.error(f"ëª¨ë“  ë°©ë²• ì‹¤íŒ¨. ì›ë˜ì˜ Google ë§í¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {google_link}")
    return google_link

def decode_google_news_url(source_url):
    """Google ë‰´ìŠ¤ URLì„ ë””ì½”ë”©í•˜ì—¬ ì›ë³¸ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    url = urlparse(source_url)
    path = url.path.split('/')
    if url.hostname == "news.google.com" and len(path) > 1 and path[-2] == "articles":
        base64_str = path[-1]
        decoded_str = decode_base64_url_part(base64_str)
        
        # ì¼ë°˜ URL í˜•íƒœì¸ì§€ ë¨¼ì € í™•ì¸
        regular_url = extract_regular_url(decoded_str)
        if regular_url:
            logging.info(f"ì¼ë°˜ ë§í¬ ì¶”ì¶œ ì„±ê³µ: {source_url} -> {regular_url}")
            return regular_url
        
        # ì¼ë°˜ URLì´ ì•„ë‹Œ ê²½ìš° ìœ íŠœë¸Œ ID í˜•íƒœì¸ì§€ í™•ì¸
        youtube_id = extract_youtube_id(decoded_str)
        if youtube_id:
            youtube_url = f"https://www.youtube.com/watch?v={youtube_id}"
            logging.info(f"ìœ íŠœë¸Œ ë§í¬ ì¶”ì¶œ ì„±ê³µ: {source_url} -> {youtube_url}")
            return youtube_url
    
    logging.warning(f"Google ë‰´ìŠ¤ URL ë””ì½”ë”© ì‹¤íŒ¨, ì›ë³¸ URL ë°˜í™˜: {source_url}")
    return source_url

def get_original_url(google_link, session, max_retries=5):
    """
    Google ë‰´ìŠ¤ ë§í¬ë¥¼ ì›ë³¸ URLë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
    ORIGIN_LINK_TOPIC ì„¤ì •ì— ë”°ë¼ ë™ì‘ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤:
    - ì„¤ì •í•˜ì§€ ì•Šì•˜ê±°ë‚˜ True: ì˜¤ë¦¬ì§€ë„ ë§í¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    - False: ì› ë§í¬(êµ¬ê¸€ ë§í¬)ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    logging.info(f"ORIGIN_LINK_TOPIC ê°’ í™•ì¸: {ORIGIN_LINK_TOPIC}")

    if ORIGIN_LINK_TOPIC:
        # ì˜¤ë¦¬ì§€ë„ ë§í¬ë¥¼ ê°€ì ¸ì˜¤ë ¤ê³  ì‹œë„
        original_url = decode_google_news_url(google_link)
        if original_url != google_link:
            return original_url

        # ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ requests ë°©ì‹ ì‹œë„
        retries = 0
        while retries < max_retries:
            try:
                response = session.get(google_link, allow_redirects=True)
                if response.status_code == 200:
                    return response.url
            except requests.RequestException as e:
                logging.error(f"ì›ë³¸ URL ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            retries += 1
        
        # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš° ì› ë§í¬ ë°˜í™˜
        logging.warning(f"ì˜¤ë¦¬ì§€ë„ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return google_link
    else:
        # ORIGIN_LINK_TOPICê°€ Falseì¸ ê²½ìš° ì› ë§í¬ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        logging.info(f"ORIGIN_LINK_TOPICê°€ False, ì› ë§í¬ ì‚¬ìš©: {google_link}")
        return google_link

def fetch_rss_feed(url):
    """RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    response = requests.get(url)
    return response.content

def replace_brackets(text):
    """ëŒ€ê´„í˜¸ì™€ êº¾ì‡ ê´„í˜¸ë¥¼ ìœ ë‹ˆì½”ë“œ ë¬¸ìë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."""
    text = text.replace('[', 'ï¼»').replace(']', 'ï¼½')
    text = text.replace('<', 'ã€ˆ').replace('>', 'ã€‰')
    text = re.sub(r'(?<!\s)(?<!^)ï¼»', ' ï¼»', text)
    text = re.sub(r'ï¼½(?!\s)', 'ï¼½ ', text)
    text = re.sub(r'(?<!\s)(?<!^)ã€ˆ', ' ã€ˆ', text)
    text = re.sub(r'ã€‰(?!\s)', 'ã€‰ ', text)
    return text

def parse_html_description(html_desc, session):
    """HTML ì„¤ëª…ì„ íŒŒì‹±í•˜ì—¬ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(html_desc, 'html.parser')
    items = soup.find_all('li')

    news_items = []
    full_content_link = ""
    for item in items:
        if 'Google ë‰´ìŠ¤ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë³´ê¸°' in item.text or 'View Full Coverage on Google News' in item.text:
            full_content_link_match = item.find('a')
            if full_content_link_match:
                full_content_link = full_content_link_match['href']
            continue

        title_match = item.find('a')
        press_match = item.find('font', color="#6f6f6f")
        if title_match and press_match:
            google_link = title_match['href']
            link = get_original_url(google_link, session)
            title_text = replace_brackets(title_match.text)
            press_name = press_match.text
            news_item = f"- [{title_text}](<{link}>) | {press_name}"
            news_items.append(news_item)

    news_string = '\n'.join(news_items)
    if full_content_link:
        news_string += f"â–¶ï¸ [Google ë‰´ìŠ¤ì—ì„œ ì „ì²´ ì½˜í…ì¸  ë³´ê¸°]({full_content_link})"

    return news_string

def extract_news_items(description, session):
    """HTML ì„¤ëª…ì—ì„œ ë‰´ìŠ¤ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(description, 'html.parser')
    news_items = []
    for li in soup.find_all('li'):
        a_tag = li.find('a')
        if a_tag:
            title = replace_brackets(a_tag.text)
            google_link = a_tag['href']
            link = get_original_url(google_link, session)
            press = li.find('font', color="#6f6f6f").text if li.find('font', color="#6f6f6f") else ""
            news_items.append({"title": title, "link": link, "press": press})
    return news_items

def parse_rss_date(pub_date):
    """RSS ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ í˜•ì‹í™”ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dt = parser.parse(pub_date)
    dt_kst = dt.astimezone(gettz('Asia/Seoul'))
    return dt_kst.strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')

def send_discord_message(webhook_url, message, avatar_url=None, username=None):
    """Discord ì›¹í›…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
    payload = {"content": message}
    
    # ì•„ë°”íƒ€ URLì´ ì œê³µë˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ payloadì— ì¶”ê°€
    if avatar_url and avatar_url.strip():
        payload["avatar_url"] = avatar_url
    
    # ì‚¬ìš©ì ì´ë¦„ì´ ì œê³µë˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ payloadì— ì¶”ê°€
    if username and username.strip():
        payload["username"] = username
    
    headers = {"Content-Type": "application/json"}
    response = requests.post(webhook_url, json=payload, headers=headers)
    if response.status_code != 204:
        logging.error(f"Discordì— ë©”ì‹œì§€ë¥¼ ê²Œì‹œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}")
        logging.error(response.text)
    else:
        logging.info("Discordì— ë©”ì‹œì§€ ê²Œì‹œ ì™„ë£Œ")
    time.sleep(3)

def apply_advanced_filter(title, description, advanced_filter):
    """ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²Œì‹œë¬¼ì„ ì „ì†¡í• ì§€ ê²°ì •í•©ë‹ˆë‹¤."""
    if not advanced_filter:
        return True

    text_to_check = (title + ' ' + description).lower()

    # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ê²€ìƒ‰ ì¿¼ë¦¬ íŒŒì‹±
    terms = re.findall(r'([+-]?)(?:"([^"]*)"|\S+)', advanced_filter)

    for prefix, term in terms:
        term = term.lower() if term else prefix.lower()
        if prefix == '+' or not prefix:  # í¬í•¨í•´ì•¼ í•˜ëŠ” ë‹¨ì–´
            if term not in text_to_check:
                return False
        elif prefix == '-':  # ì œì™¸í•´ì•¼ í•˜ëŠ” ë‹¨ì–´ ë˜ëŠ” êµ¬ë¬¸
            # ì—¬ëŸ¬ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ì œì™¸ êµ¬ë¬¸ ì²˜ë¦¬
            exclude_terms = term.split()
            if len(exclude_terms) > 1:
                if ' '.join(exclude_terms) in text_to_check:
                    return False
            else:
                if term in text_to_check:
                    return False

    return True

def parse_date_filter(filter_string):
    """ë‚ ì§œ í•„í„° ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ê¸°ì¤€ ë‚ ì§œì™€ ê¸°ê°„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    since_date = None
    until_date = None
    past_date = None

    # since ë˜ëŠ” until íŒŒì‹±
    since_match = re.search(r'since:(\d{4}-\d{2}-\d{2})', filter_string)
    until_match = re.search(r'until:(\d{4}-\d{2}-\d{2})', filter_string)
    
    if since_match:
        since_date = datetime.strptime(since_match.group(1), '%Y-%m-%d')
    elif until_match:
        until_date = datetime.strptime(until_match.group(1), '%Y-%m-%d')

    # past íŒŒì‹±
    past_match = re.search(r'past:(\d+)([hdmy])', filter_string)
    if past_match:
        value = int(past_match.group(1))
        unit = past_match.group(2)
        now = datetime.now()
        if unit == 'h':
            past_date = now - timedelta(hours=value)
        elif unit == 'd':
            past_date = now - timedelta(days=value)
        elif unit == 'm':
            past_date = now - timedelta(days=value*30)  # ê·¼ì‚¬ê°’ ì‚¬ìš©
        elif unit == 'y':
            past_date = now - timedelta(days=value*365)  # ê·¼ì‚¬ê°’ ì‚¬ìš©

    return since_date, until_date, past_date

def is_within_date_range(pub_date, since_date, until_date, past_date):
    """ì£¼ì–´ì§„ ë‚ ì§œê°€ í•„í„° ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    pub_datetime = parser.parse(pub_date)
    
    if past_date:
        return pub_datetime >= past_date
    
    if since_date:
        return pub_datetime >= since_date
    if until_date:
        return pub_datetime <= until_date
    
    return True

def get_topic_category(keyword, lang='en'):
    """í† í”½ í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    categories = {
        "headlines": {
            "en": "Headlines news",
            "ko": "í—¤ë“œë¼ì¸ ë‰´ìŠ¤",
            "zh": "å¤´æ¡æ–°é—»",
            "ja": "ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Schlagzeilen",
            "fr": "ActualitÃ©s Ã  la une",
            "es": "Titulares",
            "pt": "NotÃ­cias principais",
            "it": "Notizie in primo piano",
            "nl": "Hoofdnieuws",
            "sv": "Nyheter i fokus",
            "ar": "Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±",
            "ru": "Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
            "keywords": ["headlines", "korea", "world", "politics"]
        },
        "entertainment": {
            "en": "Entertainment news",
            "ko": "ì—°ì˜ˆ ë‰´ìŠ¤",
            "zh": "å¨±ä¹æ–°é—»",
            "ja": "èŠ¸èƒ½é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Nachrichten aus dem Bereich Unterhaltung",
            "fr": "Actus divertissements",
            "es": "Noticias sobre espectÃ¡culos",
            "pt": "NotÃ­cias de entretenimento",
            "it": "Notizie di intrattenimento",
            "nl": "Entertainmentnieuws",
            "sv": "UnderhÃ¥llningsnyheter",
            "ar": "Ø£Ø®Ø¨Ø§Ø± ØªØ±ÙÙŠÙ‡ÙŠØ©",
            "ru": "Ğ Ğ°Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
            "keywords": ["entertainment", "celebrity", "tv", "music", "movies", "theater"]
        },
        "sports": {
            "en": "Sports news",
            "ko": "ìŠ¤í¬ì¸  ë‰´ìŠ¤",
            "zh": "ä½“è‚²æ–°é—»",
            "ja": "ã‚¹ãƒãƒ¼ãƒ„é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Nachrichten aus dem Bereich Sport",
            "fr": "Actus sportives",
            "es": "Noticias sobre deportes",
            "pt": "NotÃ­cias de esportes",
            "it": "Notizie sportive",
            "nl": "Sportnieuws",
            "sv": "Sportnyheter",
            "ar": "Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©",
            "ru": "Ğ¡Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
            "keywords": ["sports", "soccer", "cycling", "motorsports", "tennis", "martial_arts", 
                         "basketball", "baseball", "american_football", "sports_betting", 
                         "water_sports", "hockey", "golf", "cricket", "rugby"]
        },
        "business": {
            "en": "Business news",
            "ko": "ë¹„ì¦ˆë‹ˆìŠ¤ ë‰´ìŠ¤",
            "zh": "è´¢ç»æ–°é—»",
            "ja": "ãƒ“ã‚¸ãƒã‚¹é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Wirtschaftsmeldungen",
            "fr": "Actus Ã©conomiques",
            "es": "Noticias de negocios",
            "pt": "NotÃ­cias de negÃ³cios",
            "it": "Notizie economiche",
            "nl": "Zakennieuws",
            "sv": "Ekonominyheter",
            "ar": "Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø£Ø¹Ù…Ø§Ù„",
            "ru": "Ğ‘Ğ¸Ğ·Ğ½ĞµÑ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
            "keywords": ["business", "economy", "personal_finance", "finance", "digital_currency"]
        },
        "technology": {
            "en": "Technology news",
            "ko": "ê¸°ìˆ  ë‰´ìŠ¤",
            "zh": "ç§‘æŠ€æ–°é—»",
            "ja": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Nachrichten aus dem Bereich Technologie",
            "fr": "Actus technologie",
            "es": "Noticias de tecnologÃ­a",
            "pt": "NotÃ­cias de tecnologia",
            "it": "Notizie di tecnologia",
            "nl": "Technologienieuws",
            "sv": "Teknologinyheter",
            "ar": "Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§",
            "ru": "Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
            "keywords": ["technology", "mobile", "energy", "games", "internet_security", 
                         "electronics", "virtual_reality", "robotics"]
        },
        "health": {
            "en": "Health news",
            "ko": "ê±´ê°• ë‰´ìŠ¤",
            "zh": "å¥åº·æ–°é—»",
            "ja": "å¥åº·é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Nachrichten aus dem Bereich Gesundheit",
            "fr": "Actus santÃ©",
            "es": "Noticias sobre salud",
            "pt": "NotÃ­cias de saÃºde",
            "it": "Notizie di salute",
            "nl": "Gezondheidsnieuws",
            "sv": "HÃ¤lsonews",
            "ar": "Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØµØ­Ø©",
            "ru": "ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ",
            "keywords": ["health", "nutrition", "public_health", "mental_health", "medicine"]
        },
        "science": {
            "en": "Science news",
            "ko": "ê³¼í•™ ë‰´ìŠ¤",
            "zh": "ç§‘å­¦æ–°é—»",
            "ja": "ç§‘å­¦é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Nachrichten aus dem Bereich Wissenschaft",
            "fr": "Actus sciences",
            "es": "Noticias de ciencia",
            "pt": "NotÃ­cias de ciÃªncia",
            "it": "Notizie di scienza",
            "nl": "Wetenschapsnieuws",
            "sv": "Vetenskapsnyheter",
            "ar": "Ø£Ø®Ø¨Ø§Ø± Ø¹Ù„Ù…ÙŠØ©",
            "ru": "ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
            "keywords": ["science", "space", "wildlife", "environment", "neuroscience", 
                         "physics", "geography", "paleontology", "social_science"]
        },
        "education": {
            "en": "Education news",
            "ko": "êµìœ¡ ë‰´ìŠ¤",
            "zh": "æ•™è‚²æ–°é—»",
            "ja": "æ•™è‚²é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Nachrichten aus dem Bereich Bildung",
            "fr": "Actus enseignement",
            "es": "Noticias sobre educaciÃ³n",
            "pt": "NotÃ­cias de educaÃ§Ã£o",
            "it": "Notizie di istruzione",
            "nl": "Onderwijsnieuws",
            "sv": "Utbildningsnyheter",
            "ar": "Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ…",
            "ru": "ĞĞ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
            "keywords": ["education", "job_market", "online_education", "higher_education"]
        },
        "lifestyle": {
            "en": "Lifestyle news",
            "ko": "ë¼ì´í”„ìŠ¤íƒ€ì¼ ë‰´ìŠ¤",
            "zh": "ç”Ÿæ´»æ—¶å°šæ–°é—»",
            "ja": "ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "de": "Nachrichten aus dem Bereich Lifestyle",
            "fr": "Actus mode de vie",
            "es": "Noticias de estilo de vida",
            "pt": "NotÃ­cias de estilo de vida",
            "it": "Notizie di lifestyle",
            "nl": "Lifestyle nieuws",
            "sv": "Livsstilsnyheter",
            "ar": "Ø£Ø®Ø¨Ø§Ø± Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø­ÙŠØ§Ø©",
            "ru": "ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ° Ğ¶Ğ¸Ğ·Ğ½Ğ¸",
            "keywords": ["lifestyle", "automotive", "art_design", "beauty", "food", "travel", 
                         "shopping", "home", "outdoor", "fashion"]
        }
    }
    
    for category, data in categories.items():
        if keyword in data["keywords"]:
            return data[lang]
    
    return "ê¸°íƒ€ ë‰´ìŠ¤" if lang == 'ko' else "Other News"

def get_topic_display_name(keyword):
    """í† í”½ í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” í‘œì‹œ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return TOPIC_MAP.get(keyword, (keyword, ''))[0]

def get_country_emoji(country_code):
    """êµ­ê°€ ì½”ë“œë¥¼ ìœ ë‹ˆì½”ë“œ í”Œë˜ê·¸ ì´ëª¨ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if len(country_code) != 2:
        return ''
    return chr(ord(country_code[0].upper()) + 127397) + chr(ord(country_code[1].upper()) + 127397)

def is_korean_params(params):
    """íŒŒë¼ë¯¸í„°ê°€ í•œêµ­ì–´ ì„¤ì •ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return 'hl=ko' in params and 'gl=KR' in params and 'ceid=KR%3Ako' in params

def main():
    """ë©”ì¸ í•¨ìˆ˜: RSS í”¼ë“œë¥¼ ê°€ì ¸ì™€ ì²˜ë¦¬í•˜ê³  Discordë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    init_db(reset=INITIALIZE_TOPIC)

    session = requests.Session()

    since_date, until_date, past_date = parse_date_filter(DATE_FILTER_TOPIC)

    if TOPIC_MODE:
        topic_id = TOPIC_MAP[TOPIC_KEYWORD][1]
        rss_url = f"https://news.google.com/rss/topics/{topic_id}"
        if TOPIC_PARAMS:
            rss_url += TOPIC_PARAMS
    else:
        rss_url = RSS_URL_TOPIC

    rss_data = fetch_rss_feed(rss_url)
    if rss_data is None:
        logging.error("RSS ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return

    root = ET.fromstring(rss_data)

    news_items = root.findall('.//item')
    if INITIALIZE_TOPIC:
        news_items = sorted(news_items, key=lambda item: parser.parse(item.find('pubDate').text))
    else:
        news_items = list(reversed(news_items))

    for item in news_items:
        guid = item.find('guid').text

        # ì´ˆê¸°í™” ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¤‘ë³µ ê²€ì‚¬
        if not INITIALIZE_TOPIC and is_guid_posted(guid):
            continue

        title = replace_brackets(item.find('title').text)
        google_link = item.find('link').text
        link = get_original_url(google_link, session)
        pub_date = item.find('pubDate').text
        description_html = item.find('description').text
        
        formatted_date = parse_rss_date(pub_date)

        # ë‚ ì§œ í•„í„° ì ìš©
        if not is_within_date_range(pub_date, since_date, until_date, past_date):
            logging.info(f"ë‚ ì§œ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
            continue

        related_news = extract_news_items(description_html, session)
        related_news_json = json.dumps(related_news, ensure_ascii=False)

        description = parse_html_description(description_html, session)

        # ê³ ê¸‰ ê²€ìƒ‰ í•„í„° ì ìš©
        if not apply_advanced_filter(title, description, ADVANCED_FILTER_TOPIC):
            logging.info(f"ê³ ê¸‰ ê²€ìƒ‰ í•„í„°ì— ì˜í•´ ê±´ë„ˆë›°ì–´ì§„ ë‰´ìŠ¤: {title}")
            continue

        is_korean = is_korean_params(TOPIC_PARAMS)
        lang = 'ko' if is_korean else 'en'
        
        if TOPIC_MODE:
            category = get_topic_category(TOPIC_KEYWORD, lang)
            topic_name = get_topic_display_name(TOPIC_KEYWORD)
        else:
            category = "ì¼ë°˜ ë‰´ìŠ¤" if lang == 'ko' else "General news"
            topic_name = "RSS í”¼ë“œ" if lang == 'ko' else "RSS Feed"
        
        # gl íŒŒë¼ë¯¸í„°ì—ì„œ êµ­ê°€ ì½”ë“œ ì¶”ì¶œ
        gl_param = re.search(r'gl=(\w+)', TOPIC_PARAMS)
        country_emoji = get_country_emoji(gl_param.group(1) if gl_param else 'KR')
        
        news_prefix = get_news_prefix(lang)

        # ë¡œê¹…ì„ í†µí•´ ê° ê°’ í™•ì¸
        logging.info(f"news_prefix: {news_prefix}")
        logging.info(f"category: {category}")
        logging.info(f"topic_name: {topic_name}")
        logging.info(f"country_emoji: {country_emoji}")

        discord_message = f"`{news_prefix} - {category} - {topic_name} {country_emoji}`\n**{title}**\n{link}"
        if description:
            discord_message += f"\n>>> {description}\n\n"
        else:
            discord_message += "\n\n"
        discord_message += f"ğŸ“… {formatted_date}"

        send_discord_message(
            DISCORD_WEBHOOK_TOPIC,
            discord_message,
            avatar_url=DISCORD_AVATAR_TOPIC,
            username=DISCORD_USERNAME_TOPIC
        )

        save_news_item(pub_date, guid, title, link, TOPIC_KEYWORD if TOPIC_MODE else "general", related_news_json)

        if not INITIALIZE_TOPIC:
            time.sleep(3)

if __name__ == "__main__":
    try:
        check_env_variables()
        main()
    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹„ì •ìƒ ì¢…ë£Œ
    else:
        logging.info("í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")
